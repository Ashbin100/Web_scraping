A Large-scale Analysis of the
Mnemonic Password Advice
Johannes Kiesel Benno Stein Stefan Lucks
Bauhaus-Universität Weimar
<first name>.<last name>@uni-weimar.de
Abstract—How to choose a strong but still easily memorable extraeffortofmemorizingstrongpasswords.Toanimateusers
password? An often recommended advice is to memorize a to devise stronger passwords, so-called mnemonic passwords
random sentence (the mnemonic) and to concatenate the words’ areoftenrecommended,whichshallprovidebothstrengthand
initials: a so-called mnemonic password. The paper in hand memorability [14], [31], [41]. Such advice boils down to the
analyzestheeffectivenessofthisadvice—intermsoftheobtained
following:
password strength—and sheds light on various related aspects.
While it is infeasible to obtain a sufficiently large sample of Createasentence.Memorizeit.Concatenatethefirst
human-chosen mnemonics, the password strength depends only charactersofeachword.Usethestringaspassword.
onthedistributionofcertaincharacterprobabilities.Weprovide
several pieces of evidence that these character probabilities The strength of mnemonic passwords is based on three as-
are approximately the same for human-chosen mnemonics and sumptions. First, humans can easily remember their mnemon-
sentences from a web crawl and exploit this connection for our ics, a fact that has been shown within several studies [26],
analyses. The presented analyses are independent of cracking
[41]. Second, it is infeasible to guess a mnemonic, even if an
software, avoid privacy concerns, and allow full control over
adversarywasabletogenerateandtestmillionsofguessesper
the details of how passwords are generated from sentences. In
second. This can be assumed, if the user in fact follows the
particular, the paper introduces the following original research
advice and creates the mnemonic himself instead of picking
contributions: (1) construction of one of the largest corpora of
human-chosen mnemonics, (2) construction of two web sentence a famous sentence [26]. Third, and most importantly, the
corpora from the 27.3 TB ClueWeb12 web crawl, (3) demon- derivedpasswordsinheritmostoftheguessingdifficultyofthe
stration of the suitability of web sentences as substitutes for mnemonic, so that guessing the password remains infeasible
mnemonics in password strength analyses, (4) improved estima- as well. To the best of our knowledge, regarding the last point
tion of password probabilities by position-dependent language no results have been published in the relevant literature.
models,and(5)analysisoftheobtainedpasswordstrengthusing
web sentence samples of different sentence complexity and using This paper contributes various new and interesting results
18 generation rules for mnemonic password construction. in this regard. Our approach is to generate passwords from a
hugesampleofhuman-generatedsentencesusingageneration
Ourfindingsincludebothexpectedandlessexpectedresults,
rule (a variant of “concatenate the first characters of each
among others: mnemonic passwords from lowercase letters only
word”), estimate the resulting password distribution with lan-
providecomparablestrengthtomnemonicpasswordsthatexploit
guage models, and calculate common strength estimates from
the 7-bit visible ASCII character set, less complex mnemonics
reduce password strength in offline scenarios by less than ex- the distribution. The contributions in detail:
pected, and longer mnemonic passwords provide more security
• We collect one of the largest available corpora of
in an offline but not necessarily in an online scenario. When
human-chosen mnemonics (Section III-A).
compared to passwords generated by uniform sampling from a
dictionary, distributions of mnemonic passwords can reach the • We extract a total of 3.1 billion web sentences from
same strength against offline attacks with less characters.
the ClueWeb12 crawl [36] with a specialized filter
algorithm (Section III-B), show that these sentences
I. INTRODUCTION are more complex than mnemonics using a standard
readability score, and take a sample with appropriate
Password authentication is widely accepted, has low tech-
sentence complexity (Section III-C).
nical requirements, and hence is expected to stay as a part of
authentication systems [4], [16]. Irrespective their popularity, • We use the corpus of mnemonics to provide evidence
passwordauthenticationhasalwaysbeencriticisedforthefact that the distributions of the character probabilities
thatuserstendtochooseweakpasswords—simplytoavoidthe which are used by common password strength mea-
sures are approximately the same for mnemonics
and web sentences (both all and the less complex
Permission to freely reproduce all or part of this paper for noncommercial sample). This allows us to substitute web sentences
purposes is granted provided that copies bear this notice and the full citation for mnemonics (Section III-D).
on the first page. Reproduction for commercial purposes is strictly prohibited
without the prior written consent of the Internet Society, the first-named author • To model mnemonic password distributions we opti-
(for reproduction of an entire paper only), and the author’s employer if the mizelanguagemodels.Forthis,weintroduceposition-
paper was prepared within the scope of employment. dependentlanguagemodelstopasswordmodeling,for
NDSS ’17, 26 February - 1 March 2017, San Diego, CA, USA
which we show that they improve the estimation over
Copyright 2017 Internet Society, ISBN 1-891562-46-0
http://dx.doi.org/10.14722/ndss.2017.23077 regular language models (Section IV-B).• Usingcommonpasswordstrengthmeasuresthatcover
TableI. NUMBEROFMNEMONICSANDPASSWORDSINTHECORPORA
OFTHISANDOTHERSTUDIES.FORTHECORPORAOFTHISSTUDY,THE
bothonlineandofflineattackscenarios(SectionIV-C), NUMBEROFPASSWORDSISAVERAGEDOVERGENERATIONRULES.
we compare the strengths of password distributions
from all and only the simpler sentences under 18 dif- Corpus #Mnemonics #Passwords
ferent password-generation rules (Section V).
Webis-Sentences-17 3369618811 1381862722
Webis-Simple-Sentences-17 471085690 234106405
Our approach comes along with a number of important
Webis-Mnemonics-17 1048 1035
advantages. It is fully reproducible since it uses a static web
crawl. It exploits the knowledge of the password generation Obfuscated Yahoo! passwords [3] - 70000000
Leaked from RockYou (e.g., [39]) - 32000000
to its full effect, which makes the strength estimates more
University passwords [28] - 44000
reliable comparedto estimates obtainedfrom dictionary-based
Phished from MySpace (e.g., [9]) - 34000
cracking attempts. It causes no privacy concerns since no
Survey by Kelley et al. [21] - 12000
private authentication data is involved. It allows to precisely
Survey by Yang et al. [42] 5334 6236
compare password generation rules, such as concatenating the Survey by Kuo et al. [26] 140 290
words’ last characters instead of the first. Creation advised by Yan et al. [41] 97 290
Received from Passware [30] - 140
II. RELATEDWORK Survey by Vu et al. [38] 40 40
Different to existing studies we do not analyze a password
corpus, but put a well-known1 generation principle for pass- Extending the usual mnemonic password advice, Top-
words to the test. kara et al. [37] suggest complex generation rules to create
passwords very different to the mnemonic. This allows to
Mnemonic password strength analyses have previously
producefromthesamemnemonicsomewhatindependentpass-
focused on cracking them by using dictionary or brute-force
words with different generation rules, which aims at reducing
attacks [41] or a collection of quotes, lyrics, and similar
password-reuse between services. Our estimates could also be
knownphrases[26].However,theseanalysesarebasedonvery
calculated for such rules.
small sample sizes (see Table I), the results depend largely
on the employed cracking dictionaries, and they leave the The good memorability of human-chosen mnemonics has
exact generation process to the participants. Also, the used been shown by previous studies. For example, Yan et al.
mnemonics are not available. It is interesting to note that found that mnemonic passwords are about as memorable as
Kuo et al. find that, if not explicitly forbidden, users tend to passwords selected freely but with at least one non-letter [41].
choosefamoussentencesasmnemonics,withthe—expected— As memorability measure, they used the time needed until the
negative impact on security. passwords—whichthe290participantshadtousefrequently—
are memorized. Random passwords, on the other hand, took
Veryrecently,Yangetal.[42]publishedastrengthanalysis
about 8 times as long to remember.
onwhattheycallmnemonic-basedstrategyvariants,whichare
variations of the “create a sentence” part of the mnemonic Adifferentapproachtomnemonicpasswordsistogenerate
password advice. They find that the security against online the mnemonics for the users using either sentence templates
attacks can be increased when suggesting to the users to and dictionaries [1], [20], linguistic transformations [19], or
use personalized mnemonics and providing them an example language models [13]. While this removes the problem of hu-
mnemonic and password. In contrast, we analyze the security manschoosingweakmnemonics,itisunclearhowthischanges
for different variants of generating the password from the the memorability compared to human-chosen mnemonics.
sentence. Furthermore, since we use a much larger sample of
passwords, we can also estimate the strength of mnemonic
III. SENTENCECORPORAACQUISITION
passwords against offline attacks and our estimates against
online attacks are more robust. The analysis of a password advice requires a huge sam-
ple of the random element of that advice. In the case
Password strength analysis in general used way larger
of the mnemonic password advice, the random element is
passwordsamples(upto70million[3]),butdonotdistinguish
the mnemonic. Section III-A introduces the new Webis-
between mnemonic passwords and others. Especially inter-
Mnemonics-17 corpus, which now is the largest corpus of
esting is the analysis by Bonneau, who found differences in
human-chosen password mnemonics, but which is still far
password strength between different user groups (determined
too small for a well-founded statistical analysis. Hence this
byaccountsettings)[3].Ourcurrentdatadoesnotprovidethis
sectionintroducesalsotwonewcorporaofwebsentences:the
kind of meta information.
Webis-Sentences-17corpus(SectionIII-B),aswellasasubset
An overview of the cracking methods used in these anal- called the Webis-Simple-Sentences-17 corpus whose overall
yses is presented by Dell’Amico et al. [9]. Language models, sentence complexity better fits that of password mnemonics
which we use for our analysis, are also used in password (Section III-C). Section III-D demonstrates that mnemonics
cracking [9], [27], [30], [34]. Presumably, these password and web sentences, though different, are very similar in the
crackers would also benefit from our contribution of position- distributions of character probabilities which are relevant for
dependent language models. estimatingthepasswordstrength.Withthisknowledge,wecan
then estimate the strength of mnemonic passwords using the
1For example in a 2011 survey of 195 university people, about 40% had web sentence corpora.
alreadyusedamnemonicpassword[25].
2(b)
(d)
(a) (c) (e)
Figure1. TheHTMLinterfaceusedtocollecttheWebis-Mnemonics-17corpus(SectionIII-A).(a)Completeinterfaceatthesurveystart.Participantshaveto
readasecurityguidance.Afterthat,thesteps(b-e)areshownoneatatime.(b)Participantshavetoenterasentencethatfulfillsourrequirements(automatically
checked,SectionIII-A).(c)Participantsseetheirsentenceandthecorrespondingpasswordandaretoldtomemorizeboth.Theyhavetotypeinthepassword.
Shouldtheytrytopastethepassword,thepastingfailsandtheyaretoldnottodoso.Theycangobacktostep1tochooseanothersentence.(d)Participants
havetoselectoneoptionforeachquestion.(e)Participantsareaskedtorecallsentenceandpassword.
0.4 ll
0.3
l
l
0.2
ll l
0.1
l
0.0 ll l l l l l l ll
12 14 16 18 20
Words per sentence
ytisneD
l Observed distribution Instead of trying to reproduce the memorability results of
l Geometric model previousresearch(cf.SectionII),weoptedforashorterstudy
with more participants.
In detail, the workers were asked to “create a new, mean-
ingful, and easily memorable English sentence that no one
canguess.”Toresemblethe adviceofchoosingthemnemonic
related to the web page for which it is used (e.g., [14]), we
randomlyshowedonetopicsuggestion(money,shopping,mail,
talking with friends, or no suggestion) to the workers. The
survey interface automatically enforced certain constraints to
Figure2. DistributionofsentencelengthsintheMnemonicsSurveycorpus
mirror plausible password requirements: The mnemonic must
andfittedgeometricmodel.
contain (1) only 7-bit ASCII characters; (2) at least 12 words;
(3) at least 9 different words from an English dictionary (to
A. The Webis-Mnemonics-17 Corpus
ensureEnglishmnemonics);and(4)nosequenceof6ormore
With the aid of the crowd-sourcing platform Amazon words that also occurs in the Webis-Sentences-17 (detailed
Mechanical Turk, 1117 mnemonics were collected in a short below, like a blacklist of known phrases).
survey,eachfromadifferentworker.Figure1showsthestudy
Aftermanualcleaning,1048mnemonicsremain.Indetail,
interface. The workers are told to chose a mnemonic and
we rejected 17 workers that submitted grammatically incor-
remember it (without writing or copying) while answering
rect mnemonics, and filtered mnemonics that were inherently
password-related multiple-choice questions. The study has
meaningless (10), contained several phrases (40) or a known
been designed to fulfill best practices for Mechanical Turk
phrase missed by our filter (1 mnemonic), and where the
user studies [23]. For example, encouraging a participation in
interface did not record correctly (1 mnemonic). As Figure 2
good faith by disabling copy-and-paste. The workers took on
shows, the length of the remaining mnemonics follows a
average 3 minutes and 35 seconds to complete the study.2
geometric distribution, which is similar to password length
distributions in general [27]. Table III gives a few examples
2Thecorpuswithdetailedinteractionlogsoftheworkersisavailableat
from the corpus for each suggested topic.
www.uni-weimar.de/en/media/chairs/webis/corpora/webis-mnemonics-17
3Despite being one of the largest available corpora of
human-chosenmnemonicsforpasswordgeneration,theWebis-
Mnemonics-17 corpus is still too small for the calculation of
theoretical password strength estimates. Such strength esti-
mates rely on the probability distribution of the passwords,
which can not be estimated for corpora of such a small size:
EverysentencefromtheWebis-Mnemonics-17corpusleadsto
a different password, which makes it impossible to infer the
probability distribution from the data. Because of this, most
previous work on strength estimates for mnemonic password
strengths [26], [41] were restricted to reporting the percentage
of cracked passwords when using cracking software, with
the usual drawbacks [3]: results are hard to compare, hard
to repeat, and rely on the specific cracking method. For
example, because they use different cracking methods, Yan
et al. and Kuo et al. come to different conclusions regarding
the password strengths. In order to resolve these problems, we use a web crawl to collect a huge amount of sentences that are sufficiently similar to human-chosen mnemonics like
those in the Webis-Mnemonics-17 corpus. We then use these
web sentences in place of the mnemonics (see below).
B. The Webis-Sentences-17 Corpus
To analyze natural language sentences at huge scale we
specifically designed the new Webis-Sentences-17 corpus,3
which is based on the ClueWeb12 web page crawl (ver-
sion 1.1) [36]. The ClueWeb12 is a 27.3 TB collection of
733 million English web pages crawled in 2012. It covers
authors from a wide range of age, education, and English-
speaking countries. The ClueWeb12 is distributed as HTML,
making an automatic sentence extraction method necessary.
Since we are interested in content sentences only, we
design an automatic extraction algorithm and test it by com-
paring it to human extraction capabilities. For this purpose,
924 sentences were manually extracted by copy-and-pasting
all fitting sentences from 100 random ClueWeb12 web pages.
Outofthepasswordsfromautomaticsentenceextraction,81%
match those from the human extraction.4 As Section III-D
shows, this quality is sufficient for the purposes of this paper.
We use an own open source extraction method with opti-
mized parameters:5 The method renders the web page text6
and removes non-English paragraphs [15], paragraphs with
less than 400 characters, sentences with less than 50% letter-
only tokens,7 and sentences without English function word.
Wefoundthatsomedomainsusethesamesentencefrequently
andfilteredsuchsentencesbyremovingre-occurrenceswithin
1000 extracted sentences. Further excluding spam pages [8]
could not improve the method. We also tried the standard
Boilerpipe ArticleSentenceExtractor [24], but found that it
performed worse in our tests.
The final Webis-Sentences-17 corpus contains 3.4 billion
sentences. From these, we generate on average 1.4 billion
passwords of length 8 to 20 per generation rule. We chose
this range based on length limits in popular web pages [18].
Table IV gives a few examples from this corpus.
3www.uni-weimar.de/en/media/chairs/webis/corpora/webis-sentences-17
4Testedforthelowercaseletterwordinitialspasswordgenerationrule
5Source:github.com/webis-de/aitools4-aq-web-page-content-extraction
6RenderingbyJerichoHTML:jericho.htmlparser.net v.3.2
7TokenizationbyICU4J:site.icu-project.org/home v.53.1
ytisneD
3.0 Observed distribution
2.5 Negative binomial model
2.0
1.5
1.0
0.5
0.0
1.0 1.5 2.0 2.5 3.0
Syllables per word, averaged by sentence
0.25
0.20
0.15
0.10
0.05 0.00
ytisneD
l Observed distribution
l Negative binomial model
l l Model from Webis−Mnemonics−17
l
l l
l ll l l l l
ll ll lll ll
l
ll
l
l
l l l l l l l l l l l l l l l l l l l l l l l l ll lll lll lll lll l ll l ll l ll l ll
12 18 24 30 36
Syllables
Figure 3. Distribution and fitted model of syllable counts per word for
the Webis-Mnemonics-17 corpus (top) and per sentence of length 12 in the
Webis-Sentences-17corpus(bottom)
C. The Webis-Simple-Sentences-17 Corpus
As the Webis-Sentences-17 corpus intuitively contains
morecomplexsentencesthancanbeexpectedformnemonics,
we created the Webis-Simple-Sentences-17 sub-corpus with a
sentencecomplexitylikeintheWebis-Mnemonics-17corpus.3
Formeasuringsentencecomplexity,weusethestandardFlesch
reading ease test [11] (higher F means more readable):
#syllables #words
F=206.835−84.6· −1.015· . (1)
#words #sentences
For the Webis-Simple-Sentences-17 corpus, we sample
sentences from the Webis-Sentences-17 corpus such that, for
each sentence length, the syllable distribution of the sampled
sentences matches the syllable distribution in the Webis-
Mnemonics-17 corpus. Since this requires only to compare
Flesch values for single sentences of the same length, Equa-
tion1essentiallyreducestothenumberofsyllables,whereless
syllables correspond to simpler sentences. For the sampling
probabilities, we fit negative binomial models—which are
usualforsyllablecountsofEnglishsentences[17]—totheob-
servedsyllablecountsoftheWebis-Mnemonics-17andWebis-
Sentences-17 corpora.8 Figure 3 shows these models. When
sampling sentences, the appropriate sampling probability for
each sentence length and syllable count follows directly from
these models. Also, the Figure shows that the web sentences
are indeed significantly more complex than human-chosen
mnemonics. Hence, the Webis-Simple-Sentences-17 corpus
is more similar to mnemonics than the Webis-Sentences-17
corpus.Thefinalcorpusconsistsof0.5billionsentences.From
thesesentences,wegenerateonaverage0.23billionpasswords
of length 8 to 20 per generation rule. Table V gives a few
examples from this corpus.
8Asthenegativebinomialdistributionisadiscretedistribution,themodel
fortheWebis-Mnemonics-17syllablecountsisfirstfittoatransformedvalue
of(syllables-per-word−1)·100andthentransformedinversely
4TableII. CHARACTER-WISECROSSENTROPYESTIMATESFOR TableIII. EXAMPLESENTENCESDRAWNRANDOMLYFROMTHE
PASSWORDSFROMTHEWEBIS-MNEMONICS-17CORPUSOFLENGTH12. WEBIS-MNEMONICS-17CORPUSFOREACHOFTHETOPICSUGGESTIONS
MODELCORPORA:WEBIS-MNEMONICS-17(WM), FROMTHEUSERSTUDY(CF.SECTIONIII-C).
WEBIS-SENTENCES-17(WS),WEBIS-SIMPLE-SENTENCES-17(WSS).
Model order No suggestion
Character set Model corpus 0 1 2 3 4 5 • Whatwasthecolorofyourcarwhenyouweretwentyyearsold?
• The order of my favorite colors followed by my cousin’s pets is
WS 4.95 4.64 4.58 4.56 4.62 4.75
the password that I use.
ASCII WSS 4.94 4.63 4.56 4.55 4.62 4.76 • The five green ships docked at the west yellow arrow pointing
WM 4.59 4.51 4.54 4.55 4.55 4.55
south.
• i have an upside down kayak that floats on air without wings
WS 4.17 4.11 4.08 4.06 4.07 4.14
Lowercase • Three birds are sitting on a hibiscus tree driving their cars fast
WSS 4.16 4.09 4.06 4.04 4.06 4.14
letters • my very eager mother just served us pickles, never eat shredded
WM 4.14 4.10 4.18 4.20 4.20 4.20
wheat
• My parents are driving here from Michigan to visit for a week.
D. Web Sentence and Mnemonic Similarity Your sentence should be related to mail
• beautiful mails require a touch of golden heart and brave minds
We will now argue why password strength estimates will that also pray
be approximately the same for passwords from mnemonics • Savings under the floorboards are safer than inside a big bank
and from web sentences. (a) Strength estimates for password vault.
distributions depend on the distribution of password proba- • Boy, you must be Fedex because you look like a hot mail.
• Isitalljunktoday,oristhereanythingworthwhileforachange?
bilities and not on the literal passwords (cf. Section IV-C).
• Iliketalkingwithmyfriendsaboutcurrenteventsandthingsthat
(b)Passwordprobabilitiescanbeestimatedwellfromasample
will happen in the near time coming.
usinglanguagemodels,assuccessfullyexploitedforpassword
• i can remember very well what i try to keep as a secret
cracking [9], [27], [30], [34]. (c) Language models estimate • I pick up the mail at noon from the mailbox in the lobby of the
password probabilities using only the conditional probabilities building
of the characters given their preceding characters [7]. • Iwanttobecomeasuccessfulteacheraswellasalovablemother
Hence, given passwords from two different password sources
in which these conditional character probabilities follow ap- Your sentence should be related to shopping
proximately the same distributions, the password probabilities • Whileshoppingiusuallypurchasemeaninglessitemsthatiwrap
of these two sources will also follow approximately the same up in shinny paper.
distribution (from b+c), and the strength estimates will there- • whenIdon’thavemoneyIwantit,ifIhavemoneyIwantmore.
fore be approximately the same for both password sources • thecatlikedtoshopforcookiesandbananasatthestoreinfrance
(from a). It is important to note that the above reasoning does • I go shopping in the spring only when it’s raining in Paris.
• There is a little girl shopping for a blue dress for her sister.
not require that both sources contain the same passwords.
• WhenIgoshopping,Ialwaysbuyatleasttwobunchesofbananas.
Moreover,algorithmicsuccessessuggestthattheseconditional
• Warehouse savings can multiply with money deposited into my
character probabilities from mnemonics and web sentences
account every day.
follow approximately the same distributions: (1) Automatic • My three sons bought the faith of the king with a robe.
language identification based on related conditional charac-
ter probabilities works robustly on short texts from various
Your sentence should be related to money
sources [15]; (2) Human-chosen password phrases—a similar
• Cash is king of the hill and worth every penny and cent.
setting to that of mnemonics—can be cracked using language • Thecrispgreenbilldidnotleavethefrugalboy’spocketuntilthe
models from a few million web sentences [34]. day he died.
• The community i was born and raised in until I turned legal age.
In order to provide further evidence for the similarity,
• I like to bathe in a vat of crisp tens and twenties.
we show that, while complete passwords from mnemonics
• Just like my inventory in Dragon Age Origins I am hella loaded
and web sentences are likely different, they are composed • My wife and I are often worried we will have enough money.
from a very similar set of common substrings. This suggests • She will get a new apron on her 3rd birthday next year.
that the difference between mnemonics and web sentences is • i have huge amount of money and have kept all of my money in
more of a topical than a linguistic kind, and has therefore savings banks
not much impact on the strength estimates. To show that
both kind of sentences are composed from a very similar Your sentence should be related to talking with friends
set of common substrings, we compare the cross-entropy— • How do you know that carrots are good for the eye sight?
a standard similarity measure of distributions—of different • I told my friend a secret and told her not to tell anyone
sentence corpora to the Webis-Mnemonics-17 corpus using • Hey tell me what friends usually talk when they meet or call?
• it is important to wash your hands through out the day to keep
language models with specific model orders (cf. Section IV-A
proper hygine.
for details). A model of order o only considers substrings up
• IlikechatwithfriendsbecausetheyaresofunnyandIamhappy
to o+1 characters. As Table II shows, the cross entropy from
I have them.
the web sentences corpora to the mnemonic corpus gets about
• My dear friend how are you and do you know the secret about
as low as the cross entropy between different subsets of the our teacher mallika
mnemoniccorpus.Therefore,thesubstringsuptolength4or5 • Talkingtofriendscanbefunandsometimeswelearnnewthings.
in passwords from the web sentences corpora are very similar • My friends make me feel confident about myself and my work
to those in human-chosen mnemonics. skills
5TableIV. EXAMPLESENTENCESDRAWNRANDOMLYFROMTHE TableV. EXAMPLESENTENCESDRAWNRANDOMLYFROMTHE
WEBIS-SENTENCES-17CORPUS(CF.SECTIONIII-B). WEBIS-SIMPLE-SENTENCES-17CORPUS(CF.SECTIONIII-C).
• There are also other retail outparcel developments on the other • Please do not ask to return an item after 7 days of when you
side of the interchange as well as some industrial development received the item.
in the immediate area, so the center promises to have a strong • Thisguidehasalotofnuggets,andIcouldonlystopwhenIwas
regional draw. finished with it.
• TheADArecommendsthatthecostsassociatedwithpostexposure • She acted as a student leader during her primary school, high
prophylaxis and exposure sequelae be a benefit of Workers’ school, college and graduate studies.
Compensation insurance coverage. • As mentioned, some gyms also have a daycare program so that
• Your agents will come away with the knowledge of how service you can drop the kids off there while you work out.
level and quality go hand-in-hand and how that affects the entire • How much you lose depends on the compression level, but it
contact center. happens with all saves.
• This distance, the ’local loop’, helps determine which of the • Sofaritlookstotopthecurrentkingofthehill(Radeon4870X2)
providersinManhattanwillbethebestoptionstoprovideservice in most but not all benchmarks.
to your location. • Your dog will be well behaved and all your friends will want to
• The arena act was the product of gate keeping & was only ever know how you did it.
important from a commercial standpoint. • And if that is what we want, then talking about "attraction" and
• And when it comes to painting, throw out your color charts "bonding" is a good place to begin.
because rural Pennsylvanians use an array of hues not found in • The ramps vary in size and height and you will want to look
nature or in any hardware stores looking to remain on the right around to find the best one for your ATV needs.
side of the Better Business Bureau. • That’s blatant right there, you should have seen how wroth Bela
• Nominationsare calledfor Vice-presidentand twoDirector posi- Karolyi was about that.
tions on the Board of Directors of ALIA, as incorporated under • Some of the more commonly known herbs to avoid during
Corporations Law. pregnancy include:
• The lack of initiative in this case seemed puzzling due to nearly • Additional cost and energy savings are realized by reducing or
all Americans’ faith at the time in the strength and reliability of eliminating the need for hot water, detergent, labor costs, and
the constitutional machinery of due process. capital costs.
• It will be better for you if you renounce meat & masalas. • You can structure it and then restructure it as per your needs.
IV. PASSWORDSTRENGTHESTIMATION corresponds to passwords that occur not a single time in the
corpus. Therefore, using the maximum-likelihood estimate for
Password strength is measured on the password distribu- each password is unsuitable.
tion, which is unknown for mnemonic passwords but which
Themostwidespreadlanguagemodelsforpasswords,often
can be estimated from huge password samples using lan-
referred to as Markov chains or n-gram models, employ the
guage models. Language models are detailed in general in
chain-ruleofprobabilitytodescribeapasswordprobabilityby
Section IV-A and optimized for mnemonic passwords in Sec-
its length and character probabilities.10 Let the probability of
tionIV-B.Afterthis,SectionIV-Cdetailsthecommonstrength
password x be
measures we use in our analysis. i
For a formal discussion, this section uses the following
(cid:89)(cid:96)i
p =Pr[L=(cid:96) ]· p , where
notations. X is a random variable distributed over a set of i i i,j
n passwords {x ,...,x } according tothe password distribu- j=1
1 n
tionX.Weusep i =Pr[X =x i]todenotetheprobabilitythat p =Pr(cid:104) Xj=xj(cid:12) (cid:12)X1···Xj−1=x1···xj−1,L=(cid:96) (cid:105) . (2)
a password X drawn from X is equal to x . We enumerate i,j i (cid:12) i i
i
passwords in descending order of their associated probability
in X, that means p ≥ ... ≥ p . Furthermore, x1···x(cid:96)i Instead of the exact probabilities in Equation 2, language
1 n i i modelsapproximatethecharacterprobabilitiesbyconditioning
denotes the (cid:96) characters of password x and Xj denotes the
i i on only the o preceding characters [7], and thus require much
random variable of the j-th character of a password. Finally,
less passwords. Therefore, they reduce the model complexity
L denotes a random variable distributed according to the
by assuming that
password lengths in X.
xj−o···xj =xt−o···xt → p =p , (3)
i i k k i,j k,t
A. Language Models
which leads to robust models used successfully in various
Even password corpora several orders of magnitude larger naturallanguagetasks[7].ApplyingEquation3toEquation2,
than the Webis-Sentences-17 corpus would not suffice to cal- (cid:104) (cid:12) (cid:105)
p ≈Pr Xj=xj(cid:12)Xj−o···Xj−1=xj−o···xj−1,L=(cid:96) ,
culatereliablemaximum-likelihoodestimatesfortheprobabil- i,j i(cid:12) i i i
itiesofveryrarepasswords.Themaximum-likelihoodestimate
where a special start-of-password symbol is used to cope with
of a password probability is the number of its occurrences di-
characters preceding x1:
videdbythesizeoftheentirepasswordsample.However,even i
(cid:26)
intheWebis-Sentences-17corpus,theoften-usedGood-Turing Pr(cid:2) Xj =start-of-password symbol(cid:3) = 1 if j ≤0
method9 [12]estimatesthatabout75%oftheprobabilitymass 0 if j >0
9Theestimateiscalculatedasthenumberofpasswordsoccurringonlyonce 10An alternative method introduces an end-of-password symbol that is
treatedlikeanormalcharacterbythelanguagemodel[7].However,theeffect
dividedbythenumberofdifferentpasswordsinthecorpus
ofthischoiceisusuallynegligible[27].
654
52
50
48
46
44
0(cid:215)107 1(cid:215)107 2(cid:215)107 3(cid:215)107 4(cid:215)107 5(cid:215)107 6(cid:215)107 7(cid:215)107
Model training sample size
1H
yportne
nonnahS
ASCII Lowercase letters Models trained on 2.8(cid:215)107 passwords
Standard lllll lll Stanll l dard l l l l l l l l l l l l l l l l
l With smoothing ll With smoothing l l ASCII
With smoothing and position−dependent ll With smoothing and position−dependent l Lowercase letters
0(cid:215)107 1(cid:215)107 2(cid:215)107 3(cid:215)107 4(cid:215)107 5(cid:215)107 6(cid:215)107 7(cid:215)107 0 2 4 6 8
Model training sample size Model order
Figure4. Effectofthesamplesizeandmodelorderinmodeltrainingonestimatedcrossentropyforpasswordsoflength12usingtheASCII(triangles)and
lowercaseletters(circles)charactersets.Theleftandcenterplotsshowtheeffectofthesamplesizefordifferentmodelsettingsandoptimalorder.Theright
plotshowstheeffectofthemodelorderfortheselectedsamplesize.
B. Empirical Language Model Optimization Since the different sentence corpora and generation rules
lead to password corpora of different sizes, we optimize lan-
Language models have several parameter, which are com-
guage models for two scenarios: using all available passwords
monly optimized for a given task using the cross entropy on
(for the best strength estimates) and using only a sample of a
an independent password sample [7]. The cross entropy is
specific size that is reached by most password corpora (for a
(cid:88)n fair strength comparison). In order to ensure a safe optimiza-
H 1(X,X(cid:48))=− p i·logp(cid:48) i, tion without overfitting to the data, we create the language
i=1 models13 from passwords from 19 of the 20 ClueWeb12 parts
where p and p(cid:48) are the probabilities of x under X and X(cid:48) and evaluate them on the last part that contains mostly web
i i i
respectively.Inourcase,X isthecorrectpassworddistribution pages from different domains. Therefore, a smaller entropy
(approximated by the independent password sample) and X(cid:48) estimate directly corresponds to a better model. Figure 4
is the distribution as estimated by the language model. Note (left, center) shows how the entropy estimates decrease with
that, when the language model is perfect, that means X =X(cid:48), increasingsamplesize.Intoensureafaircomparisonbetween
the cross entropy is minimal and equal to H (X). Conversely, generation rules for which we have different sample sizes, we
1
becausealowercrossentropycorrespondstoabetterlanguage use only 2.8 · 107 passwords per password length and rule
model,itissafetooptimizelanguagemodelsforcrossentropy. when comparing rules (Sections V-A,V-B). We chose this size
so that it is reached for most generation rules.
Model Order. The model order o governs the strength of the
Furthermore, Figure 4 shows that smoothed position-
assumptioninEquation3.Forexample,o=(cid:96) givestheunre-
i
dependent models of the highest order perform best, and we
liablemaximum-likelihoodestimateofpasswordprobabilities.
therefore use these models in our experiments in Section V.
On the other hand, o=0 assumes that character probabilities
As the Figure demonstrates, position-dependent models are
areindependentofprecedingcharacters,whichleadstorobust
but heavily biased estimates.11 In general, the best value for o especiallyadvantageousforASCIIpasswords,probablydueto
the included punctuation that occurs mostly as last characters.
depends on the amount of passwords in the sample.
Smoothing. Smoothing methods use prior-assumptions to im- C. Password Distribution Strength Measures
prove the unreliable probability estimates for rarely occurring
Apassword-generationruleisstrongerwhenthepasswords
sequences [7]. We use the interpolated Witten-Bell smoothing
itgeneratesaremoredifficulttoguess.However,thisdifficulty
method [7], [40] for our experiments, which is suggested for
depends largely on the knowledge of the guesser. We employ
character-based models [35]. This method blends unreliable
the common Kerckhoffs’ principle [22]: since we cannot
higher-order estimates with more-reliable lower-order ones.
estimate the knowledge of the adversary, we use the worst-
Position-dependency. For the special case of password distri- case scenario that she knows the full distribution. Even if the
butions,weproposetouseposition-dependentlanguagemodel. adversary would not know the generation rule, related results
Position-dependent models account for the different character suggest that users employ only very few different rules [42].
distributions the start, middle, and end of sentences.12 This is The adversary tries to guess by choosing one password,
done by estimating the conditional character probabilities for verifyingit,andrepeatingtochooseandverifyuntilthecorrect
eachcharacterpositioninapasswordseparately.Formally,this one is found. Since she knows the full password distribution,
corresponds to adding the requirement j = t to Equation 3. she guesses passwords ordered by their probability.
To the best of our knowledge, we are the first to apply
We follow related work on password security and distin-
position-dependent models to passwords. As the results below
guish two scenarios: online, where adversaries have a small
show, position-dependent models are superior for estimating
numberofguessesuntiltheauthenticationsystemblocksthem,
mnemonic password distributions.
and offline, where they are limited only by their time [3].
11This and similar choices between too complex (o = (cid:96)i) and too
13WeuseSRILMv.1.7.1(www.speech.sri.com/projects/srilm/)togenerate
simple(o=0)areknownasbias-variancetrade-offinmachinelearning[2].
language models and a custom implementation based on KenLM (kheafield.
12For example, in web sentences of length 8, a total of 21% of the first
com/code/kenlm/)togetprobabilities.
wordsstartwith“t”,butonly8%ofthelastwordsdoso,too.
7For all the measures detailed below, a higher value corre-
sponds to a stronger password distribution.
Min-entropy. The min-entropy models the very extreme case
where the adversary guesses only a single password [3]. The
min-entropy H is a widespread measure to assess distribu-
∞
tions, not only of passwords. It is defined by
H (X)=−logp
∞ 1
Failure Probability. The failure probability is a measure for
theonlinescenario.Thefailureprobabilityλ reflectstheaver-
β
ageprobabilityofnotguessingapasswordwithβ guesses[5].
β
(cid:88)
λ (X)=1− p β i
i=1
We report on β =10 and β =100 (like [3], [6]).
Work-factor. The α-work-factor is a measure for the offline
scenario.Itmodelsthecasewhereadversariesguessuntilthey
haveguessedafractionαofpasswords.Theα-work-factorµ
α
gives the expected number of guesses [32].
µ (X)=min{β|1−λ (X)≥α} α β
We report on α=0.5 (like [3], [5]).
ShannonEntropy.TheShannonentropyH measuresthebits
1
needed to encode events from a distribution. Unlike the other
strength measures, H considers the full distribution. For a
1
uniform distribution, H =H , and H >H otherwise.
1 ∞ 1 ∞
n
(cid:88)
H (X)=− p ·logp (4)
1 i i
i=1
Shannonentropyisusuallyapproximatedbythecrossentropy
on a held-out password sample (cf. Section IV-B).
The computational cost of the work-factor µ makes it
0.5
infeasiblealreadyforpasswordsoflength9or10,butwefind
that it strongly correlates with the Shannon entropy H in our
1
case (Figure 5, Pearson’s r = 0.71). H has been criticized as
1
a strength measure for password distributions as it does not
clearly model the offline scenario [5], [32]. However, due the
observedstrongcorrelation,weseeitasameaningfulstrength
measure in the case of mnemonic passwords.
40
35
l l l
l
30 ll
ll l
l
25 l
106 107 108 109
Work factor m 0.5
1H
yportne
nonnahS
V. EXPERIMENTS
This section analyzes the strength of mnemonic password
distributions. It addresses the following research questions:
• Which of the password generation rules generates the
strongest password distribution? (Section V-A)
• What effect does sentence complexity have on pass-
word distribution strength? (Section V-B)
• Does password distribution strength increase linearly
with password length? (Section V-C)
• Security-wise, how far are mnemonic passwords from
uniformly sampled character strings? (Section V-D).
• How strong are mnemonic passwords compared to
other password approaches? (Section V-E, V-F)
A. Estimates by Generation Rules
This experiment compares the strength of password distri-
butions from 18generation rules in terms ofcommon strength
measures (Section IV-C). A password generation rule is an
algorithm which a human can apply to transform a short text
into a password. For this evaluation, we selected rules that
varybytheemployedcharacterset,replacementrules,andthe
chosenwordsfromthesentenceandcharactersfromthewords.
Theselectedrulesfollowthestandardruleofwordinitials(no
replacement, every word, first character) [14], [26], [31], [41]
withsomevariationstotesttheeffectofsuchvariationsonthe
reachedsecuritylevel.Ifnotsaidotherwise,otherexperiments
use this standard rule. Our implementation of the generation
rules is available open source.14
Characterset.Thegeneratedpasswordsconsistofeitherlow-
ercase letters (26 characters) or 7-bit visible ASCII characters
(94 characters). Each sentence is processed by a Unicode
compatibility and canonical decomposition and stripped of
diacritical marks. For lowercase passwords, all letters are con-
verted to lowercase. Then, remaining unfitting characters are
removed. Punctuation is treated as an own “word” for ASCII
passwords.15 Whilealargercharactersettheoreticallyleadsto
stronger passwords, especially users of on-screen keyboards
are tempted to use only lowercase letters as switching to
uppercase or special characters is an extra effort.
Replacement. Sometimes the mnemonic password advice in-
cludes to replace words by similar-sounding characters. To
analyze this advice, we include deterministically replacing
ASCII rules
l Lowercase letter rules word prefixes (like “towards” → “2wards”) as a variant.16
Model
Word. We use either every word or every second word in the
sentence for generating the password. Theoretically, omitting
words increases the difficulty of guessing the next character.
26.9
Characterposition.Besidesconcatenatingthefirstcharacters,
we analyze using the last or both characters as variants. For
one-character words, all three variants use this character once.
14https://github.com/webis-de/password-generation-rules
Figure5. Scatterplotofstrengthestimatesfordifferentpasswordgeneration 15WeusetheICU4JBreakIterator:site.icu-project.orgv.53.1
rules and sentence corpora by work-factor (logarithmic scale) and Shannon
16Theemployedreplacementsarebasedonalistof“pronunciationrules”
entropy for passwords of length 8. All language models are trained on
withthetwoadditionalrulesof“to”→“2”and“for”→“4”:
2.8·107 passwords.Thedottedlineshowsanestimatedµ0.5 forreal-world
blog.codinghorror.com/ascii-pronunciation-rules-for-programmers
passwords[3]andthecorrespondingH1 accordingtothemodel.
8TableVI. MIN-ENTROPY(H∞),FAILUREPROBABILITY(λ β),ANDSHANNONENTROPY(H1)FORDIFFERENTPASSWORD-GENERATIONRULES,
SORTEDBYH1.THEVALUESAREFORPASSWORDSOFLENGTH12FROMTHEWEBIS-SENTENCES-17(WS)ANDWEBIS-SIMPLE-SENTENCES-17(WSS)
CORPORAWITHMODELSFROMATMOST2.8·107PASSWORDS.VALUESFROMFEWERTHAN2.8·107PASSWORDSARESHOWNGRAY.
Password generation rule H λ λ H
∞ 10 100 1
Character set Replacement Word Char. pos. WS WSS WS WSS WS WSS WS WSS
ASCII (cid:88) every 2nd 1st 13.8 13.2 0.99958 0.99940 0.99827 0.99753 56.7 55.8
ASCII - every 2nd 1st 13.8 13.2 0.99959 0.99940 0.99827 0.99752 53.6 52.9
ASCII (cid:88) every 1st 13.8 12.5 0.99949 0.99925 0.99760 0.99689 49.9 48.0
ASCII (cid:88) every 2nd last 13.8 13.3 0.99960 0.99939 0.99825 0.99745 49.8 50.0
Lowercase letters - every 2nd 1st 13.1 12.8 0.99956 0.99938 0.99840 0.99739 48.5 48.1
ASCII - every 1st 13.8 12.4 0.99948 0.99925 0.99759 0.99688 47.8 46.2
ASCII - every 2nd last 13.9 13.3 0.99960 0.99939 0.99824 0.99744 47.6 47.8
Lowercase letters - every 1st 11.4 12.8 0.99912 0.99928 0.99738 0.99739 47.3 45.7
ASCII (cid:88) every 2nd 1st+last 12.4 12.8 0.99940 0.99925 0.99775 0.99724 46.5 44.9
Lowercase letters - every 2nd last 13.1 12.8 0.99955 0.99938 0.99833 0.99735 44.6 44.7
ASCII - every 2nd 1st+last 13.1 12.5 0.99948 0.99929 0.99767 0.99725 44.6 43.0
ASCII (cid:88) every last 14.0 12.4 0.99951 0.99925 0.99759 0.99690 43.4 42.6
Lowercase letters - every last 11.4 12.8 0.99912 0.99928 0.99734 0.99738 42.7 41.8
Lowercase letters - every 2nd 1st+last 12.0 13.3 0.99933 0.99941 0.99803 0.99785 42.6 41.2
ASCII - every last 14.0 12.5 0.99950 0.99925 0.99757 0.99689 42.0 41.3
Lowercase letters - every 1st+last 10.3 9.6 0.99708 0.99650 0.99225 0.99098 36.8 35.3
ASCII (cid:88) every 1st+last 10.8 9.7 0.99772 0.99715 0.99108 0.98826 35.8 36.1
ASCII - every 1st+last 8.5 11.5 0.99400 0.99775 0.98634 0.98936 34.8 35.2
Table VI shows the estimated strength measures for pass- every second word and word prefix replacements come with
words of length 12 from the 18 employed generation rules. additional memorization and processing costs, a discussion of
The discussion below focuses on the results for the Webis- which lies outside the scope of this publication.
Sentences-17 corpus. While mnemonic password distributions
in the realworld contain passwords from differentlengths, we B. Estimates by Sentence Complexity
restricttheanalysisheretopasswordsfromonelengthinorder
Table VI also shows that strength estimates for the Webis-
tomakethecomparisoneasiertounderstand,asitremovesthe
Simple-Sentences-17 corpus are most times a bit weaker,
influenceofthelengthdistribution.Especiallygenerationrules
but still very similar, to those from the Webis-Sentences-17
that use two characters per word have very different length
corpus for all distributions with sufficient training passwords.
distributions.Strengthestimatesbasedonanaturaldistribution
The maximum difference for one generation rule between the
of password lengths are discussed from Section V-C onwards.
corporaare1.6bitforH ,0.00026forλ ,0.00071forλ ,
For a fair comparison, we use the same number of passwords ∞ 10 100
and 1.9 bit for H . This corresponds to a large difference for
for all estimates, and mark estimates for rules for which our 1
H and a still noticeable difference for H , but smaller than
data has less passwords in gray. These estimates in gray are ∞ 1
one could expect.
less reliable and biased to higher values for H .
1
Therefore, mnemonics with lower complexity do indeed
For the online scenario measures min-entropy H and
∞
lead to passwords that are easier to guess. This is likely due
failure probability λ , comparable strengths are achieved by
β
to the reduced vocabulary of the mnemonics, which is biased
all generation rules but those that use multiple characters
towards words with less syllables.
and every word, which are considerably weaker. For H , a
∞
further factor is the character set where ASCII has about 1 bit The effect of mnemonic complexity is especially strong
advantage. For λ 100, generation rules that use every second for the min-entropy H ∞, which considers the most probable
word are stronger than other rules. password only. A possible explanation for this is that the
mostprobablepasswordstemsfromsimplesentences,evenfor
For the offline scenario measure H , passwords from
1
the Webis-Sentences-17 corpus. Then, the probability of this
ASCII achieve a similar strength to passwords with only
password increases naturally when more complex sentences
lowercase letters when every word is used, but better strength
are filtered out.
when every second word is used. In total, using every second
word and only the first character with the ASCII character set On the other hand, the effect of mnemonic complexity
leadstothestrongestofthetestedpassworddistributions.Also, is still noticeable for the Shannon entropy H , which con-
1
word prefix replacements can increase the entropy by 2–3 bit. siders the entire password distribution. Therefore, reducing
Moreover, using the first character of a word is preferable. the complexity skews the entire password distribution farther
away from the uniform distribution. However, the effect is
The strongest distribution is arguably using the ASCII
much weaker than for min-entropy. An estimate of the effect
character set, every second word, and only the first characters,
can be the maximum difference in Table VI between Webis-
which achieves best or nearly-best values for all measures.
Sentences-17 and Webis-Simple-Sentences-17 for generation
Wordprefixreplacementconsiderablyincreasethestrengthfor
rules with sufficient training passwords, divided by the pass-
H , but not for the online scenario. However, both using only
1 word size. This estimates the effect to 0.16 bit per character.
9100
80
46 00 l l l l l l l l l l l l l
20
l l l l l l l l l l l l l
0
8 10 12 14 16 18 20
Password length
tiB
TableVII. ESTIMATEDENTROPYBYGENERATIONRULEAND
l U Esn ti if mor am te d di s Str hib au nt nio on n ( eH n1 t= roH p¥ y) , H1 MINIMUMPASSWORDLENGTHFORPASSWORDSFROMTHE
l Estimated min−entropy, H¥ WEBIS-SENTENCES-17CORPUS.
Char. set L. letters ASCII L. letters ASCII ASCII Replacement - - - - (cid:88)
Word every every every2nd every2nd every2nd
Char. pos. 1st 1st 1st 1st 1st
(cid:96) Shannon entropy H min 1
8 38.0 37.9 34.8 37.2 39.0
Figure 6. Shannon entropy and min-entropy estimates compared to the
optimaluniformpassworddistributionbypasswordlength.Passwordsarefrom 9 41.2 41.3 38.0 40.9 42.9
theWebis-Sentences-17corpususinglowercaselettersandthefirstcharacter.
10 44.4 44.8 41.2 44.6 46.7
11 47.6 48.3 44.4 48.3 50.6
C. Estimates by Password Length 12 50.8 51.8 47.6 52.0 54.5
13 54.0 55.2 50.8 55.7 58.4
This section analyzes how the strength of password dis-
tributions increases with password length. The number of 14 57.2 58.7 54.0 59.4 62.3
possible passwords increases exponentially with the password 15 60.4 62.2 57.2 63.1 66.2
length,theoreticallyleadingtostrongerpassworddistributions. 16 63.6 65.7 60.4 66.8 70.1
UsingtheWebis-Sentences-17corpus,weanalyzedallrulesto
17 66.8 69.1 63.6 70.5 74.0
very similar results. As an example, Figure 6 shows the result
18 70.1 72.6 66.8 74.2 77.9
for the standard generation rule using lowercase letters only.
19 73.3 76.1 70.0 77.9 81.8
Figure 6 shows that the resistance against offline at-
20 76.5 79.6 73.2 81.6 85.7
tacks (H ) increases as expected with password length, but
1 21 79.7 83.0 76.4 85.3 89.5
that the resistance against online attacks (H ) stays rather
∞
constant.17 We also found λ and λ to be rather constant. 22 82.9 86.5 79.6 89.0 93.4
10 100
23 86.1 90.0 82.8 92.7 97.3
The approximately constant resistance against online at-
24 89.3 93.5 86.0 96.4 101.2
tacks shown in Figure 6 suggests that, for each length, there
are a few sentences with a high probability irrespective the 25 92.5 97.0 89.2 100.1 105.1
length.Onlyafterthesehigh-probabilitysentences,aspreading 26 95.7 100.4 92.4 103.8 109.0
of the probability mass over the possible sentences occurs. 27 98.9 103.9 95.6 107.5 112.9
ThisspreadingisshownbythesteadyincreaseoftheShannon
28 102.1 107.4 98.8 111.2 116.8
entropy.Unfortunately,theWebis-Mnemonics-17corpusisfar
29 105.3 110.9 102.0 114.9 120.7
too small to reproduce this effect on human-chosen mnemon-
ics. It thus remains unclear to which extent this effect also 30 108.5 114.3 105.2 118.6 124.6
appearsforhuman-chosenmnemonics.However,basedonour
analysis it is reasonable to assume that the resistance against
online attacks of mnemonic passwords grows way less with
in the following. The remaining parameter is the minimum
password length than one would expect.
password length (cid:96) , which one can increase to increase
min
The linear increase of the Shannon entropy with password the password distribution strength, as it is best practice for
length leads to a simple model for estimating the entropy of password-basedauthenticationingeneral[10].Usingthemean
password distributions with several lengths. In detail, one can from the fitted geometric distribution, the average password
rewrite Equation 4 (Shannon entropy) as length is (cid:96) +1.4 for passwords that take every word and
min
(cid:96) +0.7 for passwords that take every second word, while
(cid:96)max min
(cid:88) the mode is (cid:96) in both cases. Note that this consideration
H (X)= Pr[L=(cid:96)]·(H (X )−logPr[L=(cid:96)]), (5) min
1 1 (cid:96) makes the simplifying assumption that the parameter of the
(cid:96)=(cid:96)min geometric distribution does not depend on (cid:96) .
min
where H (X ) is the entropy estimate for length (cid:96). Moreover,
1 (cid:96) Table VII shows the minimum-length based entropy esti-
for the probability of a password-length, Pr[L = (cid:96)], one
mates for a selection of the strongest generation rules. This
can use the geometric model of lengths from the Mnemonic
table aims at replacing for mnemonic passwords the “rules of
Survey corpus (Figure 2).18 Due to the geometric model and
thumb” that exist for the entropy of generic passwords (e.g.,
only a linear increase of the entropy by length, Equation 5
[6]). Unlike these rules of thumb, which were shown to not
converges as (cid:96) increases. We report the converged values
max correlatewiththepassworddistributionstrengthagainstoffline
attacks [39], we have shown that our entropy estimates do
17H∞ variesbetween11.3and14.2bitwithoutacleardirection.
correlate with it (cf. Figure 5). As the Table shows, when
18When only every second word is used, the length distribution can be
considering that rules using only every second word lead to
adjustedaccordingly.However,anadjustmentisnotasstraight-forwardwhen
twocharactersperwordareused,duetoone-characterwords.Asthisvariant shorter passwords on average, these rules lose much of their
gaveveryweakdistributions,wedonotconsiderithere. advantage,andareevenweakerforlowercaseletterpasswords.
10TableVIII. CHARACTER-WISEENTROPY(H1)ANDPERPLEXITY(PPL.)
The maximum length of a word in the dictionary is 6 char-
ESTIMATESFORPASSWORDSBYMODEL(CF.SECTIONIV-B).PASSWORDS
acters. On average, the created passwords have a length of
AREOFLENGTH12FROMTHEWEBIS-SENTENCES-17CORPUSUSINGTHE
FIRSTCHARACTEROFEVERYWORD.THEUNIFORMMODELREPRESENTS n·4.24+(n−1), where (n−1) is then number of space
THEOPTIMALDISTRIBUTIONOVER26/94CHARACTERS. characters as illustrated on the Diceware homepage.
Lowercase The comparison with Diceware highlights the relative
ASCII
letters weakness of mnemonic passwords against online attacks: al-
Model H 1 Ppl. H 1 Ppl. ready the 2-word Diceware password distribution achieves a
Uniform 4.70 26.0 6.55 94.0 failure probability λ 100 of 0.999998 and is thus considerably
Order 0 4.15 17.8 5.09 34.1 stronger in this scenario than every rule we considered and
Order 8 3.71 13.1 3.98 15.8 requires on average only 9.5 characters.
Order 8, position-dependent 3.65 12.6 3.70 13.0
However,mnemonicpasswordsprovideabettersecurityin
theofflinescenarioforthesamepasswordlength.Forexample,
3 Diceware words (average password length of 14.7) achieve
D. Comparison to Uniform Distribution 38.8 bit of Shannon entropy, which is already reached by
lowercase letter mnemonic passwords of minimum length 9
The uniform password distribution is the strongest among
(Table VII, average length of 10.4).
all distributions with the same number of elements, but
mnemonic password distributions fall short of it for three
F. Comparison to Real-world Password Distributions
reasons:(1)somecharactersoccurmorefrequentlythanothers,
(2)charactersinapasswordarenotindependentofeachother, Thissectioncomparesthestrengthestimatesformnemonic
and (3) the character distributions depend on the position in passwordswithestimatesforreal-worldpassworddistributions
the password. Table VIII illustrates exploiting these 3 effects from the literature.
stepbystepforthestandardmnemonicpasswords.Inaddition
totheShannonentropyH ,thetablealsoshowstheperplexity The currently largest-scale password strength analysis of
1
Ppl. = log(H ) which gives the number of elements in a real-worldpasswordsistheanalysisof70millionanonymized
1
uniform distribution with the same entropy. Yahoo! passwords by Bonneau [3], which results in the
following estimates: Min-entropy H ≈ 6.5, failure prob-
∞
According to the results shown in Table VIII, both pass- ability with 10 guesses λ ≈ 0.98178, and work fac-
10
word distributions provide in an offline scenario about the tor µ ≈ 2,111,739.19 While no estimate for the Shannon
0.5
same level of security as a uniform distribution over 12 Entropy H is provided, we can apply the log-linear rela-
1
to 13 characters. The biggest effect is in both cases that tionship of µ and H that we observed for mnemonic
0.5 1
the characters are not uniformly distributed. On the other passwords, which suggests an H of ~27 Bit (cf. Figure 5).
1
hand, exploiting the differences in the character distributions BonneaualsocomparestheYahoo!estimatestoestimatesfrom
by position (using position-dependent models, Section IV-B) the password lists leaked from the RockYou and Battlefield
is especially valuable for ASCII passwords, where it can Heroeswebsites.Hefindsthatthecorrespondingtwopassword
nearly reduce their strength to the strength of lowercase letter distributionsareevenweakeragainstofflineattacks.Also,only
passwords. Like discussed in Section V-A, ASCII passwords the Battlefield Heroes passwords are stronger against online
are only stronger than lowercase letter passwords for specific attacks (H ≈ 7.7, λ ≈ 0.98878).
∞ 10
generation rules.
Comparing these estimates for real-world password distri-
E. Comparison to Dictionary Passwords butionwithourestimatesformnemonicpassword,weseethat
mnemonic passwords are considerably stronger attacks both
While the discussion in the following paragraphs exem- online and offline attacks. For online attacks, our estimates
plifies how our strength estimates can be used to compare for the standard lowercase letters word initial rule are for
the strength of different password generation methods, it does H between 11.4 and 12.8, and for λ between 0.99912
∞ 10
not incorporate other important factors for password usage and 0.99928 (Table VI)—reducing the corresponding success
like memorability, typing convenience, or susceptibility to probability (1 − λ ) compared to the Battlefield Heroes
10
typing errors. Unfortunately, we are not aware of any such passwords by 92–94%.20 For offline attacks, we can extend
comparison. Table VII for smaller (cid:96) , suggesting that a higher H as for
min 1
A second prominent suggestion for password generation real-worldpasswordsisreachedbymnemonicpasswordsfrom
is to pick several words uniformly at random from a large the standard rule with a minimum length (cid:96) min of 5. While the
dictionary [29],[33]. Weuse the7776 words Dicewaredictio- length distribution of the Yahoo! passwords is unknown, the
nary [33] as an example. current minimum password length for new Yahoo! accounts
is 8 and thus considerably larger. Therefore, we can conclude
A computation of the strength of such dictionary-based thatmnemonicpasswordsarestrongeragainstbothonlineand
passwords is straight-forward. The min-entropy and Shannon offline attacks compared to the passwords in use today.
entropy are both equal to
19Thepaperprovidesnormalizedestimates,whichalluseacommonscale.
H =H =n·log7776,
∞ 1 Theestimateswereportareun-normalized.
whereas the failure probability is calculated by
20Whenknownphrasesareallowedasmnemonics,relatedresultssuggest
a similar strength against online attacks as the Battlefield Heroes passwords
β have[42],whichhighlightstheimportanceofdevelopingpassword-blacklists
λ β =1− 7776n . tokeepusersfromchoosingsucheasy-to-guessphrases.
11VI. CONCLUSIONANDOUTLOOK A more detailed study on memorability and mnemonic choice
would be needed to improve this discussion.
This paper analyzes the strength of passwords generated
accordingtothemnemonicpasswordadviceonahugecorpus Furthermore,thisanalysisisrestrictedtoEnglishmnemon-
of 3 billion human-written sentences. The detailed analysis ics only. The question if our results also apply to mnemonics
of this paper considers sentence complexity and 18 different of other languages is open for further research.
password generation rules. To this end, the paper shows that
An interesting avenue for further research could be to use
thenecessarysimilarityofhuman-chosenmnemonicsandweb
searchalgorithmstofindthebestpasswordgenerationrulefor
sentences exists. Furthermore, the paper contributes one of
a given sentence distribution. The 18 rules that we analyzed
the currently biggest corpora of human-chosen mnemonics.
cover only a very small part of the parameter space for such
Additionally,thispaperisthefirsttoapplyposition-dependent
rules.Investigationsinthisdirectionwouldrequiretolowerthe
language models to passwords, which improve on regular
computational cost of evaluating a rule, which is a problem in
language models for modeling mnemonic passwords.
its own right. Moreover, an analysis of the costs of generation
rule parameters like suggested above could also be integrated
Our analysis addressed several questions regarding the
into the cost function of the search algorithm.
strength of mnemonic passwords.
Of the 18 tested password generation rules, the strongest ACKNOWLEDGEMENT
password distribution is generated by using the ASCII charac- We would like to thank our shepherd Lujo Bauer, our
ter set, concatenating the first character of every second word, colleagues Eik List and Martin Potthast, and the anonymous
wherecommonwordprefixreplacementsareusedtoaddmore reviewers for their helpful comments and suggestions. We
special characters to the passwords. Both using only every would also like to thank Norina Marie Grosch for preparing
secondwordandwordprefixreplacementshaveonlyaneffect the Webis-Mnemonics-17 corpus for publication.
in offline attack scenarios, where adversaries are not limited
by a number of guesses but by the time they want to invest. REFERENCES
The sentence complexity of the used mnemonics has a [1] M.J.Atallah,C.J.McDonough,V.Raskin,andS.Nirenburg,“Natural
major effect when the adversary can perform only a few Language Processing for Information Assurance and Security: An
guesses, and a relatively weak effect for offline attacks. OverviewandImplementations,”inProceedingsofthe2000Workshop
on New Security Paradigms, ser. NSPW ’00. New York, NY, USA:
We showed that an attacker can use knowledge on the ACM,2000,pp.51–65.
generation process of mnemonic passwords to drastically in- [2] C. Bishop, Pattern Recognition and Machine Learning (Information
creasehissuccesschances,reducingthestrengthofmnemonic Science and Statistics). Secaucus, NJ, USA: Springer-Verlag New
York,Inc.,2006.
passwords against offline attacks to that of passwords from a
[3] J.Bonneau,“Thescienceofguessing:analyzingananonymizedcorpus
uniform distribution over only 12 to 13 characters.
of 70 million passwords,” in Security and Privacy (SP), 2012 IEEE
Symposiumon. IEEE,2012,pp.538–552.
We analyzed the effect of password length on the strength
estimates, and found that—as one would expect—the strength [4] J.Bonneau,C.Herley,P.C.vanOorschot,andF.Stajano,“ThePast,
Present, and Future of Password-based Authentication on the Web,”
of mnemonic passwords against offline attacks grows linearly
CommunicationsoftheACM,2015,toappear.
with the password length. On the other hand, if the adversary
[5] S.Boztas,Entropies,GuessingandCryptography. RoyalMelbourne
canonlyperformafewguesses,ourresultssuggestthatlonger InstituteofTechnology,Dept.ofMathematicsMelbourne,1999.
passwords provide no further advantage.
[6] W. E. Burr, D. F. Dodson, E. M. Newton, R. A. Perlner, W. T.
Polk, S. Gupta, and E. A. Nabbus, NIST Special Publication 800-63-
Using statistical modeling, this paper provides with Ta-
2: Electronic Authentication Guideline. Gaithersburg, MD: National
ble VII detailed estimates of the strength of mnemonic pass- InstituteofStandardsandTechnology,2013.
words against offline attacks for different minimum pass- [7] S. F. Chen and J. Goodman, “An Empirical Study of Smoothing
word lengths and password generation rules. This table aims Techniques for Language Modeling,” Computer Speech & Language,
to replace for mnemonic passwords the inaccurate “rule of vol.13,no.4,pp.359–393,1999.
thumb”forstrengthcalculationthatwasusedpreviously.With [8] G. V. Cormack, M. D. Smucker, and C. L. Clarke, “Efficient and
this table, we compare mnemonic passwords to a password Effective Spam Filtering and Re-ranking for Large Web Datasets,”
ComputingResearchRepository,vol.abs/1004.5168,2010.
generation approach that performs repeated uniform sampling
[9] M. Dell’Amico, P. Michiardi, and Y. Roudier, “Password Strength:
from a dictionary and found that mnemonic passwords are
An Empirical Analysis,” in Proceedings of the 29th Conference on
weaker against online, but stronger against offline attacks. Information Communications, ser. INFOCOM’10. Piscataway, NJ,
USA:IEEEPress,2010,pp.983–991.
The analysis of the password generation rules is limited to [10] D.C.FeldmeierandP.R.Karn,“UNIXPasswordSecurity-TenYears
the strength of the corresponding password distributions and Later,”inProceedingsonAdvancesinCryptology,ser.CRYPTO’89.
NewYork,NY,USA:Springer-VerlagNewYork,Inc.,1989,pp.44–63.
ignores that the different rules are associated with different
[11] R.Flesch,“ANewReadabilityYardstick,”Journalofappliedpsychol-
costs for the human. For example, the best generation rule
ogy,vol.32,no.3,p.221,1948.
requires the human to memorize a twice as long sentence.
[12] W. A. Gale, “Good-Turing Smoothing Without Tears,” Journal of
Furthermore, already having a certain generation rule in mind
QuantitativeLinguistics,vol.2,1995.
will likely have an influence on mnemonic choice. For in-
[13] M. Ghazvininejad and K. Knight, “How to Memorize a Random 60-
stance,ifthehumanwantstousearulethatincorporatesword Bit String,” in Proceedings of the 2015 Conference of the North
prefixreplacements,hemaylimittheconsideredmnemonicsto American Chapter of the Association for Computational Linguistics:
such where he can actually perform a replacement operation. Human Language Technologies. Denver, Colorado: Association for
ComputationalLinguistics,May–June2015,pp.1569–1575.
12[14] GoogleInc.,“Secureyourpasswords,”LastaccessedMay2016,www. [28] M. L. Mazurek, S. Komanduri, T. Vidas, L. Bauer, N. Christin, L. F.
google.com/safetycenter/everyone/start/password/. Cranor, P. G. Kelley, R. Shay, and B. Ur, “Measuring Password
[15] T. Gottron and N. Lipka, “A Comparison of Language Identification GuessabilityforanEntireUniversity,”inProceedingsofthe2013ACM
ApproachesonShort,Query-StyleTexts,”inAdvancesinInformation SIGSAC Conference on Computer & Communications Security, ser.
Retrieval, ser. Lecture Notes in Computer Science, C. Gurrin, Y. He, CCS’13. NewYork,NY,USA:ACM,2013,pp.173–186.
G.Kazai,U.Kruschwitz,S.Little,T.Roelleke,S.Rüger,andK.van [29] R.Munroe,“PasswordStrength,”August2011,www.xkcd.com/936/.
Rijsbergen, Eds. Springer Berlin Heidelberg, 2010, vol. 5993, pp. [30] A.NarayananandV.Shmatikov,“FastDictionaryAttacksonPasswords
611–614. UsingTime-spaceTradeoff,”inProceedingsofthe12thACMConfer-
[16] E. Grosse and M. Upadhyay, “Authentication at scale,” IEEE enceonComputerandCommunicationsSecurity,ser.CCS’05. New
Security and Privacy, vol. 11, pp. 15–22, 2013. [Online]. Avail- York,NY,USA:ACM,2005,pp.364–372.
able:http://www.computer.org/cms/Computer.org/ComputingNow/pdfs/ [31] N. Perlroth, “How to Devise Passwords that Drive Hackers Away,”
AuthenticationAtScale.pdf November 2012, www.nytimes.com/2012/11/08/technology/personaltech/
[17] P. Grzybek, “History and Methodology of Word Length Studies,” in how-to-devise-passwords-that-drive-hackers-away.html.
ContributionstotheScienceofTextandLanguage:WordLengthStudies [32] J.Pliam,“OntheIncomparabilityofEntropyandMarginalGuesswork
andRelatedIssues,P.Grzybek,Ed. Dordrecht,NL:Kluwer,2007. in Brute-Force Attacks,” in Progress in Cryptology —INDOCRYPT
[18] T. Hornby, “Password Policy Hall of Shame,” Last updated February 2000,ser.LectureNotesinComputerScience,B.RoyandE.Okamoto,
2014,www.defuse.ca/password-policy-hall-of-shame.htm. Eds. SpringerBerlinHeidelberg,2000,vol.1977,pp.67–79.
[19] S.JeyaramanandU.Topkara,“Havethecakeandeatittoo-infusing [33] A. G. Reinhold, “The Diceware Passphrase Home Page,” May 2016,
usabilityintotext-passwordbasedauthenticationsystems,”inProceed- www.world.std.com/~reinhold/diceware.html.
ings of the 21st Annual Computer Security Applications Conference, [34] P.SparellandM.Simovits,“LinguisticCrackingofPassphrases,”2016,
ser. ACSAC ’05. Washington, DC, USA: IEEE Computer Society, (Talk)RSAConference.
2005,pp.473–482.
[35] A. Stolcke, D. Yuret, and N. Madnani, “SRILM-FAQ,” Last accessed
[20] K.A.Juang,S.Ranganayakulu,andJ.S.Greenstein,“Usingsystem- May2016,www.speech.sri.com/projects/srilm/manpages/srilm-faq.7.html.
generatedmnemonicstoimprovetheusabilityandsecurityofpassword
[36] TheLemurProject,“TheClueWeb12Dataset,”2012,www.lemurproject.
authentication,”inProceedingsoftheHumanFactorsandErgonomics
org/clueweb12.
SocietyAnnualMeeting,vol.56,no.1,2012,pp.506–510.
[37] U.Topkara,M.J.Atallah,andM.Topkara,“PasswordsDecay,Words
[21] P.G.Kelley,S.Komanduri,M.L.Mazurek,R.Shay,T.Vidas,L.Bauer,
Endure: Secure and Re-usable Multiple Password Mnemonics,” in
N. Christin, L. F. Cranor, and J. Lopez, “Guess Again (and Again
Proceedingsofthe2007ACMSymposiumonAppliedComputing,ser.
and Again): Measuring Password Strength by Simulating Password-
SAC’07. NewYork,NY,USA:ACM,2007,pp.292–299.
Cracking Algorithms,” in IEEE Symposium on Security and Privacy.
IEEEComputerSociety,2012,pp.523–537. [38] K.-P.L.Vu,B.-L.B.Tai,A.Bhargav,E.E.Schultz,andR.W.Proctor,
“PromotingMemorabilityandSecurityofPasswordsThroughSentence
[22] A.Kerckhoffs,“LaCryptographieMilitaire,”JSM,vol.9,pp.161–191,
Generation,” in Proceedings of the Human Factors and Ergonomics
February1883.
Society Annual Meeting, vol. 48, no. 13. SAGE Publications, 2004,
[23] A. Kittur, E. H. Chi, and B. Suh, “Crowdsourcing User Studies with pp.1478–1482.
MechanicalTurk,”inProceedingsoftheSIGCHIConferenceonHuman
[39] M. Weir, S. Aggarwal, M. P. Collins, and H. Stern, “Testing metrics
Factors in Computing Systems, ser. CHI ’08. New York, NY, USA:
for password creation policies by attacking large sets of revealed
ACM,2008,pp.453–456.
passwords,” in ACM Conference on Computer and Communications
[24] C. Kohlschütter, P. Fankhauser, and W. Nejdl, “Boilerplate Detection Security,E.Al-Shaer,A.D.Keromytis,andV.Shmatikov,Eds. ACM,
Using Shallow Text Features,” in Proceedings of the Third ACM 2010,pp.162–175.
InternationalConferenceonWebSearchandDataMining,ser.WSDM
[40] I. H. Witten and T. Bell, “The zero-frequency problem: estimating
’10. NewYork,NY,USA:ACM,2010,pp.441–450.
the probabilities of novel events in adaptive text compression,” IEEE
[25] N. Kumar, “Password in Practice: An Usability Survey,” Journal of Transactions on Information Theory, vol. 37, no. 4, pp. 1085–1094,
GlobalResearchinComputerScience,vol.2,no.5,pp.107–112,2011. July1991.
[26] C. Kuo, S. Romanosky, and L. F. Cranor, “Human selection of [41] J. Yan, A. Blackwell, R. Anderson, and A. Grant, “Password Memo-
mnemonicphrase-basedpasswords,”inSOUPS,ser.ACMInternational rability and Security: Empirical Results,” IEEE Security and Privacy,
ConferenceProceedingSeries,L.F.Cranor,Ed.,vol.149. ACM,2006, vol.2,no.5,pp.25–31,Sep.2004.
pp.67–78.
[42] W. Yang, N. Li, O. Chowdhury, A. Xiong, and R. W. Proctor, “An
[27] J.Ma,W.Yang,M.Luo,andN.Li,“AStudyofProbabilisticPassword Empirical Study of Mnemonic Sentence-based Password Generation
Models,”inProceedingsofthe2014IEEESymposiumonSecurityand Strategies,” in Proceedings of the 2016 ACM SIGSAC Conference on
Privacy,ser.SP’14. Washington,DC,USA:IEEEComputerSociety, Computer and Communications Security, ser. CCS ’16. New York,
2014,pp.689–704. NY,USA:ACM,2016,pp.1216–1229.
13