M M D : Detecting Android Malware by
A A ROID
Building Markov Chains of Behavioral Models
Enrico Mariconti†, Lucky Onwuzurike†, Panagiotis Andriotis‡,
Emiliano De Cristofaro†, Gordon Ross†, and Gianluca Stringhini†
†University College London, ‡University of the West of England
{enrico.mariconti.14,lucky.onwuzurike.13,e.decristofaro,g.ross,g.stringhini}@ucl.ac.uk,
panagiotis.andriotis@uwe.ac.uk
Abstract—The rise in popularity of the Android platform users to install apps that come from third-party market places,
has resulted in an explosion of malware threats targeting it. As which might not perform any malware checks at all, or anyway
bothAndroidmalwareandtheoperatingsystemitselfconstantly not as accurately [67].
evolve,itisverychallengingtodesignrobustmalwaremitigation
techniques that can operate for long periods of time without As a result, the research community has devoted significant
the need for modifications or costly re-training. In this paper, attention to malware detection on Android. Previous work
wepresent MAMADROID,anAndroidmalwaredetectionsystem has often relied on the permissions requested by apps [20],
that relies on app behavior. MAMADROID builds a behavioral
[46], using models built from malware samples. This strategy,
model, in the form of a Markov chain, from the sequence of
however, is prone to false positives, since there are often
abstractedAPIcallsperformedbyanapp,andusesittoextract
legitimate reasons for benign apps to request permissions
features and perform classification. By abstracting calls to their
classified as dangerous [20]. Another approach, used by
packages or families, MAMADROID maintains resilience to API
changes and keeps the feature set size manageable. We evaluate DROIDAPIMINER [2], is to perform classification based on
its accuracy on a dataset of 8.5K benign and 35.5K malicious API calls frequently used by malware. However, relying on
apps collected over a period of six years, showing that it not the most common calls observed during training prompts the
only effectively detects malware (with up to 99% F-measure), need for constant retraining, due to the evolution of malware
but also that the model built by the system keeps its detection and the Android API alike. For instance, “old” calls are often
capabilities for long periods of time (on average, 86% and 75% deprecated with new API releases, so malware developers may
F-measure,respectively,oneandtwoyearsaftertraining).Finally,
switchtodifferentcallstoperformsimilaractions,whichaffects
we compare against DROIDAPIMINER, a state-of-the-art system
DROIDAPIMINER’seffectivenessduetoitsuseofspecificcalls.
that relies on the frequency of API calls performed by apps,
showing that MAMADROID significantly outperforms it.
In this paper, we present a novel malware detection system
for Android that instead relies on the sequence of abstracted
I. INTRODUCTION APIcallsperformedbyanappratherthantheiruseorfrequency,
aiming to capture the behavioral model of the app. Our system,
In the first quarter of 2016, 85% of smartphone sales were which we call MAMADROID, abstracts API calls to either
devices running Android [49]. Due to its popularity, cyber- the package name of the call (e.g., java.lang) or its source
criminals have increasingly targeted this ecosystem [17], as (e.g., java, android, google), which we refer to as family.
malwarerunningonmobiledevicescanbeparticularlylucrative Abstraction provides resilience to API changes in the Android
–e.g.,allowingattackerstodefeattwofactorauthentication[51], framework as families and packages are added and removed
[53] or trigger leakage of sensitive information [27]. Detecting lessfrequentlythansingleAPIcalls.Atthesametime,thisdoes
malware on mobile devices presents additional challenges com- notabstractawaythebehaviorofanapp:forinstance,packages
pared to desktop/laptop computers: smartphones have limited includeclassesandinterfacesusedtoperformsimilaroperations
battery life, making it infeasible to use traditional approaches onsimilarobjects,sowecanmodelthetypesofoperationsfrom
requiring constant scanning and complex computation [43]. the package name, independently of the underlying classes and
Therefore, Android malware detection is typically performed interfaces. For example, we know that the java.io package
by Google in a centralized fashion, i.e., by analyzing apps is used for system I/O and access to the file system, even
submitted to the Play Store using a tool called Bouncer [40]. though there are different classes and interfaces provided by
However, many malicious apps manage to avoid detection [1], the package for such operations.
and anyway Android’s openness enables manufacturers and
After abstracting the calls, MAMADROID analyzes the
sequence of API calls performed by an app, aiming to model
Permission to freely reproduce all or part of this paper for noncommercial the app’s behavior. Our intuition is that malware may use calls
purposes is granted provided that copies bear this notice and the full citation
for different operations, and in a different order, than benign
on the first page. Reproduction for commercial purposes is strictly prohibited
without the prior written consent of the Internet Society, the first-named author apps. For example, android.media.MediaRecorder can be used
(for reproduction of an entire paper only), and the author’s employer if the by any app that has permission to record audio, but the call
paper was prepared within the scope of employment. sequence may reveal that malware only uses calls from this
NDSS ’17, 26 February - 1 March 2017, San Diego, CA, USA
class after calls to getRunningTasks(), which allows recording
Copyright 2017 Internet Society, ISBN 1-891562-46-0
http://dx.doi.org/10.14722/ndss.2017.23353 conversations[65],asopposedtobenignappswherecallsfromthe class may appear in any order. Relying on the sequence
of abstracted calls allows us to model behavior in a more
complex way than previous work, which only looked at the
presenceorabsenceofcertainAPIcallsorpermissions[2],[5],
while still keeping the problem tractable [33]. MAMADROID Call Graph Sequence Markov Chain Classification
builds a statistical model to represent the transitions between ? Extraction (1) Extraction (2) Modeling (3) (4)
the API calls performed by an app, specifically, we model
these transitions as Markov chains, and use them to extract
featuresandperformclassification(i.e.,labelingappsasbenign Fig. 1: Overview of MAMADROID operation. In (1), it extracts the
or malicious). Calls are abstracted to either their package or call graph from an Android app, next, it builds the sequences of
their family, i.e., MAMADROID operates in one of two modes, (abstracted) API calls from the call graph (2). In (3), the sequences
depending on the abstraction granularity. of calls are used to build a Markov chain and a feature vector for
that app. Finally, classification is performed in (4), labeling the app
We present a detailed evaluation of both classification as benign or malicious.
accuracy (using F-measure, precision, and recall) and runtime
performance of MAMADROID, using a dataset of almost 44K
apps (8.5K benign and 35.5K malware samples). We include
then, Section III introduces the datasets used in our evaluation
a mix of older and newer apps, from October 2010 to May
(Section IV), while Section V further discusses our results
2016, verifying that our model is robust to changes in Android
as well as its limitations. After reviewing related work in
malware samples and APIs. To the best of our knowledge,
Section VI, the paper concludes in Section VII.
this is the largest malware dataset used to evaluate an Android
malwaredetectionsysteminaresearchpaper.Ourexperimental
analysis shows that MAMADROID can effectively model both
II. THEMAMADROIDSYSTEM
benign and malicious Android apps, and perform an efficient A. Overview
classification on them. Compared to other systems such as
DROIDAPIMINER [2], our approach allows us to account for We now introduce MAMADROID, a novel system for
changes in the Android API, without the need to frequently Android malware detection. MAMADROID characterizes the
retrain the classifier. transitions between different API calls performed by Android
apps – i.e., the sequence of API calls. It then models these
We show that MAMADROID is able to effectively detect
transitions as Markov chains, which are in turn used to extract
unknown malware samples not only in the “present,” (with
features for machine learning algorithms to classify apps as
F-measure up to 99%) but also consistently over the years (i.e.,
benign or malicious. MAMADROID does not actually use the
when the system is trained on older samples and classification sequence of raw API calls, but abstracts each call to either its
performed over newer ones), as it keeps an average detection
package or its family. For instance, the API call getMessage()
accuracy, evaluated in terms of F-measure, of 86% after one
is parsed as:
year and 75% after two years (as opposed to 46% and 42%
package
achieved by DROIDAPIMINER [2]). We also highlight that (cid:122) (cid:125)(cid:124) (cid:123)
java.lang.Throwable: String getMessage()
when the system is not efficient anymore (when the test set is
(cid:124)(cid:123)(cid:122)(cid:125)
newer than the training set by more than two years), it is as a family
result of MAMADROID having low recall, but maintaining (cid:124) AP(cid:123) I(cid:122)
call
(cid:125)
high precision. We also do the opposite, i.e., training on
newer samples and verifying that the system can still detect Given these two different types of abstractions, we have
old malware. This is particularly important as it shows that two modes of operation for MAMADROID, each using one
MAMADROID can detect newer threats, while still identifying of the types of abstraction. We test both, highlighting their
malware samples that have been in the wild for some time. advantages and disadvantages — in a nutshell, the abstraction
to family is more lightweight, while that to package is more
Summary of Contributions. First, we introduce a novel fine-grained.
approach,implementedinatoolcalledMAMADROID,todetect
Android malware by abstracting API calls to their package
MAMADROID’s operation goes through four phases, as
depicted in Fig. 1. First, we extract the call graph from each
and family, and using Markov chains to model the behavior
app by using static analysis (1), next we obtain the sequences
of the apps through the sequences of API calls. Second, we
of API calls for the app using all unique nodes in the call
can detect unknown samples on the same year of training
graph and associating, to each node, all its child nodes (2). As
with an F-measure of 99%, but also years after training the
mentioned, we abstract a call to either its package or family.
system, meaning that MAMADROID does not need continuous
re-training. Our system is scalable as we model every single
Finally,bybuildingonthesequences,MAMADROIDconstructs
aMarkovchainmodel(3),withthetransitionprobabilitiesused
app independently from the others and can easily append app
as the feature vector to classify the app as either benign or
features in a new training set. Finally, compared to previous
malware using a machine learning classifier (4). In the rest of
work [2], MAMADROID achieves significantly higher accuracy
this section, we discuss each of these steps in detail.
with reasonably fast running times, while also being more
robust to evolution in malware development and changes in
the Android API. B. Call Graph Extraction
Paper Organization. The rest of the paper is organized as The first step in MAMADROID is to extract the app’s call
follows. The next section presents the MAMADROID system, graph. We do so by performing static analysis on the app’s
2package com.fa.c; com.fa.c.RootCommandExecutor:
Execute()
import android.content.Context;
import android.os.Environment;
import android.util.Log;
import com.stericson.RootShell.execution.Command;
import com.stericson.RootShell.execution.Shell; android.util.Log: com.stericson.RootTools.RootTools:
import com.stericson.RootTools.RootTools; d() getShell()
import java.io.File;
public class RootCommandExecutor { java.lang.Throwable:
public static boolean Execute(Context paramContext) { getMessage()
paramContext = new Command(0, new String[] { "cat " + Environment. com.stericson.RootShell.execution.Shell:
getExternalStorageDirectory().getAbsolutePath() + File.separator + Utilities add()
.GetWatchDogName(paramContext) + " > /data/" + Utilities.GetWatchDogName(
paramContext), "cat " + Environment.getExternalStorageDirectory().
getAbsolutePath() + File.separator + Utilities.GetExecName(paramContext) + " Fig. 3: Call graph of the API calls in the try/catch block of Fig. 2.
> /data/" + Utilities.GetExecName(paramContext), "rm " + Environment.
getExternalStorageDirectory().getAbsolutePath() + File.separator + Utilities (Return types and parameters are omitted to ease presentation).
.GetWatchDogName(paramContext), "rm " + Environment.
getExternalStorageDirectory().getAbsolutePath() + File.separator + Utilities
.GetExecName(paramContext), "chmod 777 /data/" + Utilities.GetWatchDogName(
paramContext), "chmod 777 /data/" + Utilities.GetExecName(paramContext), "/
data/" + Utilities.GetWatchDogName(paramContext) + " " + Utilities. In this phase, MAMADROID operates as follows. First, it
GetDeviceInfoCommandLineArgs(paramContext) + " /data/" + Utilities.
GetExecName(paramContext) + " " + Environment.getExternalStorageDirectory(). identifies a set of entry nodes in the call graph, i.e., nodes with
getAbsolutePath() + File.separator + Utilities.GetExchangeFileName(
paramContext) + " " + Environment.getExternalStorageDirectory(). no incoming edges (for example, the Execute method in the
getAbsolutePath() + File.separator + " " + Utilities.GetPhoneNumber( snippet from Fig. 2 is the entry node if there is no incoming
paramContext) });
try { edge from any other call in the app). Then, it enumerates the
RootTools.getShell(true).add(paramContext);
paths reachable from each entry node. The sets of all paths
return true;
} identified during this phase constitutes the sequences of API
catch (Exception paramContext) {
Log.d("CPS", paramContext.getMessage()); calls which will be used to build a Markov chain behavioral
} model and to extract features (see Section II-D).
return false;
}
}
Abstracting Calls to Families/Packages. Rather than analyz-
Fig. 2: Codesnippetfromamaliciousapp(com.g.o.speed.memboost) ing raw API calls, we build MAMADROID to work at a higher
executing commands as root.
level, and operate in one of two modes by abstracting each
call to either its package or family. This allows the system
to be resilient to API changes and achieve scalability. In fact,
apk.1 Specifically, we use a Java optimization and analysis our experiments, presented in Section III, show that, from a
framework, Soot [52], to extract call graphs and FlowDroid [6] dataset of 44K apps, we extract more than 10 million unique
to ensure contexts and flows are preserved. API calls, which would result in a very large number of nodes,
with the corresponding graphs (and feature vectors) being quite
To better clarify the different steps involved in our system,
sparse. Since as we will see the number of features used by
we employ a “running example,” using a real-world malware
MAMADROID is the square of the number of nodes, having
sample. Specifically, Fig. 2 lists a class extracted from the
more than 10 million nodes would result in an impractical
decompiledapkofmalwaredisguisedasamemoryboosterapp
computational cost.
(with package name com.g.o.speed.memboost), which executes
commands (rm, chmod, etc.) as root.2 To ease presentation, we When operating in package mode, we abstract an API
focusontheportionofthecodeexecutedinthetry/catchblock. call to its package name using the list of Android pack-
Theresultingcallgraphofthetry/catchblockisshowninFig.3. ages3, which as of API level 24 (the current version as of
Note that, for simplicity, we omit calls for object initialization, September 2016) includes 243 packages, as well as 95 from
return types and parameters, as well as implicit calls in a the Google API.4 Moreover, we abstract developer-defined
method. Additional calls that are invoked when getShell(true) packages (e.g., com.stericson.roottools) as well as obfuscated
is called are not shown, except for the add() method that is ones (e.g. com.fa.a.b.d), respectively, as self-defined and
directly called by the program code, as shown in Fig. 2. obfuscated. Note that we label an API call’s package
as obfuscated if we cannot tell what its class implements,
extends, or inherits, due to identifier mangling [47]. When
C. Sequence Extraction
operating in family mode, we abstract to nine possible families,
Next, we extract the sequences of API calls from the call i.e., android, google, java, javax, xml, apache, junit,
graph. Since MAMADROID uses static analysis, the graph json, dom, which correspond to the android.*, com.google.*,
obtained from Soot represents the sequence of functions that java.*, javax.*, org.xml.*, org.apache.*, junit.*, org.json, and
are potentially called by the program. However, each execution org.w3c.dom.* packages. Again, API calls from developer-
of the app could take a specific branch of the graph and only defined and obfuscated packages are abstracted to families
execute a subset of the calls. For instance, when running the labeledasself-definedandobfuscated,respectively.Over-
code in Fig. 2 multiple times, the Execute method could be all, there are 340 (243+95+2) possible packages and 11 (9+2)
followedby different calls, e.g.,getShell() in the tryblock only families.InFig.4,weshowthesequenceofAPIcallsobtained
or getShell() and then getMessage() in the catch block. fromthecallgraphinFig.3.Wealsoreport,insquarebrackets,
the family and the package to which the call is abstracted.
1ThestandardAndroidarchivefileformatcontainingallfiles,includingthe
Javabytecode,makinguptheapp. 3https://developer.android.com/reference/packages.html
2https://www.hackread.com/ghost-push-android-malware/ 4https://developers.google.com/android/reference/packages
3com.fa.c.RootCommandExecutor: com.stericson.RootTools.RootTools: com.stericson.RootShell.
Execute() getShell() execution.Shell:add()
[self-defined,self-defined] [self-defined,self-defined] [self-defined,self-defined]
com.fa.c.RootCommandExecutor: android.util.Log:
Execute() d()
[self-defined,self-defined] [android.util,android]
com.fa.c.RootCommandExecutor: java.lang.Throwable:
Execute() getMessage()
[self-defined,self-defined] [java.lang,java]
Fig. 4: Sequence of API calls extracted from the call graphs in Fig. 3, with the corresponding package/family abstraction in square brackets.
calls in order to deceive signature-based systems (see Sec-
self-defined self-defined tion VI). In fact, MAMADROID considers all possible calls –
0.5 0.5
i.e., all the branches originating from a node – in the Markov
chain, so adding calls would not significantly change the
probabilities of transitions between nodes (specifically, families
0.25 0.25 0.25 0.25 or packages, depending on the operational mode) for each app.
Feature Extraction. Next, we use the probabilities of transi-
tioningfromonestate(abstractedcall)toanotherintheMarkov
java.lang android.util java android chain as the feature vector of each app. States that are not
presentinachainarerepresentedas0inthefeaturevector.Also
(a) (b)
note that the vector derived from the Markov chain depends on
Fig. 5: Markov chains originating from the call sequence example in the operational mode of MAMADROID. With families, there
Section II-C when using packages (a) or families (b).
are 11 possible states, thus 121 possible transitions in each
chain,while,whenabstractingtopackages,thereare340states
and 115,600 possible transitions.
D. Markov-chain Based Modeling
We also apply Principal Component Analysis (PCA) [32],
Next, MAMADROID builds feature vectors, used for classi-
which performs feature selection by transforming the feature
fication,basedontheMarkovchainsrepresentingthesequences
space into a new space made of components that are a linear
of extracted API calls for an app. Before discussing this in
combination of the original features. The first components
detail, we review the basic concepts of Markov chains.
contain as much variance (i.e., amount of information) as
Markovchainsarememorylessmodelswheretheprobability possible.Thevarianceisgivenaspercentageofthetotalamount
of transitioning from a state to another only depends on the of information of the original feature space. We apply PCA to
current state [39]. Markov chains are often represented as the feature set in order to select the principal components, as
a set of nodes, each corresponding to a different state, and PCA transforms the feature space into a smaller one where the
a set of edges connecting one node to another labeled with variance is represented with as few components as possible,
the probability of that transition. The sum of all probabilities thus considerably reducing computation/memory complexity.
associated to all edges from any node (including, if present, Furthermore, the use of PCA could also improve the accuracy
an edge going back to the node itself) is exactly 1.The set of the classification, by taking misleading features out of the
of possible states of the Markov chain is denoted as S. If S j feature space, i.e., those that make the classifier perform worse.
and S are two connected states, P denotes the probability
k jk
of transition from S to S . P is given by the number of
j k jk
occurrences (O jk) of state S k after state S j, divided by O ji E. Classification
for all states i in the chain, i.e., P = Ojk .
jk (cid:80) i∈SOji The last step is to perform classification, i.e., labeling apps
Building the model. MAMADROID uses Markov chains to aseitherbenignormalware.Tothisend,wetestMAMADROID
modelappbehavior,byevaluatingeverytransitionbetweencalls. usingdifferentclassificationalgorithms:RandomForests[9],1-
More specifically, for each app, MAMADROID takes as input NearestNeighbor(1-NN)[22],3-NearestNeighbor(3-NN)[22],
thesequenceofabstractedAPIcallsofthatapp–i.e.,packages and Support Vector Machines (SVM) [29]. Each model is
or families, depending on the selected mode of operation – and trained using the feature vector obtained from the apps in a
builds a Markov chain where each package/family is a state training sample. Results are presented and discussed in Section
and the transitions represent the probability of moving from IV, and have been validated by using 10-fold cross validation.
one state to another. For each Markov chain, state S is the
0 Also note that, due to the different number of features used
entry point from which other calls are made in a sequence.
infamily/packagemodes,weusetwodistinctconfigurationsfor
As an example, Fig. 5 illustrates the two Markov chains built
the Random Forests algorithm. Specifically, when abstracting
using packages and families, respectively, from the sequences
to families, we use 51 trees with maximum depth 8, while,
reported in Fig. 4.
with packages, we use 101 trees of maximum depth 64. To
We argue that considering single transitions is more robust tune Random Forests we followed the methodology applied
against attempts to evade detection by inserting useless API in [7].
4Category Name DateRange #Samples #Samples #Samples
(APICalls) (CallGraph)
oldbenign Apr2013 –Nov2013 5,879 5,837 5,572
Benign
newbenign Mar2016 –Mar2016 2,568 2,565 2,465
TotalBenign: 8,447 8,402 8,037
drebin Oct2010 –Aug2012 5,560 5,546 5,538
2013 Jan2013 –Jun2013 6,228 6,146 6,123
Malware 2014 Jun2013 –Mar2014 15,417 14,866 14,827
2015 Jan2015 –Jun2015 5,314 5,161 4,725
2016 Jan2016 –May2016 2,974 2,802 2,657
TotalMalware: 35,493 34,521 33,870
TABLE I: Overview of the datasets used in our experiments.
III. DATASETS 1.0
In this section, we introduce the datasets used in the
evaluation of MAMADROID (presented later in Section IV), 0.8
which include 43,940 apk files – 8,447 benign and 35,493
malware samples. We include a mix of older and newer apps, 0.6
ranging from October 2010 to May 2016, as we aim to verify
that MAMADROID is robust to changes in Android malware
0.4
samples as well as APIs. To the best of our knowledge, we are
leveraging the largest dataset of malware samples ever used in
a research paper on Android malware detection. 0.2
Benign Samples. Our benign datasets consist of two sets of
0.0
0 5000 10000 15000 20000 25000 30000 35000
samples: (1) one, which we denote as oldbenign, includes
#API Calls
5,879 apps collected by PlayDrone [55] between April and
November 2013, and published on the Internet Archive5 on
August 7, 2014; and (2) another, newbenign, obtained by
downloadingthetop100appsineachofthe29categoriesonthe
Google Play store6 as of March 7, 2016, using the googleplay-
api tool.7 Due to errors encountered while downloading some
apps, we have actually obtained 2,843 out of 2,900 apps. Note
that 275 of these belong to more than one category, therefore,
the newbenign dataset ultimately includes 2,568 unique apps.
Android Malware Samples. The set of malware samples
includes apps that were used to test DREBIN [5], dating back
to October 2010 – August 2012 (5,560), which we denote as
drebin, as well as more recent ones that have been uploaded
on the VirusShare8 site over the years. Specifically, we gather
from VirusShare, respectively, 6,228, 15,417, 5,314, and 2,974
samples from 2013, 2014, 2015, and 2016. We consider each
of these datasets separately for our analysis.
API Calls and Call Graphs. For each app in our datasets,
we extract the list of API calls, using Androguard9, since, as
explained in Section IV-E, these constitute the features used by
DROIDAPIMINER [2], against which we compare our system.
Due to Androguard failing to decompress some of the apks,
bad CRC-32 redundancy checks, and errors during unpacking,
we are not able to extract the API calls for all the samples,
but only for 42,923 (8,402 benign, 34,521 malware) out of the
43,940 apps (8,447 benign, 35,493 malware) in our datasets.
Also, to extract the call graph of each apk, we use Soot. Note
that for some of the larger apks, Soot requires a non-negligible
amount of memory to extract the call graph, so we allocate
5https://archive.org/details/playdrone-apk-e8
6https://play.google.com/store
7https://github.com/egirault/googleplay-api
8https://virusshare.com/
9https://github.com/androguard/androguard
FDC
2013
2014
2015
2016
drebin
newbenign
oldbenign
Fig. 6: CDF of the number of API calls in different apps in each
dataset.
16GB of RAM to the Java VM heap space. We find that for
2,027 (410 benign + 1,617 malware) samples, Soot is not able
to complete the extraction due to it failing to apply the jb
phase as well as reporting an error in opening some zip files
(i.e., the apk). The jb phase is used by Soot to transform Java
bytecode into jimple intermediate representation (the primary
IR of Soot) for optimization purposes. Therefore, we exclude
these apps in our evaluation and discuss this limitation further
in Section V-C. In Table I, we provide a summary of our seven
datasets, reporting the total number of samples per dataset, as
well as those for which we are able to extract the API calls
(second-to-last column) and the call graphs (last column).
Characterization of the Datasets. Aiming to shed light on
the evolution of API calls in Android apps, we also performed
some measurements over our datasets. In Fig. 6, we plot the
Cumulative Distribution Function (CDF) of the number of
unique API calls in the apps in different datasets, highlighting
thatnewerapps,bothbenignandmalicious,areusingmoreAPI
calls overall than older apps. This indicates that as time goes
by, Android apps become more complex. When looking at the
fractionofAPIcallsbelongingtospecificfamilies,wediscover
someinterestingaspectsofAndroidappsdevelopedindifferent
years. In particular, we notice that API calls to the android
family become less prominent as time passes (Fig. 7(a)), both
in benign and malicious datasets, while google calls become
more common in newer apps (Fig. 7(b)).
In general, we conclude that benign and malicious apps
show the same evolutionary trends over the years. Malware,
however, appears to reach the same characteristics (in terms
of level of complexity and fraction of API calls from certain
families) as legitimate apps with a few years of delay.
51.0
0.8
0.6
0.4
0.2
0.0
0.0 0.2 0.4 0.6 0.8 1.0
Fraction of Calls
FDC
2013
2014
2015
2016
drebin
newbenign
oldbenign
(a) android
1.0
0.8
0.6
0.4
0.2
0.0
0.0 0.2 0.4 0.6 0.8 1.0
Fraction of Calls
FDC
1.0
0.8
0.6
0.4
0.2
0.0
0.2
0.4
0.6
0.8
1.0 0.5 0.0 0.5 1.0 1.5
PCA1
2013
2014
2015
2016
drebin
newbenign
oldbenign
(b) google
Fig. 7: CDFsofthepercentageofandroidandgooglefamilycalls
in different apps in each dataset.
Principal Component Analysis. Finally, we apply PCA to
select the two most important PCA components. We plot
and compare the positions of the two components for benign
(Fig.8(a))andmalicioussamples(Fig.8(b)).AsPCAcombines
the features into components, it maximizes the variance of the
distribution of samples in these components, thus, plotting the
positions of the samples in the components shows that benign
apps tend to be located in different areas of the components
space,dependingonthedataset,whilemalwaresamplesoccupy
similar areas but with different densities. These differences
highlight a different behavior between benign and malicious
samples, and these differences should also be found by the
machine learning algorithms used for classification.
IV. EVALUATION
We now present a detailed experimental evaluation of
MAMADROID. Using the datasets summarized in Table I, we
perform four sets of experiments: (1) we analyze the accuracy
of MAMADROID’s classification on benign and malicious
samples developed around the same time; (2) we evaluate its
robustnesstotheevolutionofmalwareaswellasoftheAndroid
framework by using older datasets for training and newer ones
for testing (and vice-versa); (3) we measure MAMADROID’s
runtime performance to assess its scalability; and, finally, (4)
wecompareagainstDROIDAPIMINER[2],amalwaredetection
system that relies on the frequency of API calls.
2ACP
oldbenign
newbenign
(a) benign
1.0
0.8
0.6
0.4
0.2
0.0
0.2
0.4
0.6
0.8
1.0 0.5 0.0 0.5 1.0 1.5
PCA1
2ACP
drebin
2013
2014
2015
2016
(b) malware
Fig. 8: Positions of benign vs malware samples in the feature space
of the first two components of the PCA (family mode).
A. Preliminaries
When implementing MAMADROID in family mode, we
exclude the json and dom families because they are almost
neverusedacrossallourdatasets,andjunit,whichisprimarily
used for testing. In package mode, to avoid mislabeling when
self-defined APIs have “android” in the name, we split the
android package into its two classes, i.e., android.R and
android.Manifest. Therefore, in family mode, there are 8
possible states, thus 64 features, whereas, in package mode,
we have 341 states and 116,281 features (cf. Section II-D).
As discussed in Section II-E, we use four different ma-
chine learning algorithms for classification – namely, Random
Forests [9], 1-NN [22], 3-NN [22], and SVM [29]. Since both
accuracy and speed are worse with SVM than with the other
threealgorithms,weomitresultsobtainedwithSVM.Toassess
theaccuracyoftheclassification,weusethestandardF-measure
metric, i.e.:
precision·recall
F =2·
precision+recall
where precision = TP/(TP+FP) and recall = TP/(TP+FN).
TP denotes the number of samples correctly classified as
malicious, while FP an FN indicate, respectively, the number
of samples mistakenly identified as malicious and benign.
Finally, note that all our experiments perform 10-fold cross
validation using at least one malicious and one benign dataset
from Table I. In other words, after merging the datasets, the
61.0
0.8
0.6
0.4
0.2
0.0
Drebin & 2013 & 2014 & 2014 & 2015 & 2016 &
OldBenign OldBenign OldBenign Newbenign Newbenign Newbenign
erusaem-F
1.0
RF
1-NN
3-NN
0.8
0.6
0.4
0.2
0.0
Drebin & 2013 & 2014 & 2014 & 2015 & 2016 &
OldBenign OldBenign OldBenign Newbenign Newbenign Newbenign
Fig. 9: F-measure of MAMADROID classification with datasets from
the same year (family mode).
resulting set is shuffled and divided into ten equal-size random
subsets. Classification is then performed ten times using nine
subsetsfortrainingandonefortesting,andresultsareaveraged
out over the ten experiments.
B. Detection Performance
We start our evaluation by measuring how well MA-
MADROID detects malware by training and testing using
samples that are developed around the same time. To this end,
we perform 10-fold cross validations on the combined dataset
composedofabenignsetandamaliciousone.TableIIprovides
anoverviewofthedetectionresultsachievedby MAMADROID
on each combined dataset, in the two modes of operation,
both with PCA features and without. The reported F-measure,
precision, and recall scores are the ones obtained with Random
Forest, which generally performs better than 1-NN and 3-NN.
Family mode. In Fig. 9, we report the F-measure when
operating in family mode for Random Forests, 1-NN and 3-
NN. The F-measure is always at least 88% with Random
Forests, and, when tested on the 2014 (malicious) dataset, it
reaches 98%. With some datasets, MAMADROID performs
slightly better than with others. For instance, with the 2014
malware dataset, we obtain an F-measure of 92% when using
the oldbenign dataset and 98% with newbenign. In general,
lower F-measures are due to increased false positives since
recall is always above 95%, while precision might be lower,
also due to the fact that malware datasets are larger than the
benign sets. We believe that this follows the evolutionary trend
discussed in Section III: while both benign and malicious apps
become more complex as time passes, when a new benign app
is developed, it is still possible to use old classes or re-use
code from previous versions and this might cause them to be
more similar to old malware samples. This would result in
false positives by MAMADROID. In general, MAMADROID
performs better when the different characteristics of malicious
and benign training and test sets are more predominant, which
corresponds to datasets occupying different positions of the
feature space.
Package mode. When MAMADROID runs in package mode,
the classification performance improves, ranging from 92%
F-measure with 2016 and newbenign to 99% with 2014 and
erusaem-F
RF
1-NN
3-NN
Fig.10:F-measureofMAMADROIDclassificationwithdatasetsfrom
the same year (package mode).
newbenign, using Random Forests. Fig. 10 reports the F-
measure of the 10-fold cross validation experiments using
Random Forests, 1-NN, and 3-NN (in package mode). The
former generally provide better results also in this case.
With some datasets, the difference in performance between
the two modes of operation is more noticeable: with drebin
and oldbenign, and using Random Forests, we get 95% F-
measure in package mode compared to 88% in family mode.
These differences are caused by a lower number of false
positives in package mode. Recall remains high, resulting
in a more balanced system overall. In general, abstracting
to packages rather than families provides better results as
the increased granularity enables identifying more differences
betweenbenignandmaliciousapps.Ontheotherhand,however,
this likely reduces the efficiency of the system, as many of the
states deriving from the abstraction are used a only few times.
The differences in time performance between the two modes
are analyzed in details in Section IV-F.
Using PCA. As discussed in Section II-D, PCA transforms
large feature spaces into smaller ones, thus it can be useful
to significantly reduce computation and, above all, memory
complexities of the classification task. When operating in pack-
age mode, PCA is particularly beneficial, since MAMADROID
originally has to operate over 116,281 features. Therefore, we
compare results obtained using PCA by fixing the number
of components to 10 and checking the quantity of variance
included in them. In package mode, we observe that only 67%
of the variance is taken into account by the 10 most important
PCA components, whereas, in family mode, at least 91% of
the variance is included by the 10 PCA Components.
AsshowninTableII,theF-measureobtainedusingRandom
Forests and the PCA components sets derived from the family
and package features is only slightly lower (up to 4%) than
using the full feature set. We note that lower F-measures are
caused by a uniform decrease in both precision and recall.
C. Detection Over Time
AsAndroidevolvesovertheyears,sodothecharacteristics
of both benign and malicious apps. Such evolution must be
taken into account when evaluating Android malware detection
7(cid:80) M(cid:80) od(cid:80) eD(cid:80)at (cid:80)as (cid:80)et
drebin&oldbenign 2013&oldbenign
2014[P &rec oi lsi do bn e, nR ie gc nall, 20F 1-m 4e &as nu er we]
benign 2015&newbenign 2016&newbenign
Family 0.82 0.95 0.88 0.91 0.93 0.92 0.88 0.97 0.92 0.97 0.99 0.98 0.90 0.94 0.92 0.87 0.92 0.89
Package 0.95 0.97 0.96 0.98 0.96 0.97 0.93 0.98 0.96 0.98 1.00 0.99 0.93 0.98 0.96 0.91 0.91 0.91
Family(PCA) 0.83 0.93 0.87 0.93 0.90 0.91 0.86 0.94 0.90 0.96 0.99 0.97 0.87 0.93 0.90 0.86 0.87 0.87
Package(PCA) 0.93 0.95 0.94 0.97 0.94 0.95 0.92 0.96 0.94 0.98 1.00 0.99 0.92 0.97 0.94 0.88 0.90 0.89
TABLE II: F-measure, precision, and recall obtained by MAMADROID, using Random Forests, on various dataset combinations with different
modes of operation, with and without PCA.
systems, since their accuracy might significantly be affected 1.0
as newer APIs are released and/or as malicious developers
modify their strategies in order to avoid detection. Evaluating 0.8
this aspect constitutes one of our research questions, and one
of the reasons why our datasets span across multiple years
0.6
(2010–2016).
As discussed in Section II-B, MAMADROID relies on the 0.4
sequence of API calls extracted from the call graphs and
abstracted at either the package or the family level. Therefore,
0.2
it is less susceptible to changes in the Android API than
other classification systems such as DROIDAPIMINER [2]
and DREBIN [5]. Since these rely on the use, or the frequency, 0.0 0 1 2 3 4
Years
of certain API calls to classify malware vs benign samples,
they need to be retrained following new API releases. On the
contrary, retraining is not needed as often with MAMADROID,
since families and packages represent more abstract function-
alities that change less over time. Consider, for instance, the
android.os.health package: released with API level 24, it
contains a set of classes helping developers track and monitor
system resources.10 Classification systems built before this
release – as in the case of DROIDAPIMINER [2] (released in
2013, when Android API was up to level 20) – need to be
retrained if this package is more frequently used by malicious
apps than benign apps, while MAMADROID only needs to add
a new state to its Markov chain when operating in package
mode, while no additional state is required when operating in
family mode.
Toverifythishypothesis,wetestMAMADROIDusingolder
samples as training sets and newer ones as test sets. Fig. 11
reports the F-measure of the classification in this setting, with
MAMADROIDoperatinginfamilymode.Thex-axisreportsthe
difference in years between training and test data. We obtain
86% F-measure when we classify apps one year older than
the samples on which we train. Classification is still relatively
accurate, at 75%, even after two years. Then, from Fig. 12,
we observe that the F-measure does not significantly change
when operating in package mode. Both modes of operations
are affected by one particular condition, already discussed in
Section III: in our models, benign datasets seem to “anticipate”
malicious ones by 1–2 years in the way they use certain
API calls. As a result, we notice a drop in accuracy when
classifying future samples and using drebin (with samples
from 2010 to 2012) or 2013 as the malicious training set and
oldbenign (late 2013/early 2014) as the benign training set.
More specifically, we observe that MAMADROID correctly
detects benign apps, while it starts missing true positives and
increasing false negatives — i.e., achieving lower recall.
Wealsosettoverifywhetheroldermalwaresamplescanstill
bedetectedbythesystem—ifnot,thiswouldobviouslybecome
10https://developer.android.com/reference/android/os/health/package-summary.
html
erusaem-F
RF
1-NN
3-NN
Fig.11:F-measureofMAMADROIDclassificationusingoldersamples
for training and newer for testing (family mode).
1.0
0.8
0.6
0.4
0.2
0.0
0 1 2 3 4
Years
erusaem-F
RF
1-NN
3-NN
Fig.12:F-measureofMAMADROIDclassificationusingoldersamples
for training and newer for testing (package mode).
vulnerable to older (and possibly popular) attacks. Therefore,
we also perform the “opposite” experiment, i.e., training
MAMADROID with newer datasets, and checking whether it
is able to detect malware developed years before. Specifically,
Fig.13and14reportresultswhentraining MAMADROID with
samples from a given year, and testing it with others that are
up to 4 years older: MAMADROID retains similar F-measure
scores over the years. Specifically, in family mode, it varies
from 93% to 96%, whereas, in package mode, from 95% to
97% with the oldest samples.
D. Case Studies of False Positives and Negatives
The experiment analysis presented above show that MA-
MADROID detects Android malware with high accuracy.
As in any detection system, however, the system makes a
small number of incorrect classifications, incurring some false
positives and false negatives. Next, we discuss a few case
studies aiming to better understandthese misclassifications. We
focus on the experiments with newer datasets, i.e., 2016 and
81.0
0.8
0.6
0.4
0.2
0.0
0 1 2 3 4
Years
erusaem-F
RF
1-NN
3-NN
Fig. 13: F-measure of MAMADROID classification using newer
samples for training and older for testing (family mode).
1.0
0.8
0.6
0.4
0.2
0.0 0 1 2 3 4
Years
erusaem-F
themasmalware.Finally,wefindthat16%ofthefalsenegatives
reported by MAMADROID are samples sending text messages
or starting calls to premium services. We also do a similar
analysis of false negatives when abstracting to packages (74
samples),withsimilarresults:thereafewmoreadwaresamples
(53%),butsimilarpercentagesforpotentiallybenignapps(15%)
and samples sending SMSs or placing calls (11%).
In conclusion, we find that MAMADROID’s sporadic
misclassifications are typically due to benign apps behaving
similarly to malware, malware that do not perform clearly-
malicious activities, or mistakes in the ground truth labeling.
E. MAMADROID vs DROIDAPIMINER
We also compare the performance of MAMADROID to
previous work using API features for Android malware clas-
sification. Specifically, we compare to DROIDAPIMINER [2],
because: (i) it uses API calls and its parameters to perform
classification;(ii)itreportshightruepositiverate(upto97.8%)
on almost 4K malware samples obtained from McAfee and
GENOME [66], and 16K benign samples; and (iii) its source
code has been made available to us by the authors.
In DROIDAPIMINER, permissions that are requested more
frequently by malware samples than by benign apps are used
to perform a baseline classification. Since there are legitimate
RF situations where a non-malicious app needs permissions tagged
1-NN
3-NN as dangerous, DROIDAPIMINER also applies frequency anal-
ysis on the list of API calls, specifically, using the 169 most
frequent API calls in the malware samples (occurring at least
Fig. 14: F-measure of MAMADROID classification using newer
6% more in malware than benign samples) —leading to a
samples for training and older for testing (package mode).
reported 83% precision. Finally, data flow analysis is applied
on the API calls that are frequent in both benign and malicious
samples, but do not occur by at least, 6% more in the malware
newbenign.
set. Using the top 60 parameters, the 169 most frequent calls
change, and authors report a precision of 97.8%.
False Positives. We analyze the manifest of the 164 apps
mistakenly detected as malware by MAMADROID, finding that Afterobtaining DROIDAPIMINER’ssourcecode,aswellas
most of them use “dangerous” permissions [4]. In particular, a list of packages used for feature refinement, we re-implement
67% of the apps write to external storage, 32% read the phone the system by modifying the code in order to reflect recent
state, and 21% access the device’s fine location. We further changes in Androguard (used by DROIDAPIMINER for API
analyzed apps (5%) that use the READ_SMS and SEND_SMS callextraction),extracttheAPIcallsforallappsinthedatasets
permissions, i.e., even though they are not SMS-related apps, listed in Table I, and perform a frequency analysis on the
theycanreadandsendSMSsaspartoftheservicestheyprovide calls. Androguard fails to extract calls for about 2% (1,017)
to users. In particular, a “in case of emergency” app is able to of apps in our datasets as a result of bad CRC-32 redundancy
send messages to several contacts from its database (possibly checks and error in unpacking, thus DROIDAPIMINER is
added by the user), which is a typical behavior of Android evaluated over the samples in the second-to-last column of
malware in our dataset, ultimately leading MAMADROID to Table I. We also implement classification, which is missing
flag it as malicious. from the code provided by the authors, using k-NN (with k=3)
sinceitachievesthebestresultsaccordingtothepaper.Weuse
False Negatives. We also check the 114 malware samples 2/3ofthedatasetfortrainingand1/3fortestingasimplemented
missed by MAMADROID when operating in family mode, by the authors [2]. A summary of the resulting F-measures
using VirusTotal.11 We find that 18% of the false negatives obtained using different training and test sets is presented in
are actually not classified as malware by any of the antivirus Table III.
engines used by VirusTotal, suggesting that these are actually
legitimate apps mistakenly included in the VirusShare dataset. We set up a number of experiments to thoroughly compare
45% of MAMADROID’s false negatives are adware, typically, DROIDAPIMINER to MAMADROID. First, we set up three
repackaged apps in which the advertisement library has been experiments in which we train DROIDAPIMINER using a
substituted with a third-party one, which creates a monetary dataset composed of oldbenign combined with one of the
profit for the developers. Since they are not performing any three oldest malware datasets each (drebin, 2013, and 2014),
clearly malicious activity, MAMADROID is unable to identify and testing on all malware datasets. With this configuration,
the best result (with 2014 and oldbenign as training sets)
11https://www.virustotal.com amounts to 62% F-measure when tested on the same dataset.
9TestingSets
drebin&oldbenign 2013&oldbenign 2014&oldbenign 2015&oldbenign 2016&oldbenign
TrainingSets [2] OurWork [2] OurWork [2] OurWork [2] OurWork [2] OurWork
drebin & oldbenign 0.32 0.96 0.35 0.96 0.34 0.79 0.30 0.42 0.33 0.43
2013 & oldbenign 0.33 0.93 0.36 0.97 0.35 0.74 0.31 0.36 0.33 0.29
2014 & oldbenign 0.36 0.92 0.39 0.93 0.62 0.95 0.33 0.79 0.37 0.78
drebin&newbenign 2013&newbenign 2014&newbenign 2015&newbenign 2016&newbenign
TrainingSets [2] OurWork [2] OurWork [2] OurWork [2] OurWork [2] OurWork
2014 & newbenign 0.76 0.99 0.75 0.99 0.92 0.99 0.67 0.89 0.65 0.83
2015 & newbenign 0.68 0.98 0.68 0.98 0.69 0.99 0.77 0.95 0.65 0.90
2016 & newbenign 0.33 0.97 0.35 0.97 0.36 0.99 0.34 0.93 0.36 0.92
TABLE III: Classification performance of DROIDAPIMINER [2] vs MAMADROID (our work).
The F-measure drops to 33% and 39%, respectively, when across apps. On average, it takes 9.2s±14 (min 0.02s, max
tested on samples one year into the future and past. If we use 13m) to complete for samples in our malware sets. Benign
the same configurations in MAMADROID, in package mode, apps usually yield larger call graphs, and the average time to
we obtain up to 97% F-measure (using 2013 and oldbenign extract them is 25.4s±63 (min 0.06s, max 18m) per app. Note
as training sets), dropping to 74% and 93%, respectively, that we do not include in our evaluation apps for which we
one year into the future and into the past. For the datasets could not successfully extract the call graph.
where DROIDAPIMINER achieves its best result (i.e., 2014
Next, we measure the time needed to extract call sequences
and oldbenign), MAMADROID achieves an F-measure of
while abstracting to families or packages, depending on
95%, which drops to respectively, 79% and 93% one year into
MAMADROID’smodeofoperation.Infamilymode,thisphase
the future and the past. The F-measure is stable even two years
completesinabout1.3sonaverage(andatmost11.0s)withboth
into the future and the past at 78% and 92%, respectively.
benign and malicious samples. Abstracting to packages takes
Asasecondsetofexperiments,wetrain DROIDAPIMINER slightlylonger,duetotheuseof341packagesinMAMADROID.
using a dataset composed of newbenign combined with one On average, this extraction takes 1.67s±3.1 for malicious apps
of the three most recent malware datasets each (2014, 2015, and 1.73s±3.2 for benign samples. As it can be seen, the call
and 2016). Again, we test DROIDAPIMINER on all malware sequenceextractioninpackagemodedoesnottakesignificantly
datasets. The best result is obtained with the dataset (2014 and more than in family mode.
newbenign) used for both testing and training, yielding a F-
MAMADROID’sthirdstepincludesMarkovchainmodeling
measureof92%,whichdropsto67%and75%oneyearintothe
and feature vector extraction. This phase is fast regardless of
futureandpastrespectively.Likewise,weusethesamedatasets
the mode of operation and datasets used. Specifically, with
for MAMADROID, with the best results achieved on the same
malicious samples, it takes on average 0.2s±0.3 and 2.5s±3.2
datasetas DROIDAPIMINER.Inpackagemode, MAMADROID
(and at most 2.4s and 22.1s), respectively, with families and
achieves an F-measure of 99%, which is maintained more than
packages, whereas, with benign samples, averages rise to
two years into the past, but drops to respectively, 89% and
0.6s±0.3 and 6.7s±3.8 (at most 1.7s and 18.4s).
83% one and two years into the future.
Finally,thelaststepinvolvesclassification,andperformance
As summarized in Table III, MAMADROID achieves
depends on both the machine learning algorithm employed and
significantly higher performance than DROIDAPIMINER in
the mode of operation. More specifically, running times are
all but one experiment, with the F-measure being at least 79%
affected by the number of features for the app to be classified,
even after two years into the future or the past when datasets
andnotbytheinitialdimensionofthecallgraph,orbywhether
from 2014 or later are used for training. Note that there is only
the app is benign or malicious. Regardless, in family mode,
onesettinginwhich DROIDAPIMINER performsslightlybetter
RandomForests,1-NN,and3-NNalltakelessthan0.01s.With
than MAMADROID: this occurs when the malicious training
packages, it takes, respectively, 0.65s, 1.05s, and 0.007s per
set is much older than the malicious test set. Specifically,
app with 1-NN, 3-NN, Random Forests.
MAMADROID presents low recall in this case: as discussed,
MAMADROID’s classification performs much better when the Overall, when operating in family mode, malware and
training set is not more than two years older than the test set. benign samples take on average, 10.7s and 27.3s respectively
to complete the entire process, from call graph extraction
to classification. Whereas, in package mode, the average
F. Runtime Performance
completion times for malware and benign samples are 13.37s
We envision MAMADROID to be integrated in offline and 33.83s respectively. In both modes of operation, time is
detection systems, e.g., run by Google Play. Recall that mostly (> 80%) spent on call graph extraction.
MAMADROID consists of different phases, so in the following,
We also evaluate the runtime performance of
we review the computational overhead incurred by each of
them, aiming to assess the feasibility of real-world deployment.
DROIDAPIMINER [2]. Its first step, i.e., extracting API
calls, takes 0.7s±1.5 (min 0.01s, max 28.4s) per app in our
Werunourexperimentsonadesktopequippedwithan40-core
malware datasets. Whereas, it takes on average 13.2s±22.2
2.30GHz CPU and 128GB of RAM, but only use one core and
(min 0.01s, max 222s) per benign app. In the second phase,
allocate 16GB of RAM for evaluation.
i.e., frequency and data flow analysis, it takes, on average,
MAMADROID’s first step involves extracting the call graph 4.2s per app. Finally, classification using 3-NN is very fast:
from an apk and the complexity of this task varies significantly 0.002s on average. Therefore, in total, DROIDAPIMINER takes
10respectively, 17.4s and 4.9s for a complete execution on one We argue that the effectiveness of MAMADROID’s classifi-
app from our benign and malware datasets, which while faster cationremainsrelativelyhigh“overtheyears”owingtoMarkov
than MAMADROID, achieves significantly lower accuracy. models capturing app behavior. These models tend to be more
robust to malware evolution because abstracting to families or
In conclusion, our experiments show that our prototype
packages makes the system less susceptible to the introduction
implementation of MAMADROID is scalable enough to be
of new API calls. Abstraction allows MAMADROID to capture
deployed. Assuming that, everyday, a number of apps in the
newer classes/methods added to the API, since these are
order of 10,000 are submitted to Google Play, and using the
abstractedtoalready-knownfamiliesorpackages.Incasenewer
average execution time of benign samples in family (27.3s)
packages are added to the API, and these packages start being
and package (33.83s) modes, we estimate that it would take
used by malware, MAMADROID only requires adding a new
less than an hour and a half to complete execution of all apps
state to the Markov chains, and probabilities of a transition
submitted daily in both modes, with just 64 cores. Note that
from a state to this new state in old apps would be 0. Adding
we could not find accurate statistics reporting the number of
only a few nodes does not likely alter the probabilities of the
apps submitted everyday, but only the total number of apps on
other 341 nodes, thus, two apps created with the same purpose
Google Play.12 On average, this number increases of a couple
will not strongly differ in API calls usage if they are developed
of thousands per day, and although we do not know how many
using almost consecutive API levels.
apps are removed, we believe 10,000 apps submitted every day
is likely an upper bound. We also observe that abstracting to packages provides a
slightlybettertradeoffthanfamilies.Infamilymode,thesystem
V. DISCUSSION is lighter and faster, and actually performs better when there
are more than two years between training and test set samples
We now discuss the implications of our results with respect However, even though both modes of operation effectively
to the feasibility of modeling app behavior using static analysis detect malware, abstracting to packages yields better results
and Markov chains, discuss possible evasion techniques, and overall. Nonetheless, this does not imply that less abstraction
highlight some limitations of our approach. is always better: in fact, a system that is too granular, besides
incurring untenable complexity, would likely create Markov
A. Lessons Learned models with low-probability transitions, ultimately resulting
in less accurate classification. We also highlight that applying
Our work yields important insights around the use of API
PCA is a good strategy to preserve high accuracy and at the
callsinmaliciousapps,showingthat,bymodelingthesequence
same time reducing complexity.
of API calls made by an app as a Markov chain, we can
successfully capture the behavioral model of that app. This
allows MAMADROID to obtain high accuracy overall, as well B. Evasion
as to retain it over the years, which is crucial due to the
Next, we discuss possible evasion techniques and how they
continuous evolution of the Android ecosystem.
can be addressed. One straightforward evasion approach could
As discussed in Section III, the use of API calls changes be to repackage a benign app with small snippets of malicious
over time, and in different ways across malicious and benign code added to a few classes. However, it is difficult to embed
samples. From our newer datasets, which include samples up malicious code in such a way that, at the same time, the
to Spring 2016 (API level 23), we observe that newer APIs resulting Markov chain looks similar to a benign one. For
introduce more packages, classes, and methods, while also instance,ourrunningexamplefromSectionII(malwareposing
deprecating some. Fig. 6, 7(a), and 7(b) show that benign apps as a memory booster app and executing unwanted commands
are using more calls than malicious ones developed around asroot)iscorrectlyclassifiedby MAMADROID;althoughmost
the same time. We also notice an interesting trend in the use functionalities in this malware are the same as the original app,
of Android and Google APIs: malicious apps follow the same injected API calls generate some transitions in the Markov
trend as benign apps in the way they adopt certain APIs, but chain that are not typical of benign samples.
with a delay of some years. This might be a side effect of
Theoppositeprocedure–i.e.,embeddingportionsofbenign
Android malware authors’ tendency to repackage benign apps,
code into a malicious app – is also likely ineffective against
adding their malicious functionalities onto them.
MAMADROID,since,foreachapp,wederivethefeaturevector
Given the frequent changes in the Android framework from the transition probability between calls over the entire
and the continuous evolution of malware, systems like app.Inotherwords,amalwaredeveloperwouldhavetoembed
DROIDAPIMINER [2] – being dependent on the presence or benign code inside the malware in such a way that the overall
theuseofcertainAPIcalls–becomeincreasinglylesseffective sequence of calls yields similar transition probabilities as those
with time. As shown in Table III, malware that uses API calls in a benign app, but this is difficult to achieve because if the
released after those used by samples in the training set cannot sequences of calls have to be different (otherwise there would
be identified by these systems. On the contrary, as shown in be no attack), then the models will also be different.
Fig. 11 and 12, MAMADROID detects malware samples that
are 1 year newer than the training set obtaining an 86% F- An attacker could also try to create an app from scratch
measure (as opposed to 46% with DROIDAPIMINER). After 2 with a similar Markov chain to that of a benign app. Because
years, the value is still at 75% (42% with DROIDAPIMINER), this is derived from the sequence of abstracted API calls in the
dropping to 51% after 4 years. app, it is actually very difficult to create sequences resulting
in Markov chains similar to benign apps while, at the same
12http://www.appbrain.com/stats/number-of-android-apps time, actually engaging in malicious behavior. Nonetheless, in
11future work, we plan to systematically analyze the feasibility dimensions of the features space where the classifier operates
of this strategy. is remarkably reduced.
Moreover, attackers could try using reflection, dynamic Soot [52], which we use to extract call graphs, fails to
code loading, or native code [42]. Because MAMADROID uses analyze some apks. In fact, we were not able to extract call
static analysis, it fails to detect malicious code when it is graphs for a fraction (4.6%) of the apps in the original datasets
loaded or determined at runtime. However, MAMADROID can due to scripts either failing to apply the jb phase, which is
detect reflection when a method from the reflection package used to transform Java bytecode to the primary intermediate
(java.lang.reflect) is executed. Therefore, we obtain the representation (i.e., jimple) of Soot or not able to open the
correct sequence of calls up to the invocation of the reflection apk. Even though this does not really affect the results of
call, which may be sufficient to distinguish between malware our evaluation, one could avoid it by using a different/custom
and benign apps. Similarly, MAMADROID can detect the intermediate representation for the analysis or use different
usage of class loaders and package contexts that can be used tools to extract the call graphs.
to load arbitrary code, but it is not able to model the code
In general, static analysis methodologies for malware
loaded; likewise, native code that is part of the app cannot be
detection on Android could fail to capture the runtime en-
modeled, as it is not Java and is not processed by Soot. These
vironment context, code that is executed more frequently, or
limitationsarenotspecificof MAMADROID,butareaproblem
other effects stemming from user input [5]. These limitations
of static analysis in general, which can be mitigated by using
can be addressed using dynamic analysis, or by recording
MAMADROID alongside dynamic analysis techniques.
function calls on a device. Dynamic analysis observes the live
Malware developers might also attempt to evade MA- performance of the samples, recording what activity is actually
MADROIDbynamingtheirself-definedpackagesinsuchaway performed at runtime. Through dynamic analysis, it is also
thattheylooksimilartothatoftheandroid,java,orgoogle possible to provide inputs to the app and then analyze the
APIs, e.g., creating packages like java.lang.reflect.malware reactionoftheapptotheseinputs,goingbeyondstaticanalysis
and java.lang.malware, aiming to confuse MAMADROID into limits. To this end, we plan to integrate dynamic analysis to
abstracting them to respectively, java.lang.reflect and build the models used by MAMADROID as part of future work.
java.lang. However, this is easily prevented by whitelisting
the list of packages from android, java, or google APIs. VI. RELATEDWORK
Another approach could be using dynamic dispatch so that Over the past few years, Android security has attracted a
a class X in package A is created to extend class Y in package wealth of work by the research community. In this section,
B with static analysis reporting a call to root() defined in Y as we review (i) program analysis techniques focusing on general
X.root(), whereas, at runtime Y.root() is executed. This can be security properties of Android apps, and then (ii) systems that
addressed, however, with a small increase in MAMADROID’s specifically target malware on Android.
computational cost, by keeping track of self-defined classes
A. Program Analysis
that extend or implement classes in the recognized APIs, and
abstract polymorphic functions of this self-defined class to the Previous work on program analysis applied to Android
corresponding recognized package, while, at the same time, security has used both static and dynamic analysis. With
abstracting as self-defined overridden functions in the class. the former, the program’s code is decompiled in order to
extract features without actually running the program, usually
Finally, identifier mangling and other forms of obfuscation
employingtoolssuchasDare[41]toobtainJavabytecode.The
could be used aiming to obfuscate code and hide malicious
latter involves real-time execution of the program, typically in
actions. However, since classes in the Android framework
an emulated or protected environment.
cannot be obfuscated by obfuscation tools, malware developers
can only do so for self-defined classes. MAMADROID labels Static analysis techniques include work by Felt et al. [21],
obfuscated calls as obfuscated so, ultimately, these would who analyze API calls to identify over-privileged apps, while
be captured in the behavioral model (and the Markov chain) Kirin [20] is a system that examines permissions requested
for the app. In our sample, we observe that benign apps use by apps to perform a lightweight certification, using a set
significantly less obfuscation than malicious apps, indicating of security rules that indicate whether or not the security
that obfuscating a significant number of classes is not a good configuration bundled with the app is safe. RiskRanker [28]
evasion strategy since this would likely make the sample more aims to identify zero-day Android malware by assessing
easily identifiable as malicious. potential security risks caused by untrusted apps. It sifts
through a large number of apps from Android markets and
C. Limitations examines them to detect certain behaviors, such as encryption
and dynamic code loading, which form malicious patterns and
MAMADROID requires a sizable amount of memory in
can be used to detect stealthy malware. Other methods, such
ordertoperformclassification,whenoperatinginpackagemode,
as CHEX [37], use data flow analysis to automatically vet
workingonmorethan100,000featurespersample.Thequantity
Android apps for vulnerabilities. Static analysis has also been
of features, however, can be further reduced using feature
applied to the detection of data leaks and malicious data flows
selection algorithms such as PCA. As explained in Section IV
from Android apps [6], [34], [35], [62].
whenweuse10componentsfromthePCAthesystemperforms
almost as well as the one using all the features; however, using DroidScope [59] and TaintDroid [19] monitor run-time app
PCA comes with a much lower memory complexity in order behavior in a protected environment to perform dynamic taint
to run the machine learning algorithms, because the number of analysis. DroidScope performs dynamic taint analysis at the
12machine code level, while TaintDroid monitors how third-party beinadequateduetothelowprobabilityoftriggeringmalicious
appsaccessormanipulateusers’personaldata,aimingtodetect behavior,andcanbeside-steppedbyknowledgeableadversaries,
sensitivedataleavingthesystem.However,asitisunrealisticto as suggested by Wong and Lie [56]. Other approaches include
deploy dynamic analysis techniques directly on users’ devices, random fuzzing [38], [63] and concolic testing [3], [26].
due to the overhead they introduce, these are typically used Dynamicanalysiscanonlydetectmaliciousactivitiesifthecode
offline [45], [50], [67]. ParanoidAndroid [44] employs a virtual exhibiting malicious behavior is actually running during the
clone of the smartphone, running in parallel in the cloud and analysis. Moreover, according to [54], mobile malware authors
replaying activities of the device – however, even if minimal often employ emulation or virtualization detection strategies to
execution traces are actually sent to the cloud, this still takes a change malware behavior and eventually evade detection.
non-negligible toll on battery life.
Aiming to complement static and dynamic analysis tools,
Recently, hybrid systems like IntelliDroid [56] have also machine learning techniques have also been applied to assist
been proposed that use input generators, producing inputs Androidmalwaredetection.Droidmat[57]usesAPIcalltracing
specifictodynamicanalysistools.Otherworkcombiningstatic and manifest files to learn features for malware detection,
and dynamic analysis include [8], [25], [31], [58]. while Gascon et al. [24] rely on embedded call graphs. Droid-
Miner [60] studies the program logic of sensitive Android/Java
B. Android Malware Detection framework API functions and resources, and detects malicious
behavior patterns. MAST [14] statically analyzes apps using
A number of techniques have used signatures for Android featuressuchaspermissions,presenceofnativecode,andintent
malware detection. NetworkProfiler [18] generates network filtersandmeasuresthecorrelationbetweenmultiplequalitative
profilesforAndroidappsandextractsfingerprintsbasedonsuch data. Crowdroid [10] relies on crowdsourcing to distinguish
traces,whileworkin[12]obtainsresource-basedmetrics(CPU, between malicious and benign apps by monitoring system calls.
memory,storage,network)todistinguishmalwareactivityfrom AppContext [61] models security-sensitive behavior, such as
benign one. Chen et al. [15] extract statistical features, such activation events or environmental attributes, and uses SVM
as permissions and API calls, and extend their vectors to add to classify these behaviors, while RevealDroid [23] employs
dynamicbehavior-basedfeatures.Whiletheirexperimentsshow supervised learning and obfuscation-resilient methods targeting
that their solution outperforms, in terms of accuracy, other API usage and intent actions to identify their families.
antivirus systems, Chen et al. [15] indicate that the quality of
their detection model critically depends on the availability of DREBIN [5] automatically deduces detection patterns and
representativebenignandmaliciousappsfortraining.Similarly, identifies malicious software directly on the device, performing
ScanMeMobile[64]usestheGoogleCloudMessagingService a broad static analysis. This is achieved by gathering numerous
(GCM) to perform static and dynamic analysis on apks found features from the manifest file as well as the app’s source code
on the device’s SD card. (API calls, network addresses, permissions). Malevolent behav-
iorisreflectedinpatternsandcombinationsofextractedfeatures
Thesequencesofsystemcallshavealsobeenusedtodetect
from the static analysis: for instance, the existence of both
malware in both desktop and Android environments. Hofmeyr
SEND_SMS permission and the android.hardware.telephony
et al. [30] demonstrate that short sequences of system calls
componentinanappmightindicateanattempttosendpremium
can be used as a signature to discriminate between normal
SMS messages, and this combination can eventually constitute
and abnormal behavior of common UNIX programs. Signature-
a detection pattern.
based methods, however, can be evaded using polymorphism
andobfuscation,aswellasbycallre-orderingattacks[36],even In Section IV, we have already introduced, and compared
though quantitative measures, such as similarity analysis, can against, DROIDAPIMINER [2]. This system relies on the top-
be used to address some of these attacks [48]. MAMADROID 169APIcallsthatareusedmorefrequentlyinthemalwarethan
inherits the spirit of these approaches, proposing a statistical in the benign set, along with data flow analysis on calls that
method to model app behavior that is more robust against are frequent in both benign and malicious apps, but occur up
evasion attempts. to 6% more in the latter. As shown in our evaluation, using the
most common calls observed during training requires constant
IntheAndroidcontext,Canforaetal.[11]usethesequences
retraining,duetotheevolutionofbothmalwareandtheAndroid
of three system calls (extracted from the execution traces of
API.Onthecontrary,MAMADROIDcaneffectivelymodelboth
apps under analysis) to detect malware. This approach models
benign and malicious Android apps, and perform an efficient
specificmalwarefamilies,aimingtoidentifyadditionalsamples
classification on them. Compared to DROIDAPIMINER, our
belonging to such families. In contrast, MAMADROID’s goal
approachismoreresilienttochangesintheAndroidframework
is to detect previously-unseen malware, and we also show that
than DROIDAPIMINER, resulting in a less frequent need to
our system can detect new malware families that even appear
re-train the classifier.
years after the system has been trained. In addition, using strict
sequences of system or API calls can be easily evaded by Overall, compared to state-of-the-art systems like
malwareauthorswhocouldaddunnecessarycallstoeffectively DREBIN [5]and DROIDAPIMINER [2], MAMADROID ismore
evade detection. Conversely, MAMADROID builds a behavioral generic and robust as its statistical modeling does not depend
model of an Android app, which makes it robust to this type on specific app characteristics, but can actually be run on any
of evasion. app created for any Android API level.
Dynamic analysis has also been applied to detect Android Finally, also related to MAMADROID are Markov-chain
malwarebyusingpredefinedscriptsofcommoninputsthatwill based models for Android malware detection. Chen et al. [16]
be performed when the device is running. However, this might dynamically analyze system- and developer-defined actions
13fromintentmessages(usedbyappcomponentstocommunicate REFERENCES
with each other at runtime), and probabilistically estimate
[1] GooglePlayhashundredsofAndroidappsthatcontainmalware. http://
whether an app is performing benign or malicious actions
www.trustedreviews.com/news/malware-apps-downloaded-google-play,
at run time, but obtain low accuracy overall. Canfora et al. [13] 2016.
use a Hidden Markov model (HMM) to identify malware [2] Y. Aafer, W. Du, and H. Yin. DroidAPIMiner: Mining API-Level
samples belonging to previously observed malware families, FeaturesforRobustMalwareDetectioninAndroid. InSecureComm,
whereas, MAMADROID can detect previously unseen malware, 2013.
not relying on specific malware families. [3] S.Anand,M.Naik,M.J.Harrold,andH.Yang. AutomatedConcolic
TestingofSmartphoneApps. InACMSymposiumontheFoundations
ofSoftwareEngineering(FSE),2012.
[4] P. Andriotis, M. A. Sasse, and G. Stringhini. Permissions snapshots:
Assessingusers’adaptationtotheandroidruntimepermissionmodel. In
VII. CONCLUSION IEEEWorkshoponInformationForensicsandSecurity(WIFS),2016.
[5] D. Arp, M. Spreitzenbarth, M. Hubner, H. Gascon, and K. Rieck.
DREBIN: Effective and Explainable Detection of Android Malware
This paper presented MAMADROID, an Android malware
in Your Pocket. In Annual Symposium on Network and Distributed
detection systembased on modeling thesequences of API calls
SystemSecurity(NDSS),2014.
as Markov chains. Our system is designed to operate in one of
[6] S.Arzt,S.Rasthofer,C.Fritz,E.Bodden,A.Bartel,J.Klein,Y.LeTraon,
twomodes,withdifferentgranularities,byabstractingAPIcalls D.Octeau,andP.McDaniel. FlowDroid:PreciseContext,Flow,Field,
toeitherfamiliesorpackages.Werananextensiveexperimental Object-sensitiveandLifecycle-awareTaintAnalysisforAndroidApps.
evaluation using, to the best of our knowledge, the largest InACMSIGPLANConferenceonProgrammingLanguageDesignand
Implementation,2014.
malwaredataseteveranalyzedinanAndroidmalwaredetection
[7] S. Bernard, S. Adam, and L. Heutte. Using random forests for
research paper, and aiming at assessing both the accuracy
handwritten digit recognition. In Ninth International Conference on
of the classification (using F-measure, precision, and recall)
DocumentAnalysisandRecognition(ICDAR),2007.
and runtime performances. We showed that MAMADROID
[8] R. Bhoraskar, S. Han, J. Jeon, T. Azim, S. Chen, J. Jung, S. Nath,
effectively detects unknown malware samples developed earlier R. Wang, and D. Wetherall. Brahmastra: Driving Apps to Test the
or around the same time as the samples on which it is trained SecurityofThird-PartyComponents. InUSENIXSecuritySymposium,
(F-measure up to 99%). It also maintains good detection 2014.
performance: one year after the model has been trained the [9] L.Breiman. Randomforests. MachineLearning,45,2001.
F-measure value is 86%, and after two years it is 75%. [10] I.Burguera,U.Zurutuza,andS.Nadjm-Tehrani. Crowdroid:Behavior-
basedMalwareDetectionSystemforAndroid. InACMWorkshopon
SecurityandPrivacyinSmartphonesandMobileDevices(SPSM),2011.
We compared MAMADROID to DROIDAPIMINER [2], a
[11] G. Canfora, E. Medvet, F. Mercaldo, and C. A. Visaggio. Detecting
state-of-the-art system based on API calls frequently used
AndroidMalwareUsingSequencesofSystemCalls. InWorkshopon
by malware, showing that, not only does MAMADROID SoftwareDevelopmentLifecycleforMobile,2015.
outperforms DROIDAPIMINER when trained and tested on [12] G.Canfora,E.Medvet,F.Mercaldo,andC.A.Visaggio. Acquiring
the same datasets, but that it is also much more resilient andAnalyzingAppMetricsforEffectiveMobileMalwareDetection.
over the years to changes in the Android API. Overall, our InIWSPA,2016.
resultsdemonstratethatthetypeofstatisticalbehavioralmodels [13] G.Canfora,F.Mercaldo,andC.A.Visaggio. AnHMMandStructural
introduced by MAMADROID are more robust than traditional Entropy based Detector for Android malware: An Empirical Study.
Computers&Security,61,2016.
techniques, highlighting how our work can form the basis of
[14] S.Chakradeo,B.Reaves,P.Traynor,andW.Enck. MAST:Triagefor
more advanced detection systems in the future. As part of
Market-scaleMobileMalwareAnalysis.InACMConferenceonSecurity
future work, we plan to further investigate the resilience to andPrivacyinWirelessandMobileNetworks(WiSec),2013.
possible evasion techniques, focusing on repackaged malicious
[15] S. Chen, M. Xue, Z. Tang, L. Xu, and H. Zhu. StormDroid: A
apps as well as injection of API calls to maliciously alter StreaminglizedMachineLearning-BasedSystemforDetectingAndroid
Markovmodels.Wealsoplantoexploretheuseoffiner-grained Malware. InAsiaCCS,2016.
abstractions as well as the possibility to seed the behavioral [16] Y. Chen, M. Ghorbanzadeh, K. Ma, C. Clancy, and R. McGwier. A
modeling performed by MAMADROID with dynamic instead hiddenMarkovmodeldetectionofmaliciousAndroidapplicationsat
runtime. InWirelessandOpticalCommunicationConference(WOCC),
of static analysis. Due to the large size of the data, we have
2014.
not made them readily available online but both the datasets
[17] J. Clay. Continued Rise in Mobile Threats for 2016. http://blog.
and the feature vectors can be obtained upon request.
trendmicro.com/continued-rise-in-mobile-threats-for-2016/,2016.
[18] S.Dai,A.Tongaonkar,X.Wang,A.Nucci,andD.Song.NetworkProfiler:
TowardsautomaticfingerprintingofAndroidapps. InIEEEINFOCOM,
Acknowledgments.Wewishtothanktheanonymousreviewers
2013.
fortheirfeedback,ourshepherdAmirHoumansadrforhishelp
[19] W. Enck, P. Gilbert, S. Han, V. Tendulkar, B.-G. Chun, L. P. Cox,
in improving our paper, and Yousra Aafer for kindly sharing J. Jung, P. McDaniel, and A. N. Sheth. TaintDroid: An Information-
the DROIDAPIMINER source code with us. We also wish FlowTrackingSystemforRealtimePrivacyMonitoringonSmartphones.
to thank Yanick Fratantonio for his comments on an early ACMTrans.Comput.Syst.,32(2),2014.
draft of the paper. This research was supported by the EPSRC [20] W. Enck, M. Ongtang, and P. McDaniel. On Lightweight Mobile
under grant EP/N008448/1, by an EPSRC-funded “Future PhoneApplicationCertification. InACMConferenceonComputerand
CommunicationsSecurity(CCS),2009.
Leaders in Engineering and Physical Sciences” award, a Xerox
[21] A. P. Felt, E. Chin, S. Hanna, D. Song, and D. Wagner. Android
University Affairs Committee grant, and by a small grant from
Permissions Demystified. In ACM Conference on Computer and
GCHQ. Enrico Mariconti was supported by the EPSRC under CommunicationsSecurity(CCS),2011.
grant 1490017, while Lucky Onwuzurike was funded by the
[22] E.FixandJ.Hodges. Discriminatoryanalysis,non-parametricdiscrimi-
Petroleum Technology Development Fund (PTDF). nation. USAFSchoolofAviationMedicine,31,1951.
14[23] J.Garcia,M.Hammad,B.Pedrood,A.Bagheri-Khaligh,andS.Malek. ACMSymposiumonAccessControlModelsandTechnologies(SACMAT),
Obfuscation-resilient, efficient, and accurate detection and family 2012.
identification of android malware. Department of Computer Science, [47] P.Schulz. Codeprotectioninandroid. InsitituteofComputerScience,
GeorgeMasonUniversity,Tech.Rep,2015. RheinischeFriedrich-Wilhelms-UniversitgtBonn,Germany,110,2012.
[24] H.Gascon,F.Yamaguchi,D.Arp,andK.Rieck. StructuralDetection [48] M.K.Shankarapani,S.Ramamoorthy,R.S.Movva,andS.Mukkamala.
ofAndroidMalwareUsingEmbeddedCallGraphs. InACMWorkshop MalwaredetectionusingassemblyandAPIcallsequences. Journalin
onArtificialIntelligenceandSecurity(AISec),2013. ComputerVirology,7(2),2011.
[25] X.Ge,K.Taneja,T.Xie,andN.Tillmann. DyTa:DynamicSymbolic [49] Statista. Global mobile OS market share in sales to end users from
Execution Guided with Static Verification Results. In International 1stquarter2009to1stquarter2016. http://www.statista.com/statistics/
ConferenceonSoftwareEngineering(ICSE),2011. 266136/global-market-share-held-by-smartphone-operating-systems/,
2016.
[26] P. Godefroid, N. Klarlund, and K. Sen. DART: Directed Automated
RandomTesting. SIGPLANNot.,40(6),2005. [50] K.Tam,S.J.Khan,A.Fattori,andL.Cavallaro.CopperDroid:Automatic
[27] M. I. Gordon, D. Kim, J. H. Perkins, L. Gilham, N. Nguyen, and ReconstructionofAndroidMalwareBehaviors. InAnnualSymposium
M.C.Rinard. InformationFlowAnalysisofAndroidApplicationsin onNetworkandDistributedSystemSecurity(NDSS),2015.
DroidSafe. InAnnualSymposiumonNetworkandDistributedSystem [51] The Register. Google AdSense abused to distribute Android spy-
Security(NDSS),2015. ware. http://www.theregister.co.uk/2016/08/15/android_trojan_abuses_
[28] M.Grace,Y.Zhou,Q.Zhang,S.Zou,andX.Jiang.RiskRanker:Scalable google_adsense/,2016.
andAccurateZero-dayAndroidMalwareDetection. InInternational [52] R.Vallée-Rai,P.Co,E.Gagnon,L.Hendren,P.Lam,andV.Sundaresan.
ConferenceonMobileSystems,Applications,andServices(MobiSys), Soot-aJavaBytecodeOptimizationFramework. InConferenceofthe
2012. CentreforAdvancedStudiesonCollaborativeResearch,1999.
[29] M.Hearst,S.Dumais,E.Osman,J.Platt,andB.Scholkopf. Support [53] D. Venkatesan. Android.Bankosy: All ears on voice
VectorMachines. IEEEIntelligentSystemsandtheirapplications,13, call-based 2FA. http://www.symantec.com/connect/blogs/
1998. androidbankosy-all-ears-voice-call-based-2fa,2016.
[30] S.A.Hofmeyr,S.Forrest,andA.Somayaji. Intrusiondetectionusing [54] T.VidasandN.Christin. Evadingandroidruntimeanalysisviasandbox
sequencesofsystemcalls. JournalofComputerSecurity,6(3),1998. detection. InAsiaCCS,2014.
[31] Y.Z.X.Jiang. Detectingpassivecontentleaksandpollutioninandroid [55] N.Viennot,E.Garcia,andJ.Nieh. Ameasurementstudyofgoogle
applications. InAnnualSymposiumonNetworkandDistributedSystem play. 42(1),2014.
Security(NDSS),2013.
[56] M.Y.WongandD.Lie. IntelliDroid:ATargetedInputGeneratorfor
[32] I. Jolliffe. Principal Component Analysis. John Wiley & Sons, Ltd, theDynamicAnalysisofAndroidMalware. InAnnualSymposiumon
2002. NetworkandDistributedSystemSecurity(NDSS),2016.
[33] M.J.Kearns. Thecomputationalcomplexityofmachinelearning. MIT
[57] D.J.Wu,C.H.Mao,T.E.Wei,H.M.Lee,andK.P.Wu. DroidMat:
press,1990.
AndroidMalwareDetectionthroughManifestandAPICallsTracing.
[34] J.Kim,Y.Yoon,K.Yi,J.Shin,andS.Center. ScanDal:Staticanalyzer InAsiaJCIS,2012.
fordetectingprivacyleaksinandroidapplications. InMoST,2012.
[58] M. Xia, L. Gong, Y. Lyu, Z. Qi, and X. Liu. Effective Real-Time
[35] W.Klieber,L.Flynn,A.Bhosale,L.Jia,andL.Bauer. AndroidTaint Android Application Auditing. In IEEE Symposium on Security and
FlowAnalysisforAppSets. InSOAP,2014. Privacy,2015.
[36] C.Kolbitsch,P.M.Comparetti,C.Kruegel,E.Kirda,X.-y.Zhou,and [59] L.K.YanandH.Yin. DroidScope:SeamlesslyReconstructingtheOS
X.Wang. EffectiveandEfficientMalwareDetectionattheEndHost. andDalvikSemanticViewsforDynamicAndroidMalwareAnalysis.
InUSENIXsecuritysymposium,2009. InUSENIXSecuritySymposium,2012.
[37] L.Lu,Z.Li,Z.Wu,W.Lee,andG.Jiang. CHEX:StaticallyVetting [60] C.Yang,Z.Xu,G.Gu,V.Yegneswaran,andP.Porras. Droidminer:Au-
Android Apps for Component Hijacking Vulnerabilities. In ACM tomatedminingandcharacterizationoffine-grainedmaliciousbehaviors
ConferenceonComputerandCommunicationsSecurity(CCS),2012. inAndroidapplications. InESORICS,2014.
[38] A.Machiry,R.Tahiliani,andM.Naik. Dynodroid:AnInputGeneration [61] W.Yang,X.Xiao,B.Andow,S.Li,T.Xie,andW.Enck. AppContext:
SystemforAndroidApps. InJointMeetingonFoundationsofSoftware Differentiating Malicious and Benign Mobile App Behaviors Using
Engineering(ESEC/FSE),2013. Context. InInternationalConferenceonSoftwareEngineering(ICSE),
[39] J.R.Norris. Markovchains. CambridgeUniversityPress,1998. 2015.
[40] J. Oberheide and C. Miller. Dissecting the Android Bouncer. In [62] Z.Yang,M.Yang,Y.Zhang,G.Gu,P.Ning,andX.S.Wang.AppIntent:
SummerCon,2012. AnalyzingSensitiveDataTransmissioninAndroidforPrivacyLeakage
[41] D.Octeau,S.Jha,andP.McDaniel. RetargetingAndroidApplications Detection. In ACM Conference on Computer and Communications
toJavaBytecode. InACMSymposiumontheFoundationsofSoftware Security(CCS),2013.
Engineering(FSE),2012. [63] H. Ye, S. Cheng, L. Zhang, and F. Jiang. DroidFuzzer: Fuzzing the
[42] S. Poeplau, Y. Fratantonio, A. Bianchi, C. Kruegel, and G. Vigna. AndroidAppswithIntent-FilterTag. InInternationalConferenceon
ExecuteThis!AnalyzingUnsafeandMaliciousDynamicCodeLoading AdvancesinMobileComputingandMultimedia(MoMM),2013.
in Android Applications. In Annual Symposium on Network and [64] H.Zhang,Y.Cole,L.Ge, S.Wei, W. Yu,C.Lu,G.Chen, D.Shen,
DistributedSystemSecurity(NDSS),2014. E.Blasch,andK.D.Pham. ScanMeMobile:ACloud-basedAndroid
[43] I. Polakis, M. Diamantaris, T. Petsas, F. Maggi, and S. Ioannidis. MalwareAnalysisService. SIGAPPAppl.Comput.Rev.,16(1),2016.
Powerslave:AnalyzingtheEnergyConsumptionofMobileAntivirus [65] N. Zhang, K. Yuan, M. Naveed, X. Zhou, and X. Wang. Leave me
Software. InDIMVA,2015. alone:App-levelprotectionagainstruntimeinformationgatheringon
[44] G.Portokalidis,P.Homburg,K.Anagnostakis,andH.Bos. Paranoid Android. InIEEESymposiumonSecurityandPrivacy,2015.
Android: Versatile Protection for Smartphones. In Annual Computer [66] Y.ZhouandX.Jiang. DissectingAndroidMalware:Characterization
SecurityApplicationsConference(ACSAC),2010. andEvolution. InIEEESymposiumonSecurityandPrivacy,2012.
[45] V.Rastogi,Y.Chen,andX.Jiang.DroidChameleon:EvaluatingAndroid [67] Y.Zhou,Z.Wang,W.Zhou,andX.Jiang. Hey,You,GetOffofMy
Anti-malwareAgainstTransformationAttacks. InAsiaCCS,2013. Market:DetectingMaliciousAppsinOfficialandAlternativeAndroid
[46] B.P.Sarma,N.Li,C.Gates,R.Potharaju,C.Nita-Rotaru,andI.Molloy. Markets. In Annual Symposium on Network and Distributed System
AndroidPermissions:APerspectiveCombiningRisksandBenefits. In Security(NDSS),2012.
15