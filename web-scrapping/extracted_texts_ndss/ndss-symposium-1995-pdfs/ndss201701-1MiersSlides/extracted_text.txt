Making Searchable
Encryption Scale to the
Cloud
Ian Miers and Payman MohasselEnd to end Encryption
No Transport End2End
encryption encryption Encryption
Service Service Service
provider provider provider
user user user user user userE2E Encrypted Messaging
Need Want
Crypto CryptoE2E Encrypted Messaging
Ban Need Want
Crypto Crypto CryptoDeploying E2E encrypted
messaging
§ WhatsApp
§ No feature loss
§ Many users probably don’t know they are using it
§ iMessage
§ Same features as SMS
§ WebRTC video chatSearch
§ For some communication mechanisms, people expect search
§ Email is the canonical example, but not the only one.
§ Slack
§ Any “email replacement ““I’m not particularly thrilled
with building an apartment
building which has the
biggest bars on every
window” – Jeff Bonforte
(Yahoo VP mail and
messagin)E2E Encrypted Messaging
Need Want
Crypto CryptoSearchable EncryptionAn index for search
The f f f f f f f
1 5 9 52 67 72 99
Man f f f f f
7 11 21 42 67
Bites f
2
Dog f f f
21 27 31Index on client, store on server
The f1 f5 f9 f 25 f 76 f 27 f 99
Man f7 f 11 f 12 f 24 f 76
Bites f2
Dog f 12 f 72 f 13Index on client, store on server
The f1 f5 f9 f52 f67 f72 f99
Man f7 f11 f21 f42 f67
Bites f2
Dog f21 f27 f31
f 12 f 72 f 13
DogIndex on client, store on server
The f1 f5 f9 f52 f67 f72 f99
Man f7 f11 f21 f42 f67
Bites f2
Dog f21 f27 f31
The f1 f5 f9 f 25 f 76 f 27 f 99
f 12 f 72 f 13
Man f7 f 11 f 12 f 24 f 76
Dog
Bites f2
Dog f 12 f 72 f 13An index for search
The f f f f f f f
1 5 9 52 67 72 99
Man f f f f f
7 11 21 42 67
Bites f
2
Dog f f f
21 27 31A Naïvely Encrypted Index
H(k|keyWord) E(k,list of files)
8afa2
1c35f
dc4cf
9f126Index on client, store on server
8afa2
1c35f
dc4cf
9f126
8afa2
1c35f H(k|”Dog”)
dc4cf
9f126A Naïvely Encrypted Index
H(k|keyWord) E(k,list of files)
8afa2
1c35f
Leaks term frequency
dc4cf
• 8afa2 is the most
frequent keyword
9f126
• “The” is the most
frequent English word
• ……An Inefficient Encrypted index
H(k|keyWord|KeyWord_ctr)
E(k, f )
8afa2
i
§ For a given keyword, each file containing it is
1c35f
stored in a separate random location
§ This hides keyword frequency in a space
41bb
efficient way
§ Very inefficient to search:
a5l9
§ Requires one random read per result
§ Results in a ~25-50x increase in I/O usage
5r6n
§ Yahoo! Mail search is already IO bound !!!
§ Not viable for a server supporting multiple
users who are not paying for it
d4c1
d4c1Search at Cloud Scale
§ Many small indexes
§ < 1GB each
§ > 1 Billion accounts
§ Cannot store in memory
§ Must use disk storage
§ IO Bound
§ Fragmented index causes massive increase in iO for search
§ A search for one keyword returning N documents takes N
times as many reads.Good news
§ Email search queries are fairly simple
§ Typically single keyword
§ Conjunctive search nice, but not necessary
§ Most searches are on meta data
§ Searches on mail content are rare
§ ~250 searches a second across all users
§ ~300 million monthly active users
§ But we must solve the IO issue.An Inefficient Encrypted index
H(k|keyWord|KeyWord_ctr)
E(k, f )
8afa2
i
1c35f
41bb
9f126
dc4cf
d4c1
d4c1IO Efficient search for static
indexesChunked Encrypted Index
H(k|keyWord|chunk_ctr) E(k, chunk of files)
8afa2
§ Assume we have all
documents initially
1c35f
§ We break up the list into
chunks
§ Way more efficient to search
41bb
§ Can scale to terabytes
§ Cash et al (Crypto ’13, NDSS
9f126
‘14)
dc4cf
d4c1Problem: updates
H(k|keyWord|chunk_ctr) E(k, chunk of files)
8afa2
§ :“lost DOG”
§ Dog is “9f126”
1c35f
§ Need to add to “Dog” entry.
§ But … that leaks what we
41bb updated
9f126
dc4cf
d4c1Problem: updates
H(k|keyWord|chunk_ctr) E(k, chunk of files)
8afa2
§ :“lost DOG”
§ Dog is “9f126”
1c35f
§ Need to add to “Dog” entry.
§ But … that leaks what we
41bb updated
9f126
dc4cf
d4c1Problem: updates
8afa2
1c35f
41bb
9f126
dc4cf
d4c1
fe52Problem: updates
8afa2
1c35f
41bb
9f126
dc4cf
d4c1
fe52IO-DSSE: Scaling Dynamic
Searchable Encryption to Millions
of Indexes By Improving LocalityObliviously Updateable Standard search
index
Index
8afa2
1c35f
41bb
9f126
dc4cf
d4c1
fe52Obliviously Updateable
Index
8afa2
1c35f
41bb
9f126
dc4cf
d4c1
fe52Obliviously Updateable
Index
8afa2
1c35f
41bb
9f126
dc4cf
d4c1
fe52Obliviously Updateable
Index
8afa2
1c35f
41bb
9f126
dc4cf
d4c1
fe52Obliviously Updateable
Index
8afa2
1c35f
41bb
9f126
dc4cf
d4c1
fe52Obliviously Updateable
Index
?Chunked Encrypted Index
8afa2
1c35f
41bb
9f126
dc4cf
d4c1
ffee5522Buffer locally, put full chunks on server
§ Keywords have a power law distribution:
Zipfs f
1
common ones are really frequent, others are
sparse
law f f
7 11
§ We will end up with too many partial buckets
on the client
really f
2
§ We can’t upload partial buckets
kills f
2
this f f f f f f
1 5 9 52 67 72
idea f
2We need an obliviously updatable
index
overflow
Local Device Data center
k f k f f k f f f f
1 3 1 4 9 1 1, 5 , 8, 12
key-
word
k f k f f f f
k f f 2 4 4 4, 1 , 3, 9
7 4 1 qeuries
k f f f k f f f f
k f f 3 1 8 9 1 2, 3 , 8, 11
3 1 8
Obliviously Updatable Index Full Block IndexOblivious RAM
§ ORAM hides locations of access to memory (both reads and
writes)
§ How to build ORAM
1. Encrypt memory
2. “Shuffle” memory locations on reads or writes to hide locations
§ In Path ORAM, shuffling has logarithmic overhead.OUI from Path ORAM RAM
1. Read(for search)
2. Shuffle
3. Read (for search)
4. Shuffle
5. Read/write for update
6. ShufflePath ORAM
Client side stashPath ORAM
Client side stashPath ORAM
Client side stashPath ORAM
Client side stashPath ORAM
Client side stashPath ORAM
Client side stashFrom ORAM to an OUI
§ ORAM allows you to write to a location in memory without
revealing the location
§ Can add to a partial chunk without revealing we did so.
§ Bandwidth costs get worse was ORAM gets larger
§ Requires you to read and write Log(N)*B bytes for a read of B
bytes from an ORAM of size N
§ For 16GB of ORAM, server needs 32.06 GB of space and
reading 4KB takes 350KB read + 350KB write.
§ Storing full index in ORAM requires too much bandwidthFrom ORAM to an OUI
§ ORAM hides both reads and writes
§ Search explicitly leaks repeated reads
§ Same files are returned each time.
§ Same search token/hash used.
§ No need to hide reads using ORAM
§ Updates may happen in batchesOUI from Oblivious RAM
1. Read(for search)
2. Shuffle
3. Read (for search)
4. Shuffle
5. Read/write for update
6. ShufflePartial ORAM?
1. Read(for search)
2. Read (for search)
3. Read/write for update
4. ShuffleOUI
1. Read(for search)
2. Shuffle
3. Read (for search)
4. Shuffle
5. Read/write for update
6. ShuffleOUI
1. Read(for search)
2. Read (for search)
3. Shuffle + Shuffle
4. Read/write for update
5. ShuffleOUI
1. Read(for search)
2. Read (for search)
3. Shuffle + Shuffle
4. Read/write for update
5. ShuffleOUI
1. Read(for search)
2. Read (for search)
3. Shuffle + Shuffle
4. Read/write for update
5. ShuffleReadsWhy not just read directly?Leaks updatesOUI from ORAM
§ Searching triggers a read and write of Log(n)*B data
§ To avoid Log(N)*B read +write for each search
§ Just read address for chunk for given keyword
§ Defer read and write until later (i.e. when the phone is plugged in
and on Wi-Fi)
§ Search is constant bandwidth and has nice locality
§ All updates must happen after deferred IO is done
§ We get some savings from batching the IO together
§ Multiple searches on the same keyword are freeOUI from ORAM
§ Read directly from tree for search BUT
§ Must complete full path read and write prior to any updates
§ Call these “deferred” readsBatched reads and writes
§ Deferred (full reads ) reads and updates are not random events
§ They will happen in groups either
§ When an email comes in we get many updates
§ We might update the non local index only once a day (if system is not
multi client)
§ Batched reads and writes reduce the amount of data read and
written
§ For n full reads/ writes,
§ The root is only updated once instead of n times
§ Its children once instead n/2 times, etcDeferred ReadsDeferred ReadsDeferred ReadsDeferred Reads + BatchingBatched updatePerformance
IO Savings (percentage) vs
§ Simple encrypted index( including all previous
works under purely dynamic insertion)
§ Savings just for search (ignoring updates)
§ Oblivious index from path ORAMConclusion
§ Searchable encryption might be feasible for cloud based
messaging with effort
§ It pays to examine problems in context
§ You can always get better performance by relaxing security
assumptions
§ Sometimes the relaxation is inherent to the setting and freeUpdates
§ Query local, ORAM, and
index with efficient access 8afa2
1c35f
§ Update : Buffer locally, 41bb
Local
overflow to ORAM, then
9f126
buffer
commit full chunks to index
dc4cf
§ Defer ORAM I/O from
d4c1
queries until update period
Obliviously
keyword
§ Requirements updateable
data
Index
§ 40 to 250mb of client
storage to store a list of (OUI)
keywords
§ Client has fast internet
sometimes
§ Ideally, client has large
Query on keyword
local bufferPath ORAM
Client side stashPath ORAM
Client side stashPath ORAM
Client side stashPath ORAM
Client side stashPath ORAM
Client side stashPath ORAM
Client side stashFrom ORAM to an OUI
§ ORAM allows you to write to a location in memory without
revealing the location
§ Can add to a partial chunk without revealing we did so.
§ Bandwidth costs get worse was ORAM gets larger
§ Requires you to read and write Log(N)*B bytes for a read of B
bytes from an ORAM of size N
§ For 16GB of ORAM, server needs 32.06 GB of space and
reading 4KB takes 350KB read + 350KB write.
§ Storing full index in ORAM requires too much bandwidthFrom ORAM to an OUI
§ ORAM hides both reads and writes
§ Search explicitly leaks repeated reads
§ Same files are returned each time.
§ Same search token/hash used.
§ No need to hide reads using ORAM
§ Updates may happen in batchesReadsWhy not just read directly?Leaks updatesOUI from ORAM
§ Read directly from tree for search BUT
§ Must complete full path read and write prior to any updates
§ Call these “deferred” readsBatched reads and writes
§ Deferred (full reads ) reads and updates are not random events
§ They will happen in groups either
§ When an email comes in we get many updates
§ We might update the non local index only once a day (if system is not
multi client)
§ Batched reads and writes reduce the amount of data read and
written
§ For n full reads/ writes,
§ The root is only updated once instead of n times
§ Its children once instead n/2 times, etcDeferred ReadsDeferred ReadsDeferred ReadsDeferred Reads + BatchingBatched update