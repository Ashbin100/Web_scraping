ObliviSync
Prac%cal Oblivious File Backup and
Synchroniza%on
Adam J Aviv Seung Geol Choi Travis Mayberry Daniel S. Roche
United States Naval Academy
Annapolis, MDMeta Data ProtecAon
2Meta Data Threat
e.g., Access PaDerns
Cloud Provider
ER Doctor
write
read
Meta-data could reveal to a cloud provider
Oncologist informaAon about the paAent, even if the
records are encrypted!
3Oblivious RAMs (ORAMs)
Threat Model:
Preven0ng the cloud provider from learning
which files are accessed and when
Cloud Provider
write
ORAM
oblivious
read Algorithm
access
4DropBox
Cloud SynchronizaAon SeSng
• Store a local copy of files
Reading is Oblivious
across mulAple computers
(occurs locally)
• Synchronizes writes
to other clients’ local
copies
Wri7ng needs protec7ng
(revealed to cloud)
5Write Only Oblivious RAM
[BMNO-CCS’14]
No
read(a) communica7on
Cost
write(a, “foo”)
O(1) / Constant
Communica7on
Cost
aa b c
a
a
d
f e
Local Copies 6ObliviSync
b c
a d
f e
7Our Contribu0on:
ObliviSync
• Adap%ng Write-Only ORAM with the Cloud Synchroniza%on and Backup
Model
• Specifically model a‘er DropBox like systems
– Seamless file system integraAon
– Seamless oblivious synchronizaAon across clients
• Strong Security and Efficient Design
– Write Oblivious and Timing ADack protecAon
– Small overhead, 4x compared to non-private stores
– Variable Size Files
• RealisAc ImplementaAon
– Implemented using FUSE
– Seamlessly works with Dropbox
8OBLIVISYNC DESIGN
9ObliviSync Components
Read/Write Client
Local Storage Backend User Facing Frontend
file&
file&
block&
block&
file&
block&
block& file&
Cloud Synchronized Folder
Write
Cloud&Service&&
Reads
Read Client
ObliviSync
-
RW
FUSE
ObliviSync
-
RO
FUSE
File system is
Backend is a
“mounted”
collecAon of
into the
files for the
system using
write-only
FUSE
ORAM
Stored in a
synchronized
folder
User
interacAon
Encode a file occurs
system into through
the backend normal file
block that is system calls
efficient for
Write-Only
Oram
Read Client
mounts the
Updates to
encoded file
the backend
system with
are
FUSE but only
synchronized
enables
to other
reading
clients
10Why embed a file system?
• Why not just treat the Write-Only ORAM
as a block device?
– Efficiency and Security of the system will be strongly
dependent on avoiding unnecessary writes
– Block devices may reveal access Ames and file sizes
Read/Write Client
Local Storage Backend User Facing Frontend
file&
file&
block&
block& file&
block&
block& file&
Cloud Synchronized Folder
Write
Cloud&Service&&
Reads
Read Client
ObliviSync
-
RW
FUSE
ObliviSync
-
RO
FUSE
EEnnccooddee aa fifillee
ssyysstteemm iinnttoo
tthhee bbaacckkeenndd
bblloocckk tthhaatt iiss
eeffifficciieenntt ffoorr
WWrriittee--OOnnllyy
OOrraamm
11ObliviSync Backend: TERMINOLOGY
File-Id’s: idenAfier of File-segments: Files are Directory Entry: Root of
files stored with the broken up to fit within blocks, file system, always have
embedded file system can either be full or parAal File-Id 0
Split-block: Each
0! 1! 2! 3!
block in the backend
3 0
is parAAoned into Super
two split-blocks Block
Block Id’s: IdenAfier
4! 5! 6! 7!
for a split-block in the
4 5
backend
Superblock: Block
8! 9! 10! 11!
with Block-Id 0 used
to structural 4 4
informaAon for the
4
embedded file system
12Drip Rate = 3
Synchronizing Buffer
Drip Time= 5 (s)
66 5 4 3 0
0! 1! 2! 3!
Repacking Rules
creat() -> 6
• ExisAng file segments
33 500
Super
write(6)
filling a full split block Block
0
write(6)
does not change
locaAon
4! 5! 6! 7!
open(5,”a”)
write(5)
5 4 55
• ExisAng file segments
close(5)
filling less than a full 5
split block may only
8! 9! 10! 11!
open( 5)
move to the other split
rebaldo(c k5 )i n the pair. 44 4
close( 5)
6 4
13Summary of Design SeSngs
• Specialize File System Embedded within a Write-only ORAM
– FUSE based user facing frontend for transparent user experience
• Synchronize to Cloud at Regular Intervals (epochs)
– Buffer writes and synchronize buffer via write-oblivious operaAons
– Synchronize even when there is nothing in the buffer
(protec%on from %ming aOacks!)
• Mul0ple Clients
– Allow only one reading and wriAng client
– Can have any number of read-only clients receiving synchronizaAons
• Easily tuned to the right seOng: drip rate and drip 0me
– to the Cloud Storage Provider: the size of the backend blocking
• 4MB vs. 1MB vs. 4K blocks (Dropbox using 4MB backend)
– to the ApplicaAon: The amount and frequency of synchronizaAon
• Cloud File Syncs: Higher synchronizaAon rate with lower amounts
• Regular Backups: Lower synchronizaAon rate with higher amounts
14RESULTS
15Experimental Results
Latency
• Latency
– Insert a large number of files one at a %me
– How long does it take for each of the files to sync?
• As there is less empty space to pack in files, should
expect a decrease in performance
16Latency
(cid:6)
(cid:5)
(cid:4)
(cid:3)
(cid:2)
(cid:1)
(cid:1) (cid:1)(cid:7)(cid:2) (cid:1)(cid:7)(cid:3) (cid:1)(cid:7)(cid:4) (cid:1)(cid:7)(cid:5) (cid:1)(cid:7)(cid:6) (cid:1)(cid:7)(cid:8) (cid:1)(cid:7)(cid:9) (cid:1)(cid:7)(cid:10) (cid:1)(cid:7)(cid:11)
(cid:9)(cid:8)(cid:7)(cid:6)(cid:5)
(cid:4)(cid:3)(cid:2)(cid:1)
1024 Backend Blocks of
size 1MB
Only 5 epochs
Inserted 920 frontend files
one at a 7me each of size
1MB
(cid:19)(cid:20)(cid:4)
(cid:19)(cid:20)(cid:8) Drip Rate
Backend
(cid:19)(cid:20)(cid:11)
more full,
(cid:19)(cid:20)(cid:2)(cid:3)
harder to
- 1MB Blocks, 1GB backend storage make
- Drip Time of 30s progress
- Drip Rate of 3 blocks per epoch
- 90% usage: synchronize in 2.5 minutes
Make progress on each sync
(cid:12)(cid:13)(cid:14)(cid:15)(cid:7) (cid:16)(cid:17)(cid:18)(cid:18)
Theore7c Limit
17More Results in the Paper!
(cid:5)(cid:2)
(cid:5)(cid:1)
(cid:4)(cid:2)
(cid:4)(cid:1)
(cid:3)(cid:2)
(cid:3)(cid:1)
(cid:2)
(cid:1)
(cid:1) (cid:1)(cid:6)(cid:1)(cid:4) (cid:1)(cid:6)(cid:1)(cid:7) (cid:1)(cid:6)(cid:1)(cid:8) (cid:1)(cid:6)(cid:1)(cid:9) (cid:1)(cid:6)(cid:3) (cid:1)(cid:6)(cid:3)(cid:4) (cid:1)(cid:6)(cid:3)(cid:7) (cid:1)(cid:6)(cid:3)(cid:8) (cid:1)(cid:6)(cid:3)(cid:9) (cid:1)(cid:6)(cid:4)
18
(cid:18)(cid:17)(cid:16)(cid:15)(cid:14)(cid:13)(cid:10)(cid:3)(cid:6)(cid:12)(cid:11)(cid:2)(cid:10)(cid:9)(cid:2)(cid:8)(cid:4)(cid:7)(cid:6)(cid:5)(cid:4)(cid:3)(cid:2)(cid:1)
(cid:9)(cid:1)(cid:1)
(cid:17)(cid:18)(cid:16)(cid:19)(cid:20)(cid:21)(cid:22)(cid:23)(cid:13)(cid:24)(cid:25)(cid:12)(cid:20)(cid:26)(cid:11)(cid:27)
(cid:17)(cid:18)(cid:16)(cid:20)(cid:19)(cid:20)(cid:21)(cid:22)(cid:23)(cid:13)(cid:24)(cid:12)(cid:11)(cid:28)(cid:29)(cid:27)
(cid:8)(cid:1)(cid:1) (cid:30)(cid:23)(cid:13)(cid:14)(cid:21)(cid:24)(cid:25)(cid:12)(cid:20)(cid:26)(cid:11)(cid:27)
(cid:30)(cid:23)(cid:13)(cid:14)(cid:21)(cid:24)(cid:12)(cid:11)(cid:28)(cid:29)(cid:27)
(cid:7)(cid:1)(cid:1)
(cid:6)(cid:1)(cid:1)
(cid:5)(cid:1)(cid:1)
(cid:4)(cid:1)(cid:1)
(cid:3)(cid:1)(cid:1)
(cid:2)(cid:1)(cid:1)
(cid:1)
(cid:1) (cid:1)(cid:10)(cid:1)(cid:3) (cid:1)(cid:10)(cid:1)(cid:5) (cid:1)(cid:10)(cid:1)(cid:7) (cid:1)(cid:10)(cid:1)(cid:9) (cid:1)(cid:10)(cid:2) (cid:1)(cid:10)(cid:2)(cid:3) (cid:1)(cid:10)(cid:2)(cid:5) (cid:1)(cid:10)(cid:2)(cid:7) (cid:1)(cid:10)(cid:2)(cid:9) (cid:1)(cid:10)(cid:3)
(cid:10)(cid:11)(cid:12)(cid:13)(cid:6)(cid:14)(cid:15)(cid:16)(cid:16)
(cid:18)(cid:17)(cid:16)(cid:15)(cid:14)(cid:13)(cid:10)(cid:3)(cid:6)(cid:12)(cid:11)(cid:2)(cid:10)(cid:9)(cid:2)(cid:8)(cid:4)(cid:7)(cid:6)(cid:5)(cid:4)(cid:3)(cid:2)(cid:1)
(cid:3)
(cid:1)(cid:2)(cid:11)
(cid:1)(cid:2)(cid:10)
(cid:1)(cid:2)(cid:9)
(cid:1)(cid:2)(cid:8)
(cid:1)(cid:2)(cid:7)
(cid:1)(cid:2)(cid:6)
(cid:1)(cid:2)(cid:5)
(cid:1)(cid:2)(cid:4)
(cid:1)(cid:2)(cid:3)
(cid:1)
(cid:4)(cid:1)(cid:2) (cid:4)(cid:1)(cid:3) (cid:4)(cid:1)(cid:4) (cid:4)(cid:1)(cid:5) (cid:4)(cid:1)(cid:6) (cid:4)(cid:7)(cid:8) (cid:4)(cid:7)(cid:1) (cid:4)(cid:7)(cid:7)
Comparison running on DropBox
(cid:19)(cid:20)(cid:21)(cid:22)(cid:23)(cid:22)(cid:15)(cid:16)(cid:17)(cid:14)(cid:24)(cid:25)(cid:13)(cid:22)(cid:26)(cid:12)(cid:27)
(cid:19)(cid:20)(cid:21)(cid:23)(cid:22)(cid:15)(cid:16)(cid:17)(cid:14)(cid:24)(cid:13)(cid:12)(cid:28)(cid:18)(cid:27)
(cid:29)(cid:17)(cid:14)(cid:30)(cid:15)(cid:24)(cid:25)(cid:13)(cid:22)(cid:26)(cid:12)(cid:27)
(cid:29)(cid:17)(cid:14)(cid:30)(cid:15)(cid:24)(cid:13)(cid:12)(cid:28)(cid:18)(cid:27)
(cid:11)(cid:12)(cid:13)(cid:14)(cid:10)(cid:15)(cid:16)(cid:17)(cid:14)(cid:12)(cid:18)
(cid:14)(cid:5)(cid:17)(cid:16)(cid:11)(cid:9)(cid:6)(cid:5)(cid:9)(cid:11)(cid:15)(cid:14)(cid:13)(cid:7)(cid:6)(cid:12)(cid:5)(cid:11)(cid:10)(cid:9)(cid:8)(cid:7)(cid:6)(cid:5)(cid:4)(cid:2)(cid:3)(cid:2)(cid:1)
Realis7c File Sizes
(cid:9)(cid:1)(cid:1)
(cid:4)(cid:1)(cid:26)(cid:27)(cid:13)(cid:28)(cid:28)
(cid:7)(cid:1)(cid:26)(cid:27)(cid:13)(cid:28)(cid:28)
(cid:8)(cid:1)(cid:1) (cid:9)(cid:7)(cid:26)(cid:27)(cid:13)(cid:28)(cid:28)
(cid:7)(cid:1)(cid:1)
(cid:6)(cid:1)(cid:1)
(cid:5)(cid:1)(cid:1)
(cid:4)(cid:1)(cid:1)
(cid:3)(cid:1)(cid:1)
(cid:2)(cid:1)(cid:1)
(cid:1)
(cid:1) (cid:1)(cid:10)(cid:2) (cid:1)(cid:10)(cid:3) (cid:1)(cid:10)(cid:4) (cid:1)(cid:10)(cid:5) (cid:1)(cid:10)(cid:6) (cid:1)(cid:10)(cid:7) (cid:1)(cid:10)(cid:8) (cid:1)(cid:10)(cid:9) (cid:1)(cid:10)(cid:11)
(cid:12)(cid:13)(cid:14)(cid:14)(cid:15)(cid:16)(cid:17)(cid:18)(cid:19)(cid:15)(cid:20)(cid:21)(cid:22)(cid:23)(cid:15)(cid:24)(cid:25)
(cid:10)(cid:9)(cid:8)(cid:7)(cid:6)(cid:5)(cid:4)(cid:3)(cid:2)(cid:1)
Throughput Measurements
(cid:20)(cid:21)(cid:4)
(cid:20)(cid:21)(cid:7)
(cid:20)(cid:21)(cid:11)
(cid:20)(cid:21)(cid:2)(cid:3)
(cid:12)(cid:13)(cid:14)(cid:15)(cid:10)(cid:16)(cid:17)(cid:18)(cid:15)(cid:13)(cid:19)Takeaways
• Oblivious Synchroniza0on Services is PRACTICAL
– Reads are already Oblivious, need to protect writes
– Leverage properAes of the applicaAon
– Small communicaAon overhead: 4x
• ObliviSync
– AdapAng Write-Only ORAM with a specialized
Filed System
– Handles variable size files
– Is NOT suscepAble to Aming aDacks
– Tunable to the applicaAon
– Implemented for a DropBox-like applicaAon that is transparent
to the user
19THANKS! Ques7ons?
ObliviSync
Prac%cal Oblivious File Backup and Synchroniza%on
Adam J Aviv Seung Geol Choi Travis Mayberry Daniel S. Roche
United States Naval Academy
Annapolis, MD
Code Repository
hDps://github.com/oblivisync/oblivisync
YouTube Video
hDps://youtu.be/-MYgDs_sO8
20Super Superblock
Block
• Mapping of File-Id to Block-Id
– Directory entry maps filenames to File-Id’s
– Read (and wriDen) on every access to the
system
File-Id Cache
• Use a 2-level B-tree
(0,13) | (3,12) | (5,12) | (2,10) | ...
– B-Tree root is stored in the super block
– Each leaf node is treated like a block in the
B-Tree Root
system and referenced by its Block-Id
– With large blocks only need one level for
15 20 55 …
most systems
• Cache of recent mappings 21! 3! 75! 12!
– Improves access Ame
– All changes can occur within the super
block without having to access leaf nodes … … … …
21FUSE
• File System in User Space
– A process intercepts all I/O
system calls
Read/Write Client
Local Storage Backend User Facing Frontend
• FUSE mounts the embedded
file&
file&
file system such that it appears bloc bk lo& ck& file&
block&
block& file&
like any other directory to the
Cloud Synchronized Folder
user
Write
Cloud&Service&&
• FUSE client also maintains the
Reads
directory entry and is aware of
Read Client
the underlying ObliviSync
System for efficiency
ObliviSync
-
RW
FUSE
ObliviSync
-
RO
FUSE
22DetecAng Stale Data
• How do we recognize if
data is stale? 3!
– Perform a lookup in the 5
superblock for the File-Id
– If Block-Id is not listed it
must be stale
Super
5 (5,2)!
Block
23Throughput
(cid:9)(cid:1)(cid:1)
(cid:8)(cid:1)(cid:1)
(cid:7)(cid:1)(cid:1)
(cid:6)(cid:1)(cid:1)
(cid:5)(cid:1)(cid:1)
(cid:4)(cid:1)(cid:1)
(cid:3)(cid:1)(cid:1)
(cid:2)(cid:1)(cid:1)
(cid:1)
(cid:1) (cid:1)(cid:10)(cid:2) (cid:1)(cid:10)(cid:3) (cid:1)(cid:10)(cid:4) (cid:1)(cid:10)(cid:5) (cid:1)(cid:10)(cid:6) (cid:1)(cid:10)(cid:7) (cid:1)(cid:10)(cid:8) (cid:1)(cid:10)(cid:9) (cid:1)(cid:10)(cid:11)
(cid:10)(cid:9)(cid:8)(cid:7)(cid:6)(cid:5)
(cid:4)(cid:3)(cid:2)(cid:1)
1024 Backend Blocks of
size 1MB
Inserted 920 frontend files
all at once each of size
Drip Rate
1MB
(cid:20)(cid:21)(cid:4)
(cid:20)(cid:21)(cid:7) Backend
(cid:20)(cid:21)(cid:11)
more full,
(cid:20)(cid:21)(cid:2)(cid:3)
harder to
make
progress
Make progress on each sync
(cid:12)(cid:13)(cid:14)(cid:15)(cid:10) (cid:16)(cid:17)(cid:18)(cid:15)(cid:13)(cid:19)
Theore7c Limit
24How long does it
take to clear the buffer?
• A Buffer of size s will clear afer O(s/(Bk)) opera7ons
– B: Size of two split block, one backend storage file
– k: is the drip rate, the number of size B files synced per epoch
• Large percentage of backend blocks that should be empty
– 20% capacity or 80% empty for fast clearance
• Does not depend on the distribu7on of file sizes
25