Automated Analysis of Privacy Requirements
for Mobile Apps
Sebastian Zimmeck∗◦ , Ziqi Wang∗, Lieyong Zou∗, Roger Iyengar†◦ , Bin Liu∗,
Florian Schaub‡◦ , Shomir Wilson§◦ , Norman Sadeh∗, Steven M. Bellovin¶ and Joel Reidenbergk
∗School of Computer Science, Carnegie Mellon University, szimmeck@andrew.cmu.edu, sadeh@cs.cmu.edu
†Department of Computer Science and Engineering, Washington University in St. Louis
‡School of Information, University of Michigan
§Department of Electrical Engineering & Computing Systems, University of Cincinnati
¶Department of Computer Science, Columbia University
kSchool of Law, Fordham University
Abstract—Mobileappshavetosatisfyvariousprivacyrequire- Snapchat’s data deletion mechanism. His report was picked
ments. Notably, app publishers are often obligated to provide a up by the Electronic Privacy Information Center and brought
privacy policy and notify users of their apps’ privacy practices. to the attention of the Federal Trade Commission (FTC),
But how can a user tell whether an app behaves as its policy which launched a formal investigation requiring Snapchat to
promises? In this study we introduce a scalable system to help implement a comprehensive privacy program.2
analyze and predict Android apps’ compliance with privacy
requirements. We discuss how we customized our system in a The case of Snapchat illustrates that mobile apps are
collaborationwiththeCaliforniaOfficeoftheAttorneyGeneral. often non-compliant with privacy requirements. However, any
Beyonditsusebyregulatorsandactivistsoursystemisalsomeant inconsistencies can have real consequences as they may lead
to assist app publishers and app store owners in their internal
to enforcement actions by the FTC and other regulators. This
assessments of privacy requirement compliance.
is especially true if discrepancies continue to exist for many
Our analysis of 17,991 free Android apps shows the viability years,whichwasthecaseforYelp’scollectionofchildrens’in-
ofcombiningmachinelearning-basedprivacypolicyanalysiswith formation.3Thesefindingsnotonlydemonstratethatregulators
staticcodeanalysisofapps.Resultssuggestthat71%ofappsthat could benefit from a system that helps them identify potential
lack a privacy policy should have one. Also, for 9,050 apps that privacy requirement inconsistencies, but also that it would
haveapolicy,wefindmanyinstancesofpotentialinconsistencies
be a useful tool for companies in the software development
between what the app policy seems to state and what the code
process. This would be valuable because researchers found
of the app appears to do. In particular, as many as 41% of
thatprivacyviolationsoftenappeartobebasedondevelopers’
these apps could be collecting location information and 17%
difficulties in understanding privacy requirements [7] rather
could be sharing such with third parties without disclosing so in
theirpolicies.Overall,eachappexhibitsameanof1.83potential than on malicious intentions. Thus, for example, tools that
privacy requirement inconsistencies. automatically detect and describe third-party data collection
practicesmaybehelpfulfordevelopers[7].Consequently,itis
I. INTRODUCTION amajormotivationofourworktohelpcompaniesidentifyred
flags before they develop into serious and contentious privacy
“We do not ask for, track, or access any location-specific
problems.
information [...].” This is what Snapchat’s privacy policy
stated.1 However, its Android app transmitted Wi-Fi- and On various occasions, the FTC, which is responsible for
cell-based location data from users’ devices to analytics regulating consumer privacy on the federal level, expressed
service providers. These discrepancies remained undetected dissatisfaction with the current state of apps’ privacy compli-
before they eventually surfaced when a researcher examined ance. Three times it manually surveyed childrens’ apps [28],
[29], [33] and concluded that the “results of the survey are
◦Part of this work was conducted while Sebastian Zimmeck was a PhD student at disappointing” [29]. Deviating from mandatory provisions,
Columbia University working in Prof. Sadeh’s group at Carnegie Mellon University.
many publishers did not disclose what types of data they
RogerIyengar,FlorianSchaub,andShomirWilsonwereallinProf.Sadeh’sgroupat
CarnegieMellonUniversitywhileinvolvedinthisresearchproject,RogerIyengarasan collect, how they make use of the data, and with whom the
NSFREUundergraduatestudent,FlorianSchaubasapost-doctoralfellow,andShomir data is shared [29]. A similar examination of 121 shopping
Wilsonasaprojectscientist.
1ComplaintIntheMatterofSnapchat,Inc.(December31,2014). apps revealed that many privacy policies are vague and fail to
convey how apps actually handle consumers’ data [32]. Given
thattheFTClimiteditsinvestigationstosmallsamplesofapps,
apresumablylargenumberofdiscrepanciesbetweenappsand
Permission to freely reproduce all or part of this paper for noncommercial purposes
their privacy policies remain undetected.
is granted provided that copies bear this notice and the full citation on the first page.
Reproduction for commercial purposes is strictly prohibited without the prior written
In this study we are presenting a privacy analysis sys-
consent of the Internet Society, the first-named author (for reproduction of an entire
paper only), and the author’s employer if the paper was prepared within the scope of tem for Android that checks data practices of apps against
employment.
NDSS ’17, 26 February - 1 March 2017, San Diego, CA, USA 2DecisionandOrderIntheMatterofSnapchat,Inc.(December31,2014).
Copyright 2017 Internet Society, ISBN 1-891562-46-0
http://dx.doi.org/10.14722/ndss.2017.23034
3UnitedStatesofAmericav.Yelp,Inc.(September17,2014).privacy requirements derived from their privacy policies and aim to make privacy policies more comprehensible [34], there
selected laws. Our work enables app publishers to identify is a glaring absence of an automated system to accurately
potentially privacy-invasive practices in their apps before they analyzepolicycontent.Inthisstudyweaimforasolution.We
are published. Moreover, our work can also aid governmental want to automate and scale the analysis of natural language
regulators, such as the FTC, to achieve a systematic enforce- privacy policies. As of now, Massey et al. provided the most
ment of privacy laws on a large scale. App store owners, extensive evaluation of 2,061 policies, however, not focusing
researchers, and privacy advocates alike might also derive ontheirlegalanalysisbutrathertheirreadabilityandsuitability
value from our study. Our main contribution consists of the for identifying privacy protections and vulnerabilities from a
novel combination ofmachinelearning(ML)andstaticanaly- requirements engineering perspective [48]. In addition, Hoke
sis techniques to analyze apps’ potential non-compliance with et al. [40] studied the compliance of 75 policies with self-
privacyrequirements.However,wewanttoemphasizethatwe regulatory requirements, and Cranor et al. analyzed structured
donotclaimtoresolvechallengesintheindividualtechniques privacynoticeformsoffinancialinstitutionsidentifyingmulti-
we leverage beyond what is necessary for our purposes. This pleinstancesofoptoutpracticesthatappeartobeinviolation
holdsespeciallytrueforthestaticanalysisofmobileappsand of financial industry laws [16].
its many unresolved problems, for example, in the analysis of
Different from previous studies we analyze policies at a
obfuscated code. That said, the details of our contribution are
large scale with a legal perspective and not limited to the
as follows:
financial industry. We analyze whether policies are available,
1) For a set of 17,991 Android apps we check whether as sometimes required by various laws, and examine their
they have a privacy policy. For the 9,295 apps that have descriptions of data collection and sharing practices. For our
one we apply machine learning classifiers to analyze analysis we rely on the flexibility of ML classifiers [72] and
policy content based on a human-annotated corpus of introduce a new approach for privacy policy feature selection.
115 policies. We show, for instance, that only 46% of Our work is informed by the study of Costante et al., who
the analyzed policies describe a notification process for presented a completeness classifier to determine which data
policy changes. (§ III). practice categories are included in a privacy policy [15]
2) Leveraging static analysis we investigate the actual data and proposed rule-based techniques to extract data collection
practices occurring in the apps’ code. With a failure practices [14]. However, we go beyond these works in terms
rate of 0.4%, a mean F-1 score of 0.96, and a mean of both breadth and depth. We analyze a much larger policy
analysis time of 6.2 seconds per app our approach makes corpus and we focus on legal questions that have not yet
large-scaleappanalysesforlegallyrelevantdatapractices been automatically analyzed. Different from many existing
feasible and reliable. (§ IV). works that focus on pre-processing of policies, e.g. by using
3) Mappingthepolicytotheappanalysisresultsweidentify topic modeling [13], [63] and sequence alignment [46], [55]
andanalyzepotentialprivacyrequirementinconsistencies to identify similar policy sections and paragraphs, we are
between policies and apps. We also construct a statistical interested in analyzing policy content.
model that helps predict such potential inconsistencies
Supervised ML techniques, as used in this study, require
based on app metadata. For instance, apps with a Top
ground-truth. To support the development of these techniques
Developer badge have significantly lower odds for the
crowdsourcing has been proposed as a viable approach for
existence of potential inconsistencies. (§ V).
gathering rich annotations from unstructured privacy poli-
4) IncollaborationwiththeCaliforniaOfficeoftheAttorney
cies [59], [68]. While crowdsourcing poses challenges due to
General we performed a preliminary evaluation of our
the policies’ complexity [56], assigning annotation tasks to
system for use in privacy enforcement activities. Results
experts [72] and setting stringent agreement thresholds and
suggestthatoursystemcanindeedhelptheirlawyersand
evaluation criteria [68] can in fact lead to reliable policy
other users to efficiently analyze salient privacy require-
annotations. However, as itisarecurringproblem thatprivacy
ments allowing them to prioritize their work towards the
policy annotations grapple with low inter-annotator agree-
most critical areas. (§ VI).
ment [56], [72], we introduce a measure for analyzing their
reliabilitybasedonthenotionthathighannotatordisagreement
II. RELATEDWORK
does not principally inhibit the use of the annotations for ML
We leverage prior work in privacy policy analysis (§ II-A), purposes as long as the disagreement is not systematic.
mobileappanalysis(§II-B),andtheircombinationtoidentify
potential privacy requirement inconsistencies (§ II-C). B. Mobile App Analysis
Different from the closest related works [22], [62], our
A. Privacy Policy Analysis
analysis of Android apps reflects the fundamental distinction
Privacy policies disclose an organization’s data practices. between first and third party data practices. Both have to
Despite efforts to make them machine-readable, for instance, be analyzed independently as one may be allowed while the
via P3P [17], natural language policies are the de-facto stan- other may not. First and third parties have separate legal
dard. However, those policies are often long and difficult relationships to a user of an app. Among the third parties,
to read. Few lay users ever read them and regulators lack ad and analytics libraries are of particular importance. Gibler
the resources to systematically review their contents. For et al. found that ad libraries were responsible for 65% of
instance,ittook26dataprotectionagenciesoneweek,working the identified data sharing with the top four accounting for
togetherastheGlobalPrivacyEnforcementNetwork(GPEN), 43% [35]. Similarly, Demetriou et al. [18] explored their
toanalyzethepoliciesof1,211apps[38].Whilevariousworks potential reach and Grace et al. [39] their security and privacy
2risks.Theyfindthatthemostpopularlibrarieshavethebiggest AsDroidtoidentifycontradictionsbetweenappsanduserinter-
impactonsharingofuserdata,and,consequently,ouranalysis facetexts[41].Kongetal.introducedasystemtoinfersecurity
of sharing practices focuses on those as well. In fact, 75% of andprivacyrelatedappbehaviorfromuserreviews[42].Gorla
apps’ location requests serve the purpose of sharing it with ad et al. [37] used unsupervised anomaly detection techniques to
networks [44]. analyze app store descriptions for outliers, and Watanabe et
al. [66] used keyword-based binary classifiers to determine
One of our contributions lies in the extension of various
whether a resource that an app accesses (e.g., location) is
app analysis techniques to achieve a meaningful analysis
mentioned in the app’s description.
of apps’ potential non-compliance with privacy requirements
derivedfromtheirprivacypoliciesandselectedlaws.Thecore Different from most previous studies we analyze apps’
functionality of our app analyzer is built on Androguard [20], behavior for potential non-compliance with privacy require-
a static analysis tool. In order to identify the recipients of mentsderivedfromtheirprivacypoliciesandselectedlaws.A
data we create a call graph [35], [66] and use PScout [6], step in this direction was provided by Bhoraskar et al., who
which is comparable to Stowaway [26], to check whether an found that 80% of ads displayed in apps targeted at children
apphastherequiredpermissionsformakingacertainAndroid linked to pages that attempt to collect personal information
API call or allowing a library to make such. Our work takes in violation of the law [8]. The closest results to our study
further ideas from FlowDroid [4], which targeted the sharing were presented by Enck et al. [22] and Slavin et al. [62].
ofsensitivedata,itsrefinementinDroidSafe[36],andtheded In an analysis of 20 apps Enck et al. found a total of 24
decompiler for Android Application Packages (APKs) [23]. potential privacy law violations caused by transmission of
However, neither of the previous works is intended for large- phone data, device identifiers, or location data [22]. Slavin
scale privacy requirement analysis. et al. proposed a system to help software developers detect
potentialprivacypolicyviolations[62].Basedonmappingsof
Static analysis is ideally suited for large-scale analysis.
76 policy phrases to Android API calls they discovered 341
However,asitwasrecentlyshown[1],[45],italsohasitslim-
such potential violations in 477 apps.
itations. First, to the extent that the static analysis of Android
appsislimitedtoJavaitcannotreachcodeinotherlanguages. Our approach is inspired by TaintDroid [22] and similar
In this regard, it was demonstrated that 37% of Android to the studies of Slavin et al. [62] and Yu et al. [69]. How-
apps contain at least one method or activity that is executed ever, we move beyond their contributions. First, our privacy
natively(i.e.,inC/C++)[1].Inadditiontonativecodethereis requirementscoverprivacyquestionspreviouslynotexamined.
another obstacle that was shown to make the static analysis of Notably, we address whether an app needs a policy and
Android apps challenging: code obfuscation. A non-negligible analyze the policy’s content (i.e., whether it describes how
amount of apps and libraries are obfuscated at the package or usersareinformedofpolicychangesandhowtheycanaccess,
code level to prevent reverse engineering [45]. Finally, static edit, and delete data). Different from Slavin et al. we also
analysis cannot be used to identify indirect techniques, such analyze the collection and sharing of contact information.
as reflection, which often occurs in combination with native Second, TaintDroid, is not intended to have app store wide
code[1].Wewilldiscusshowtheselimitationsaffectourstudy scale. Third, previous approaches do not neatly match to legal
in § IV-B. categories. They do not distinguish between first and third
party practices [22], [62], do not take into account negative
policy statements (i.e., statements that an app does not collect
C. Potential Privacy Requirement Inconsistencies
certain data, as, for example, in the Snapchat policy quoted in
While mobile app analysis has received considerable at- § I) [62], and base their analysis on a dichotomy of strong
tention, the analysis results are usually not placed into a legal and weak violations [62] unknown to the law. Fourth, we
context. However, we think that it is particularly insightful introduce techniques that achieve a mean accuracy of 0.94
to inquire whether the apps’ practices are consistent with and a failure rate of 0.4%, which improve over the closest
the disclosures made in their privacy policies and selected comparable results of 0.8 and 21% [62], respectively.
requirements from other laws. The legal dimension is an
important one that gives meaning to the app analysis results. III. PRIVACYPOLICYANALYSIS
Forexample,forappsthatdonotprovidelocationservicesthe
In this section we present our automated large-scale ML
transfer of location data may appear egregious. Yet, a transfer
analysis of privacy policies. We discuss the law on privacy
mightbepermissibleundercertaincircumstancesifadequately
notice and choice (§ III-A), our evaluation of how many apps
disclosed in a privacy policy. Only few efforts have attempted
have a privacy policy (§ III-B), and the analysis of policy
to combine code analysis of mobile apps with the analysis of
content (§ III-C).
privacy policies, terms of service, and selected requirements.
Such analysis can identify discrepancies between what is
stated in a legal document and what is actually practiced in A. Privacy Notice and Choice
reality.Wearefillingthisvoidbyidentifyingpotentialprivacy
The privacy requirements analyzed here are derived from
requirement inconsistencies through connecting the analyses
selected laws and apps’ privacy policies. If a policy or app
of apps, privacy policies, and privacy laws.
does not appear to adhere to a privacy requirement, we define
Variousstudies,e.g.,[71],[70],demonstratedhowtocreate a potential privacy requirement inconsistency to occur (which
privacy documentation or even privacy policies from program wealsorefertoaspotentialinconsistencyornon-compliance).
code. Other works focused on comparing program behavior In this regard, we caution that a potential inconsistency does
with non-legal texts. For example, Huang et al. proposed notnecessarilymeanthatalawisviolated.First,notallprivacy
3Act of 1998 (COPPA) makes policies mandatory for apps
Privacy Notice and Choice
directed to or known to be used by children.7 Thus, we treat
the existence of a privacy policy as a privacy requirement.
CalOPPA and DOPPA demand that privacy policies de-
scribe the process by which users are notified of policy
changes.8 COPPAalsorequiresdescriptionofaccess,edit,and
deletionrights.9UndertheFTCFIPPs[27]aswellasCalOPPA
and DOPPA those rights are optional.10 We concentrate our
analysis on a subset of data types that are, depending on
the context, legally protected: device IDs, location data, and
contact information. App publishers are required to disclose
Notices Notices the collection of device IDs (even when hashed) and location
data.11 Device IDs and location data are also covered by
CalOPPA12 and for childrens’ apps according to COPPA.13
The sharing of these types of information with third parties
* IP, Android/Device ID, MAC, IMEI, Implementation
Google Ad ID and Client ID, ... requiresconsentaswell.14 Ourdefinitionofsharingcoversthe
** GPS, Cell Tower, Wi-Fi, ... direct data collection by third parties from first party apps.15
*** E-Mail, Phone Number, ...
BeyonddeviceIDsandlocationdata,contactinformation,such
Fig. 1: Per our privacy requirements, apps that process as e-mail addresses, may be protected, too.16
Personally Identifiable Information (PII) need to (1) have a
privacy policy, (2-3) include notices about policy changes and It should be noted that we interpret ad identifiers to be PII
access, edit, and deletion rights in their policy, (4-6) notify since they can be used to track users over time and across
users of data collection practices, and (7-9) disclose how devices. We are also assuming that a user did not opt out
data is shared with third parties. The notice requirements for of ads (because otherwise no ad identifiers would be sent to
policy changes and access, edit, and deletion are satisfied by opted out ad networks). We further interpret location data to
including the notices in the policies while the collection and particularly cover GPS, cell tower, and Wi-Fi locations. We
sharing practices must be also implemented in the apps. We assume applicability of the discussed laws and perform our
consider collection and sharing implemented in an app if it analysis based on the guidance provided by the FTC and
sends data to a first or third party server, respectively. Thus, the California Office of the Attorney General (Cal AG) in
it is not enough if data is kept on the phone but never sent. enforcement actions and recommendations for best practices
(e.g.,[27]and[11]).Specifically,weinterprettheFTCactions
as disallowing the omission of data practices in policies and
requirements might be applicable to all apps and policies. assumethatsilenceonapracticemeansthatitdoesnotoccur.17
Second, our system is based on a particular interpretation of Finally, we assume that all apps in the US Play store are
the law. While we believe that our interpretation is sound and subjecttoCalOPPAandDOPPA.18 Webelievethisassumption
in line with the enforcement actions of the FTC and other is reasonable as we are not aware of any US app publisher
regulatory agencies, reasonable minds may differ.4 Third, our excluding California or Delaware residents from app use or
system is based on machine learning and static analysis and, providing state-specific app versions.
thus, by its very nature errors can occur. Figure 1 provides
an overview of the law on notice and choice and the nine
B. Privacy Policy Requirement
privacy requirements that our system analyzes (Privacy Policy
Requirement, NPC, NAED, CID, CL, CC, SID, SL, SC). To assess whether apps fulfill the requirement of having
a privacy policy we crawled the Google Play store and
As to the privacy policy requirement, there is no generally downloaded a sample (n = 17,991) of free apps (full app
applicablefederalstatutedemandingprivacypoliciesforapps. set).19 We started our crawl with the most popular apps and
However, California and Delaware enacted comprehensive
followed random links on their Play store pages to other apps.
online privacy legislation that effectively serves as a national
We included all categories in our crawl, however, excluded
minimum privacy threshold given that app publishers usually
Google’s Designed for Families program (as Google already
donotprovidestate-specificappversionsorexcludeCalifornia
or Delaware residents. In this regard, the California Online
716CFR§312.4(d).
Privacy Protection Act of 2003 (CalOPPA) requires online 8Cal.Bus.&Prof.Code§22575(b)(3),Del.CodeTit.6§1205C(b)(3).
services that collect PII to post a policy.5 The same is true 916CFR§312.4(d)(3).
according to Delaware’s Online Privacy and Protection Act 10Cal.Bus.&Prof.Code§22575(b)(2),Del.CodeTit.6§1205C(a).
(DOPPA).6 In addition, the FTC’s Fair Information Practice 11IntheMatterofNomiTechnologies,Inc.(September3,2015).
Principles (FTC FIPPs) call for consumers to be given notice
12Cal.Bus.&Prof.Code§22577(a)(6)and(7)[11].
1316CFR§312.2(7)and(9).
of an entity’s information practices before any PII is col-
14Complaint In the Matter of Goldenshores Technologies, LLC, and Erik M. Geidl
lected [27]. Further, the Children’s Online Privacy Protection (April9,2014).
15Cal.Bus.&Prof.Code§22575(b)(6),Del.CodeTit.6§1205C(b)(6).
4WearefocusingontheUSlegalsystemaswearemostfamiliarwithit.However,in 16ComplaintIntheMatterofSnapchat,Inc.(December31,2014).
principle,ourtechniquesareapplicabletoanycountrywithaprivacynoticeandchoice 17ComplaintIntheMatterofSnapchat,Inc.(December31,2014).
regime. 18Cal.Bus.&Prof.Code§§22575–22579,Del.CodeTit.6§1205C.
5Cal.Bus.&Prof.Code§22575(a). 19WheneverwerefertotheGooglePlaystorewemeanitsUSsite.Detailsonthe
6Del.CodeTit.6§1205C(a). variousappandpolicysetsthatweareusingaredescribedinAppendixA.
4Apps have Policy link? Apps need policy? 90%
17,991 8,696 80%
12% No (PII not processed)
7,676 70%
17%
6,198 No (Policy Elsewhere) 60%
No 48%
71% Yes 50%
40%
9,295 0 2010 2012 2014 2016
Last Update Year
Yes 52%
0
Fig. 2: We analyze 17,991 free apps, of which 9,295 (52%)
link to their privacy policy from the Play store (left). Out of
theremainingapps,6,198(71%)appeartolackapolicywhile
engaging in at least one data practice (i.e., PII is processed)
that would require them to have one (right).
requires apps in this program to have a policy) and Android
Wear (as we want to focus on mobile apps). We assume
that our sample is representative in terms of app categories,
which we confirmed with a two-sample Kolmogorov-Smirnov
goodness of fit test (two-tailed) against a sample of a million
apps [49]. We could not reject the null hypothesis that both
were drawn from the same distribution (i.e., p > 0.05).
However, while the Play store hosts a long tail of apps that
have fewer than 1K installs (56%) [49], we focus on more
popular apps as our sample includes only 3% of such apps.
Potential Privacy Policy Requirement Inconsistencies. Out
of all policies in the full app set we found that n = 9,295
apps provided a link to their policy from the Play store (full
policy set) and n = 8,696 apps lacked such. As shown in
Figure 2, our results suggest that 71% (6,198/8,696) apps
without a policy link are indeed not adhering to the policy
requirement. We used the Play store privacy policy links as
proxies for actual policies, which we find reasonable since
regulators requested app publishers to post such links [30],
[11] and app store owners obligated themselves to provide the
necessary functionality [10]. The apps in the full app set were
offered by a total of 10,989 publishers, and their app store
pages linked to 6,479 unique privacy policies.
We arrive at 71% after making two adjustments. First,
if an app does not have a policy it is not necessarily non-
compliant with the policy requirement. After all, apps that are
not processing PII are not obligated to have a policy. Indeed,
wefoundthat12%(1,020/8,696)ofappswithoutapolicylink
are not processing PII and, thus, accounted for those apps.
Second, despite the regulators’ requests to post policy links
in the Play store, some app publishers may still decide to
post their policy elsewhere (e.g., inside their app). To account
for that possibility we randomly selected 40 apps from our
full app set that did not have a policy link in the Play store
but processed PII. We found that 83% (33/40) do not seem
to have a policy posted anywhere (with a Clopper-Pearson
confidence interval (CI) ranging from 67% to 93% at the 95%
kniL
yciloP
o/w
sppA
%
50%
January 23, 2015
40%
30% May 13, 2015
20% March 15, 2015
10%
0% May 2, 2015 December 31, 2015
500−1K 50K−100K 5M−10M
Number of Installs
kniL
yciloP
o/w
sppA
%
Fig.3:Alinearregressionmodelwiththelastappupdateyear
as independent variable and the percentage of apps without a
policy link as dependent variable gives r2 = 0.79 (top). In
addition, a polynomial regression model using the number of
installs as independent variable results in a multiple r2 =0.9
(bottom).
level based on a two-tailed binomial test).20 Thus, accounting
for an additional 17% (1,478/8,696) of apps having a policy
elsewhere leaves us with 100% − 12% − 17% = 71% out
of n = 8,696 apps to be potentially non-compliant with the
policy requirement.
Predicting Potential Privacy Policy Requirement Inconsis-
tencies.Asitappearsthatappswithfrequentupdatestypically
have a policy, we evaluate this hypothesis on our full app set
using Pearson’s Chi square test of independence. Specifically,
it is our null hypothesis that whether an app has a policy is
independent fromtheyearwhenitwasmostrecentlyupdated.
As the test returns p ≤0.05, we can reject the null hypothesis
at the 95% confidence level. Indeed, as shown in the linear
regression model of Figure 3 (top), apps with recent update
years have more often a policy than those that were updated
longer ago. In addition to an app’s update year there are
otherviablepredictors.Asshowninthepolynomialregression
modelofFigure3(bottom)thenumberofinstallsisinsightful.
Appswithhighinstallrateshavemoreoftenapolicythanapps
with average install rates (p ≤ 0.05). Surprisingly, the same
is also true for apps with low install rates. An explanation
could be that those are more recent apps that did not yet
gain popularity. Indeed, apps with low install rates are on
average more recently updated than apps with medium rates.
For example, apps with 500 to 1K installs were on average
updated on March 15, 2015 while apps with 50K to 100K
installs have an average update date as of January 23, 2015.
Further, apps with an Editors’ Choice or Top Developer
badge usually have a policy, which is also true for apps that
offerin-apppurchases.Itisfurtherencouragingthatappswith
a content rating for younger audiences often have a policy.
20AllCIsinthispaperarebasedonatwo-tailedbinomialtestandtheClopper-Pearson
intervalatthe95%level.
5Practice No. Ann Ag % Ag Fleiss /Krip
pol pol pol pol Zoe 1 1 1 0.26 0.8 1 1 0.65
NPC 395 86/115 75% 0.64 Ray 0.26 0.54 1 1 0.65 0.33 0.33 0.05
NAED 414 80/115 70% 0.59 Mae 0.87 0.65 0.35 0.02 0.43 1 0.33 0.21 p val 1u .e 00s
CID 449 92/115 80% 0.72 Liv 0.94 0.26 0.7 0.54 0.21 1 0.33 0.8
Ira 0.32 0.32 0.9 0.94 0.74 1 1 0.87 0.75
CL 326 85/115 74% 0.64
Gil 1 0.04 0.01 0.05 0.01 0.26 0.7 0.21 0.50
CC 830 86/115 75% 0.5 Dan 1 0.86 1 1 0.94 1 1 0.87
0.25
SID 90 101/115 88% 0.76 Bob 0.26 1 1 1 0.41 1 1 1
Bea 0.11 0.97 1 0.74 1 0.7 0.26 0.87
SL 51 95/115 83% 0.48
Ann 0.35 0.91 0.26 1 0.91 0.33 1 0.87
SC 276 85/115 74% 0.58 NPC (4.5) NAED (6.6) CID (5.1) CL (5.1) CC (6) SID (1.2) SL (1.8) SC (5.1)
TABLE I: Absolute numbers of annotations (No. Ann) and
Zoe 0.11 0.7 1 0.7 1 0.56 0.32 0.33
various agreement measures, specifically, absolute agree- Ray 1 1 0.04 0.7 0.7 0.26 0.74 1
ments (Ag ), percentage agreements (% Ag ), Fleiss’ κ Mae 0.54 0.54 1 1 1 0.7 1 0.74 p val 1u .e 00s
pol pol Liv 1 0.65 1 0.56 1 1 0.87 1
(Fleiss pol), and Krippendorff’s α (Krip pol). All agreement Ira 1 0.91 1 1 1 0.56 1 0.94 0.75
measures are computed on the full corpus of 115 policies Gil 0.02 1 1 1 1 0.41 1 1 0.50
Dan 1 0.7 1 0 0.56 1 0 0.41
and on a per-policy basis (e.g., for 92 out of 115 policies 0.25
Bob 0.26 0.56 1 1 0.33 0.33 0.33 0.26
the annotators agreed on whether the policy allows collection Bea 0.87 0.26 0.33 1 0.56 1 1 0.41
of identifiers). Ann 0.41 0.74 0.7 0.56 0.02 1 0.65 0.11
NPC (4.2) NAED (3.9) CID (1.8) CL (3.9) CC (2.7) SID (2.7) SL (4.2) SC (3.9)
Fig. 4: Analysis of systematic disagreement among anno-
Most apps for Everyone 10+ (75%), Teen (65%), and Mature tators for the different data practices with binomial tests.
17+ (66%) audiences have a policy while apps that have an Larger p values mean fewer disagreements. If there are no
Everyone rating (52%) or are unrated (30%) often lack one.21 disagreements, we define p = 1. An annotator can be in
Further, various app categories are particularly susceptible for the minority when omitting an annotation that the two other
not having a policy. Apps in the Comics (20%), Libraries annotatorsmade(top)oraddinganextraannotation(bottom).
& Demo (10%), Media & Video (28%), and Personalization Ourresultsshowfewinstancesofsystematicdisagreement.The
(28%) categories have particularly low policy percentages, as numbersinparenthesesshowtheaveragenumbersofabsolute
comparedtoanaverageof52%ofappshavingapolicyacross disagreements per annotator for the respective practices.
categories. Combining these predictors enables us to zoom
in to areas of apps that are unlikely to have a policy. For
instance, in the Media & Video category the percentage of of 0.72 and 0.76 is fair. However, it is below 0.67 for the
apps with a policy decreases from 28% for rated apps to 12% remaining classes. While we would have hoped for stronger
for unrated apps. A similar decrease occurs in the Libraries & agreement,theannotationswiththeobservedagreementlevels
Demo category from 10% to 8%. can stillprovide reliable ground-truth as long as the classifiers
are not misled by patterns of systematic disagreement, which
C. Privacy Policy Content canbeexploredbyanalyzingthedisagreeingannotations[57].
Wenowmovefromexaminingwhetheranapphasapolicy To analyze whether disagreements contain systematic pat-
totheanalysisofpolicycontent(i.e.,privacyrequirements2-9 terns we evaluate how often each annotator disagrees with the
in Figure 1). As a basis for our evaluation we use manually other two annotators. If he or she is in a minority position
created policy annotations. for a statistically significant number of times, there might be
a misunderstanding of the annotation task or other systematic
1)Inter-annotator Agreement: For training and testing of
reason for disagreement. However, if there is no systematic
ourpolicyclassifiersweleveragetheOPP-115corpus[67]—a
disagreement, annotations are reliable despite low agreement
corpus of 115 privacy policies annotated by ten law students
levels [57]. Assuming a uniform distribution each annotator
that includes 2,831 annotations for the practices discussed
should be in the minority in 1/3 of all disagreements. We
in this study. The annotations, which are described in detail
test this assumption with the binomial test for goodness of
in[67],serveastheground-truthforevaluatingourclassifiers.
fit. Specifically, we use the binomial distribution to calculate
Eachannotatorannotatedameanof34.5policies(median35).
the probability of an annotator being x or more times in the
We select annotations according to majority agreement (i.e.,
minoritybyaddinguptheprobabilityofbeingexactlyxtimes
two out of three annotators agreed on it). As it is irrelevant
in the minority, being x+1 times in the minority, up to x+n
from a legal perspective how often a practice is described in
(thatis,beingalwaysintheminority),andcomparingtheresult
a policy, we measure whether annotators agree that a policy
to the expected probability of 1/3. We use a one-tailed test as
describes a given practice at least once.
we are not interested in finding whether an annotator is fewer
Highinter-annotatoragreementsignalsthereliabilityofthe times in the minority than in 1/3 of the disagreements.
ground-truthonwhichclassifierscanbetrainedandtested.As
agreement measures we use Fleiss’ κ and Krippendorff’s α, As shown in Figure 4, we only found few cases with
which indicate that agreement is good above 0.8, fair between systematicdisagreement.Morespecifically,for7%(11/160)of
0.67 and 0.8, and doubtful below 0.67 [47]. From our results disagreements we found statistical significance (p ≤0.05) for
in Table I it follows that the inter-annotator agreement for rejecting the null hypothesis at the 95% confidence level that
collection and sharing of device IDs with respective values the disagreements are equally distributed. We see that nearly
half of the systematic disagreements occur for Gil. However,
21RatingsfollowtheEntertainmentSoftwareRatingBoard(ESRB)[24]. excludingGil’sandotheraffectedannotationsfromthetraining
6Base Acc 95% CI Prec Rec F-1 F-1 Pos
Practice Classifier Parameters pol neg neg neg pos
(n=40) (n=40) (n=40) (n=40) (n=40) (n=40) (n=40) (n=9,050)
NPC SVM RBF kernel, weight 0.7 0.9 0.76–0.97 0.79 0.92 0.85 0.93 46%
NAED SVM linear kernel 0.58 0.75 0.59–0.87 0.71 0.71 0.71 0.78 36%
CID Log. Reg. LIBLINEAR solver 0.65 0.83 0.67–0.93 0.77 0.71 0.74 0.87 46%
CL SVM linear kernel 0.53 0.88 0.73–0.96 0.83 0.95 0.89 0.86 34%
CC Log. Reg. LIBLINEAR, L2, weight 0.8 0.88 0.73–0.96 0.71 0.63 0.67 0.92 56%
SID Log. Reg. LBFGS solver, L2 0.88 0.88 0.73–0.96 0.94 0.91 0.93 0.55 10%
SL SVM linear kernel, weight 0.95 0.93 0.8–0.98 0.97 0.95 0.96 - 12%
SC SVM poly kernel (4 degrees) 0.73 0.78 0.62–0.89 0.79 0.93 0.86 0.47 6%
TABLE II: Classifiers, parameters, and classification results for the policy test set (n=40) and the occurrence of positive
classifications (Pos) in a set of n=9,050 policies (full app/policy set). We obtained the best results by always setting the
regularization constant to C = 1 and for NPC, CC, and SL adjusting weights inversely proportional to class frequencies
with scikit-learn’s class_weight (weight). Except for the SL practice, all classifiers’ accuracies (Acc ) reached or exceeded
pol
the baseline (Base) of always selecting the most often occurring class in the training set. Prec , Rec , and F-1 are the
neg neg neg
scores for the negative classes (e.g., data is not collected or shared) while F-1 is the F-1 score for positive classes.
pos
using a second set of keywords that refers to the actions of a
def location_feature_extraction(policy):
1 datapractice(e.g.,forthelocationsharingpracticeshareand
2 partner) to create unigram and bigram feature vectors [72].
data_type_keywords = [’geo’, ’gps’]
3 Thus, for example, if the keyword “share” is encountered, the
action_keywords = [’share’, ’partner’]
4
bigrams “not share” or “will share” would be extracted if the
relevant_sentences = ’’
5
feature_vector = ’’ words before “share” are “not” and “will,” respectively. The
6
feature vectors created from bigrams (and unigrams) are then
7
for sentence in policy: usedtoclassifythepractices.Ifnokeywordsareextracted,the
8
9 for keyword in data_type_keywords: classifier will select the negative class. We applied the Porter
10 if (keyword in sentence): stemmer to all processed text.
relevant_sentences += sentence
11
For finding the most meaningful features as well as for
12
words = tokenize(relevant_sentences) the subsequent classifier tuning we performed nested cross-
13
14 bigrams = ngrams(words,2) validationwith75policiesseparatedintotenfoldsintheinner
15 loop and 40 randomly selected policies as held out test set
16 for bigram in bigrams: (policy test set). We used the inner cross-validation to select
for keyword in action_keywords:
17 the optimal parameters during the classifier tuning phase and
if (keyword in bigram):
18 theheldoutpolicytestsetforthefinalmeasureofclassification
feature_vector += bigram, bigram[0],
19 performance. We stratified the inner cross-validation to avoid
bigram[1]
misclassifications due to skewed classes. After evaluating the
20
return feature_vector performanceofourclassifierswiththepolicytestsetweadded
21
the test data to the training data for the final classifiers to be
Listing 1: Pseudocode for the location sharing practice.
used in our large-scale analysis.
3)Classification: During the tuning phase we prototyped
set for our classifiers had only little noticeable effect. For
various classifiers with scikit-learn [51], a Python library.
some practices the classification accuracy slightly increased,
Support vector machines and logistic regression had the best
for others it slightly decreased. Thus, we believe that our
performance. We selected classification parameters individu-
annotations are sufficiently reliable to serve as ground-truth
ally for each data practice.
for our classifiers. As other works have already explored, low
levels of agreement in policy annotations are common and do ClassifierPerformanceforPolicyTestSet.Theclassification
not necessarily reflect their unreliability [56], [72]. In fact, resultsforourpolicytestset,showninTableII,suggestthatthe
different from our approach here, it could be argued that an ML analysis of privacy policies is generally feasible. For the
annotator’s addition or omission of an annotation is not a negative classifications our classifiers achieve F-1 neg scores
disagreement with the others’ annotations to begin with. between 0.67 and 0.96. These scores are the most important
measures for our task because the identification of a potential
2)Feature Selection: One of the most important tasks
privacy requirement inconsistency demands that a practice
for correctly classifying data practices described in privacy
occurring in an app is not covered by its policy (§ V-A).
policies is appropriate feature selection. Listing 1 shows a
Consequently, it is less problematic that the sharing practices,
simplified example of our algorithm for the location sharing
whichareskewedtowardsthenegativeclasses,haverelatively
practice. Using information gain and tf-idf we identified the
low F-1 scores of 0.55 (SID) and 0.47 (SC) or could not
most meaningful keywords for each practice and created sets pos
be calculated (SL) due to a lack of true positives in the policy
ofkeywords.Onesetofkeywordsreferstothedatatypeofthe
test set.
practices (e.g., for the location sharing practice geo and gps)
andisusedtoextractallsentencesfromapolicythatcontainat Classification Results for Full App/Policy Set. We applied
leastoneofthekeywords.Ontheseextractedsentencesweare our classifiers to the policies in the full app/policy set with
7Fig. 5: (1) Our system first crawls the US Google Play store for free apps. (2) Then, it performs for each app a static analysis.
Specifically, it applies permission extraction, call graph creation, and call ID analysis, the latter of which is based on Android
system and third party APIs. (3) Finally, results for the collection and sharing practices are generated and stored.
n = 9,050 policies. We obtained this set by adjusting our IV. MOBILEAPPANALYSIS
full policy set (n = 9,295) to account for the fact that
In order to compare our policy analysis results to what
not every policy link might actually lead to a policy: for 40
apps actually appear to do we now discuss our app analysis
randomly selected apps from our full policy set we checked
approach. We begin with our system design (§ IV-A) and
whether the policy link in fact lead to a policy, which was
follow up with the system’s analysis results (§ IV-B).
the case for 97.5% (39/40) of links (with a CI of 0.87 to 1
at the 95% level). As the other 2.5%, that is, one link, lead
to some other page and would not contain any data practice A. App Analysis System Design
descriptions, we randomly excluded from the full policy set
Our app analysis system is based on Androguard [20],
2.5% = 245 of policies without any data practice descriptions
an open source static analysis tool written in Python that
leaving us with n = 9,295 − 245 = 9,050 policies in the
provides extensible analytical functionality. Apart from the
full app/policy set. We emphasize that this technique does not
manual intervention in the construction and testing phase, our
allow us to determine whether the 245 documents actually
system’s analysis is fully automated. Figure 5 shows a sketch
did not contain a policy or had a policy that did not describe
of our system architecture. A brief example for sharing of
any practices. However, in any case the adjustment increases
device IDs will convey the basic program flow of our data-
the occurrence of positive data practice instances in the full
driven static analysis.
app/policysetkeepingdiscrepanciesbetweenappsandpolicies
ataconservativelevelassomeappsforwhichtheanalysisdid For each app our system builds an API invocation map,
not find any data practice descriptions are now excluded.22 which is utilized as a partial call graph (call graph creation).
To illustrate, for sharing of device IDs all calls to the
It appears that many privacy policies fail to satisfy privacy
android.telephony.TelephonyManager.getDeviceId
requirements. Most notably, per Table II, only 46% describe
API are included in the call graph because the caller can
the notification process for policy changes, a mandatory
use it to request a device ID. All calls to this and other
requirement for apps under California and Delaware law.
APIs for requesting a device ID are added to the graph and
Similarly, only 36% of policies contain a statement on user
passed to the identification routine (call ID analysis), which
access, edit, and deletion rights, which COPPA requires for
checks the package names of the callers against the package
childrens’ apps, that is, apps intended for children or known
names of third party libraries to detect sharing of data. We
to be used by children. For the sharing practices we expected
focus on a set of ten popular libraries, which are listed in
morepoliciestoengageintheSID,SL,andSCpractices.The Table III.23 In order to make use of the getDeviceId API a
respective 10%, 12%, and 6% are rather small percentages
libraryneedstheREAD_PHONE_STATEpermission.Onlyifthe
for a presumably widely occurring practice, especially, given
analysis detects that the library has the required permission
our focus on policies of free apps that often rely on targeted
(permission extraction), the app is classified as sharing device
advertising. IDs with third parties.24 We identified relevant Android API
Runtime Performance and Failure Rate. The analysis of calls for the types of information we are interested in and the
all practices for the policies in the full app/policy set required permission each call requires by using PScout [6].
abouthalfanhourintotalrunningtenthreadsinparallelonan
Our static analysis is informed by a manual evaluation
Amazon Web Services (AWS) EC2 instance m4.4xlarge with
of Android and third party APIs. Because sharing of data
2.4 GHz Intel Xeon E5-2676 v3 (Haswell), 16 vCPU, and 64
most often occurs through third party libraries [23], we can
GiB memory [2]. The feature extraction took up the majority
leverage the insight that the observation of data sharing for
of time and the training and classification finished in about
a given library allows extension of that result to all apps
one minute. There was no failure in extracting policy features
or analyzing policies. 23Thelimitationonasmallsetoflibrariesallowsustomanuallyanalyzethelibrary
functionsfreeingusfromusingresource-intensivedataflowanalysistechniquesinour
22Wealsocheckedtherandomsampleof40appsforpoliciesdynamicallyloadedvia appanalysis.However,inprinciple,itispossibletoincludemorelibraries.
JavaScriptbecauseforsuchpoliciesthefeatureextractionwouldfail.Wehadobserved 24Android’s permission model as of Android 6.0 does not distinguish between
such dynamic loading before. However, as neither of the policies in the sample was permissionsforanappandpermissionsfortheapp’slibraries,which,thus,canrequest
loadeddynamically,wedonotmakeanadjustmentinthisregard. allpermissionsoftheapp.
83rd Party Library Pract Base Acc app 95% CI Prec pos Rec pos F-1 pos F-1 neg Pos w/ pol Pos w/o pol
Crashlytics/Fabric (n=40) (n=40) (n=40) (n=40) (n=40) (n=40) (n=40) (n=9,295) (n=8,696)
Crittercism/Aptel. CID 0.8 0.9 0.76–0.97 0.89 1 0.94 0.67 95% 87%
Flurry Analytics CL 0.55 0.8 0.64–0.91 0.73 1 0.85 0.71 66% 49%
Google Analytics CC 0.78 1 0.91–1 1 1 1 1 25% 12%
Umeng SID 0.68 0.95 0.83–0.99 1 0.93 0.96 0.93 71% 62%
AdMob* SL 0.93 1 0.91–1 1 1 1 1 20% 16%
InMobi* SC 0.98 1 0.91–1 1 1 1 1 2% 0%
MoPub*
TABLE IV: App analysis results for the app test set (n=40) and the percentages of practices’
MillennialMedia*
occurrences in the full app set (n=17,991). More specifically, Pos w/ pol and Pos w/o pol are
StartApp*
showing what percentage of apps engage in a given practice for the subset of apps in the full app set
TABLE III: Ad* with a policy (n=9,295) and without a policy (n=8,696), respectively. We measure precision, recall,
and analytics li- and F-1 score for the positive and negative classes with the and subscripts designating the
pos neg
braries. respective scores.
using the same library [35]. As the top libraries have the of 0.96, show the overall reliability of our approach. For all
farthest reach [35] we focus on those. We used AppBrain [3] practices the accuracy is also above the baseline of always
to identify the ten most popular libraries by app count that selecting the test set class that occurs the most for a given
process device ID, location, or contact data. To the extent practice.Overall,asshowninTableIV,ourresultsdemonstrate
we were able to obtain them we also analyzed previous the general reliability of our analysis.
libraryversionsdatingbackto2011.Afterall,appssometimes
continue to use older library versions even after the library Data Practice Results for Full App Set. For all six data
has been updated. For each library we opened a developer practices we find a mean of 2.79 occurring practices per app
account, created a sample app, and observed the data flows for apps with policies and 2.27 occurrences for apps without
from the developer perspective. For these apps as well as policies. As all practices need to be described in a policy per
for a sample of Google Play store apps that implement the ourprivacyrequirements(§III-A),itisalreadyclearthatthere
selectedlibrariesweadditionallyobservedtheirbehaviorfrom are substantial amounts of potential inconsistencies between
the outside by capturing and decrypting packets via a man-in- appsandpoliciessimplyduetomissingpolicies.Forexample,
the-middleattackandafakecertificate[54].Wealsoanalyzed the SID practice was detected in 62% of apps that did not
librarydocumentations.Theseexercisesallowedustoevaluate have a policy (Table IV), which, consequently, appear to be
which data types were sent out to which third parties. potentially non-compliant with privacy requirements. Further-
more, for apps that had a policy only 10% disclosed the SID
practice(TableII)whileitoccurredin71%ofapps(TableIV).
B. App Analysis Results
Thus, 61% of those apps are potentially non-compliant in this
Performance Results for App Test Set. Before exploring the regard. The only practices for which we cannot immediately
analysisresultsforthefullappsetwediscusstheperformance infer the existence of potential inconsistencies are the CC
of our app analysis on a set of 40 apps (app test set), which and SC practices with policy disclosures of 56% and 6% and
we selected randomly from the publishers in the policy test occurrencesinappsof25%and2%,respectively.Wecanthink
set (to obtain corresponding app/policy test pairs for our of two reasons for this finding.
later performance analysis of potential privacy requirement
inconsistencies in § V-A). To check whether the data practices First, there could be a higher sensitivity among app
in the test apps were correctly analyzed by our system we publishers to notify users of practices related to con-
dynamically observed and decrypted the data flows from the tact data compared to practices that involve device iden-
test apps to first and third parties, performed a manual static tifiers and location data. Publishers may categorize con-
analysis for each test app with Androguard [20], and studied tact data more often as PII. Second, different from de-
the documentations of thirdparty libraries.Thus,for example, vice ID and location data, contact information is often pro-
we were able to infer from the proper implementation of a vided by the user through the app interface bypassing the
given library that data is shared as explained in the library’s APIs that we consider for our static analysis (most notably,
documentation. We did not measure performance based on the android.accounts.AccountManager.getAccounts
micro-benchmarks, such as DroidBench [4], as those do not API). Thus, our result demonstrates that the analysis approach
fully cover the data practices we are investigating. has to be custom-tailored to each data type and that the
user interface should receive heightened attention in future
Inthecontextofpotentialinconsistencies(§V-A)correctly works [62]. It also illustrates that our results only represent a
identifying positive instances of apps’ collection and sharing lower bound, particularly, for the sharing practices (SID, SL,
practices is more relevant than identifying negative instances SC),whicharelimitedtodatasenttothetenpublishersofthe
because only practices that are occurring in an app need to be libraries in Table III.
coveredinapolicy.Thus,theresultsforthedatapracticeswith
rarely occurring positive test cases are especially noteworthy: Limitations. There are various limitations of our static anal-
CC, SL, and SC all reached F-1 = 1 indicating that our ysis. At the outset our approach is generally subject to the
pos
staticanalysisisabletoidentifypositivepracticesevenifthey samelimitationsthatallstaticanalysistechniquesforAndroid
rarely occur. Further, the F-1 scores, averaging to a mean are facing, most notably, the difficulties of analyzing native
pos
9Acc Acc · Acc 95% CI Prec Rec F-1 F-1 MCC TP, FP, TN, FN Inconsistent
Practice pol app pos pos pos neg
(n=40) (n=40) (n=40) (n=40) (n=40) (n=40) (n=40) (n=40) (n=40) (n=9,050)
CID 0.95 0.74 0.83–0.99 0.75 1 0.86 0.97 0.84 6, 2, 32, 0 50%
CL 0.83 0.7 0.67–0.93 0.54 1 0.7 0.88 0.65 8, 7, 25, 0 41%
CC 1 0.88 0.91–1 - - - 1 - 0, 0, 40, 0 9%
SID 0.85 0.84 0.7–0.94 0.93 0.74 0.82 0.87 0.71 14, 1, 20, 5 63%
SL 1 0.93 0.91–1 1 1 1 1 1 3, 0, 37, 0 17%
SC 1 0.78 0.91–1 1 1 1 1 1 1, 0, 39, 0 2%
TABLE V: Results for identifying potential privacy requirement inconsistencies in the app/policy test set (n=40) and the
percentage of such potential inconsistencies for all 9,050 app/policy pairs (Inconsistent). Assuming independence of policy
and app accuracies, Acc · Acc , that is, the product of policy analysis accuracy (Table II) and app analysis accuracy
pol app
(Table IV), indicates worse results than the directly measured accuracy. The Matthews correlation coefficient (MCC), which is
insightful for evaluating classifiers in skewed classes, indicates a positive correlation between observed and predicted classes.
code, obfuscated code, and indirect techniques (e.g., reflec- m4.10xlarge with 2.4 GHz Intel Xeon E5-2676 v3 (Haswell),
tion).However,therearevariousconsiderationsthatameliorate 40 vCPU, and 160 GiB memory [2] the analysis of all 17,991
exposure of our approach to these challenges. First, if an APKs took about 31 hours. The mean runtime is 6.2 seconds
app or a library uses native code, it cannot hide its access per APK analysis.
to Android System APIs [35]. In addition, the use of native
code in ad libraries is minimal [45]. Indeed, we have rarely
V. IDENTIFYINGPOTENTIALINCONSISTENCIES
encountered native code in our analysis. Similarly, the need to
interact with a variety of app developers effectively prohibits In this section we unite our policy (§ III) and app (§ IV)
the use of indirect techniques [9]. However, code obfuscation analyses.Weexploretowhichextentappsarepotentiallynon-
in fact presents an obstacle. Our static analysis failed in 0.4% compliant with privacy requirements (§ V-A) and show how
(64/18,055) of all cases due to obfuscation (i.e., an app’s appmetadatacanbeusedtozoominonsetsofappsthathave
Dex file completely in bytecode). However, our failure rate a higher likelihood of non-compliance (§ V-B).
improves over the closest comparable rate of 21% [62].
A. Potential Inconsistencies in Individual App/Policy Pairs
Itisafurtherlimitationofourapproachthattheidentifica-
tionofdatapracticesoccursfromtheoutside(e.g.,server-side Potential Inconsistencies from a Developer’s Perspective.
code is not considered). While this limitation is not a problem As the results of a survey among app developers suggest a
for companies’ analysis of their own apps, which we see as lack of understanding privacy-best practices [7], it could be
a major application of our approach, it can become prevalent that many of the potential inconsistencies we encountered
for regulators, for instance. In many cases decrypting HTTPS are a consequence of this phenomenon as well. Especially,
trafficviaaman-in-the-middleattackandafakecertificatewill many developers struggle to understand what type of data
shed some light. However, it appears that some publishers are third parties receive, and with limited time and resources even
applying encryption inside their app or library. In those cases, self-described privacy advocates and security experts grapple
the analysis will need to rely on inferring the data practice in withimplementingprivacyandsecurityprotections[7].Inthis
question indirectly. For example, it remains possible to check regard, our analysis approach can provide developers with a
whetheralibraryisproperlyimplementedinanappaccording valuable indicator for instances of potential non-compliance.
to the library’s documentation, which lends evidence to the For identifying potential inconsistencies positive app classes
inference that the app makes use of the documented practices. and negative policy classes are relevant. In other words, if a
data practice does not occur in an app, it does not need policy
Also, our results for the sharing practices only refer to coverage because there can be no inconsistency to begin with.
the ten third parties listed in Table III. The percentages for Similarly,ifauserisnotifiedaboutadatapracticeinapolicy,
sharing of contacts, device IDs, or locations would almost it is irrelevant whether the practice is implemented in the app
certainly be higher if we would consider additional libraries. or not. Either way, the app is covered by the policy. Based on
In addition, our definition of sharing data with a third party these insights we analyze the performance of our approach.
onlyencompassessharingdatawithadnetworksandanalytics
libraries. However, as it was shown that ad libraries are the PerformanceResultsforApp/PolicyTestSet.Toevaluatethe
recipientsofdatain65%ofallcases[35],webelievethatthis performance of our system for correctly identifying potential
definition covers a substantial portion of sharing practices. It privacy requirement inconsistencies we use a test set with
should be finally noted that our investigation does not include corresponding app/policy pairs (app/policy test set). The set
collection or sharing of data that occurs through user input, contains the 40 random apps from our app test set (§ IV-B)
offline, or at app servers’ backends. However, as our analysis andtheirassociatedpoliciesfromourpolicytestset(§III-C3).
already identifies a substantial percentage of potentially non- We associate an app and a policy if the app or its Play store
compliant apps, we think that there is value in our techniques page links to the policy or if the policy explicitly declares
even with these limitations. itselfapplicabletomobileapps.Asonly23policiessatisfythis
requirement some policies are associated with multiple apps.
Runtime Performance. In terms of runtime performance, As shown in Table V, accuracy results range between 0.83
using ten threads in parallel on an AWS EC2 instance and 1 with a mean of 0.94. Although not fully comparable,
100.25
0.20
0.15
0.10
0.05
0.00
0 1 2 3 4 5 6
Number of Potential Privacy Requirement Inconsistencies
ytisneD
Variable Pos p value OR 95% CI
No. of Apps No. User Ratings 100% 0.0001 0.9 0.9999998–0.9
Overall Score 100% <0.0001 1.4 1.24–1.57
2K Badge 21% <0.0001 0.57 0.49–0.65
1.5K In-app Purchases 27% 0.08 1.15 0.99–1.34
1K Interactive Elm 45% <0.0001 1.33 1.17–1.53
500 Content Unrated 5% 0.002 0.68 0.53–0.87
0
TABLEVI:Significantvariablesforpredictingapps’potential
non-compliancewithatleastoneprivacyrequirementaseval-
uatedonourfullapp/policyset(n=9,050).TopDeveloperand
Fig. 6: For the full app/policy set (n = 9,050) we found that Editor’s Choice badges are assigned by Google. Interactive
2,455 apps have one potential inconsistency, 2,460 have two, elements and unrated content refer to the respective ESRB
and only 1,461 adhere completely to their policy. Each app classifications[24].Posdesignatesthepercentagesofpositive
exhibits a mean of 1.83 (16,536/9,050) potential inconsisten- cases (e.g., 100% apps have an overall score), OR is the odds
cies (with the following means per data practice: CID: 0.5, ratio, and the 95% CI is the profile likelihood CI.
CL: 0.41, CC: 0.09, SID: 0.63, SL: 0.17, SC: 0.02).
The average number of 1.83 potential inconsistencies per
AsDroid achieved an accuracy of 0.79 for detecting stealthy
app is high compared to the closest previous averages with
behavior [41] and Slavin et al. [62] report an accuracy of
0.62 (113/182) cases of stealthy behavior [41] and potential
0.8 for detecting discrepancies between app behavior and
privacyviolationsof1.2(24/20)[22]and0.71(341/477)[62].
policy descriptions. For the 240 classification instances in the
Figure 6 shows details of our results. In this regard, it should
app/policy test set—that is, classifying six practices for each
be noted that for apps without a policy essentially every data
of the 40 app/policy pairs—our system correctly identified
collection or sharing practice causes a potential inconsistency.
32 potential inconsistencies (TP). It also returned five false
For example, all 62% apps without a policy that share device
negatives (FN), 10 false positives (FP), and 193 true negatives
(TN).25 IDs(TableIV)arepotentiallynon-compliant.Thus,overallour
resultssuggestabroadlevelofpotentialinconsistencybetween
The F-1 pos scores for our analysis, ranging from 0.7 apps and policies.26
to 1, indicate the overall reliable identification of potential
inconsistencies.Whilewethinkthattheseresultsaregenerally
B. Potential Inconsistencies for Groups of App/Policy Pairs
promising, we obtain a relatively low precision value of
Prec =0.54 for the CL practice and for the CID practice Analyzing individual apps for potential non-compliance at
pos
we had hoped for a higher precision than Prec = 0.75 as scale is a resource-intensive task. Thus, it is worthwhile to
pos
well. These results illustrate a broader point that is applicable first estimate an app population’s potential non-compliance
beyond those practices. False positives seem to occur because as a whole before performing individual app analyses. Our
our analysis takes into account too many APIs that are only suggestion is to systematically explore app metadata for cor-
occasionallyusedforpurposesofthedatapracticeinquestion. relations with potential inconsistencies based on statistical
Despite our belief that it is better to err on the side of false models. This broad macro analysis supplements the individual
positives, which is especially true for an auditing system [35], app analysis and reveals areas of concern on which, for
in hindsight we probably would have left out some APIs. example, privacy activists can focus on. To illustrate this idea
The opposite problem seems to occur in the SID practice. We we evaluate a binary logistic regression model that determines
includedtoofewrelevantAPIs.Inthisregard,weacknowledge thedependenceofwhetheranapphasapotentialinconsistency
thechallengeofidentifyingasetofAPIsthatcapturesthebulk (the dependent variable) from six Play store app metadata
of cases for a practice without being over- or under-inclusive. variables (the independent variables). Our results, shown in
Table VI, demonstrate correlations at various statistical sig-
Potential Inconsistencies for Full App/Policy Set. As indi-
nificance levels with p values ranging from 0.0001 to 0.08.
catedbythehighpercentages showninTable V,weidentified
Particularly,withanincreaseinthenumberofuserratingsthe
potential inconsistencies on a widespread basis. Specifically,
probabilityofpotentialinconsistenciesdecreases.Thereisalso
collection of device IDs and locations as well as sharing of
a decreasing effect for apps with a badge and for apps whose
device IDs are practices that are potentially inconsistent for
content has not yet been rated.
50%, 41%, and 63% of apps, respectively. However, given the
relatively low precision and high recall for these practices, we Interestingly, apps with higher overall Google Play store
caution that their percentages might be an overestimation. It scores do not have lower odds for potential inconsistencies.
is further noteworthy that for sharing of location and contact In fact, the opposite is true. With an increase in the overall
information nearly every detection of the practices goes hand score the odds of a potential inconsistency become higher.
in hand with a potential inconsistency. For the apps that share An increase of the overall score by one unit, e.g., from 3.1
location information (20%, per Table IV) nearly all (17%, per to 4.1 (on a scale of 1 through 5), increases the odds by
Table V) do not properly disclose such sharing. Similarly, for a factor of 1.4. A reason could be that highly rated apps
the 2% of apps that share contact data only a handful provide provide functionality and personalization based on user data,
sufficient disclosure.
26Asweareevaluatingoursystemforuseinprivacyenforcementactivities(§VI)we
25AppendixBdescribesdetailsofcalculatingtrueandfalsepositivesandnegatives. decidedtoabstainfromcontactinganyaffectedapppublishersofourfindings.
110.85
0.80
0.75
0.70
0 250K 500K 750K 1M
Number of User Ratings
ytilibaborP
detciderP
Badge
No
Yes
Fig.7:Inourlogisticmodelthepredictedprobabilityofanapp
having a potential inconsistency is dependent on the number
ofuserratingsandassignmentofabadge.Theoverallscoreis
held at the mean and in-app purchases, interactive elements,
and unrated content are held to be not present. The shaded
areas identify the profile likelihood CIs at the 95% level.
whose processing is insufficiently described in their privacy
policies. Also, users do not seem to rate apps based on
privacy considerations. We found the word “privacy” in only
1% (220/17,991) of all app reviews. Beyond an app’s score,
the odds for a potential inconsistency also increase for apps
that feature in-app purchases or interactive elements. Also,
supplementing our model with category information reveals
that the odds significantly (p ≤ 0.05) surge for apps in the
Finance, Health & Fitness, Photography, and Travel & Local
categories while they decrease for apps in the Libraries &
Demo category.
Inordertoevaluatetheoverallmodelfitbasedonstatistical
significance we checked whether the model with independent
variables (omitting the category variables) had significantly
Fig. 8: Our system allows users to analyze apps for potential
better fit than a null model (that is, a model with the intercept
privacyrequirementnon-compliance.Anappcanbesubjectto
only). The result of a Chi square value of 151.03 with six
multiple privacy policies—for example, one policy from inside
degrees of freedom and value of p ≤0.001 indicates that our
theappandonefromtheapp’sPlayStorepage.Inthesecases
modelhasindeedsignificantlybetterfitthanthenullmodel.To
the app is checked against multiple policies.
see the impact of selected aspects of the model it is useful to
illustrate the predicted probabilities. An example is contained
in Figure 7. Apps with a Top Developer or Editor’s Choice
badge have a nearly 10% lower probability of a potential
scalability reasons we chose to leverage AWS EC2 t2.large
inconsistency. That probability further decreases with more
instances with up to 3.0 GHz Intel Xeon, 2 vCPU, and 8 GiB
user ratings for both apps with and without badge.
memory [2].
VI. CASESTUDY:EVALUATINGOURSYSTEMFORUSE The system’s graphical user interface applies the Flask
BYTHECALAG Python web framework [58] running on an Apache web
server [64] with Web Server Gateway Interface module [21].
WeworkedwiththeCalAG,toevaluateoursystem’scapa-
All analysis requests are added to a Celery task queue [5],
bilities for supplementing the enforcement of CalOPPA [12].
which communicates with the Flask application using the
To that end, we implemented a custom version of our system
RabbitMQ message broker [52]. Once an analysis is finished
(§ VI-A) and added various new analysis features (§ VI-B).
the results are loaded by the Flask application and displayed
The feedback we received is encouraging and confirms that
in the users’ browsers.
oursystemcouldenableregulatorstoachievemoresystematic
enforcement of privacy requirements (§ VI-C).27
Similarasinouroriginalsystem,allAPKsaredownloaded
via Raccoon [50] and apps’ privacy policy links are retrieved
A. System Implementation
from their respective Play store pages. However, in order to
As shown in Figure 8, we implemented our system for the download the websites that the links are leading to we auto-
CalAGasawebapplication.Itallowsuserstorequestanalyses mated a Firefox browser with Selenium [60] and PyVirtualD-
for individual apps and also supports batch processing. For isplay [53]. Using a real browser instead of simply crawling
the HTML of the privacy policy pages is advantageous as it
27WecontinueourworkwiththeCalAGbeyondthisstudyandexpectfurtherresults. canobtainpoliciesthatareloadeddynamicallyviaJavaScript.
12After the website with the privacy policy is downloaded Pract Acc Prec pos Rec pos F-1 pos Inconsistent
(n=20) (n=20) (n=20) (n=20) (n=20)
any elements that are not part of the policy, such as page
CID 0.85 0.5 1 0.67 15%
navigation elements, are removed. The system then runs our
CL 0.75 0.38 1 0.55 15%
feature extraction routines (§ III-C2) as well as ML classifiers
CC 0.95 1 0.75 0.86 15%
(§ III-C3) on the policy and the static analysis (§ IV) on the
SID 0.95 0.94 1 0.97 75%
downloadedAPK.Finally,theresultsaredisplayedtotheuser
SL 0.9 0.75 1 0.86 30%
with flags raised for potential inconsistencies.
SC 1 - - - 0%
TABLE VII: Classification results for the Cal AG app/policy
B. Additional Functionality
set (n=20).
We placed high emphasis on usability from both a legal
andhuman-computerinteractionperspective.Notably,insome
our system does not find potential inconsistencies. Instead,
cases the Cal AG users were interested in receiving additional
they can concentrate on examining apps for which flags were
information. For instance, one additional piece of information
raised.Inaddition,theCalAGusersexpressedthatoursystem
was the breakdown of third parties in the sharing practices.
wasusefulforestimatingthecurrentoverallstateofCalOPPA
The initial version of our system’s report simply showed
compliance. For example, the analysis results alerted them of
that the user’s contact and device ID were shared, however,
some app policies that use vague language in the descriptions
without disclosing that those were shared with, say, InMobi
of their collection and sharing practices.
and Crashlytics. Distinguishing which type of information is
shared with which third party is important under CalOPPA We evaluated our system implementation for the Cal AG
because the sharing of contact information makes a stronger on a random sample of 20 popular Play store apps and their
case than the sharing of device IDs, for example.28 associated policies (Cal AG app/policy set). We asked the Cal
AG users to give us their interpretations of the policies and
Given its importance we implemented additional contact
used these to evaluate the results of our system. As shown
information functionality. Because we believe that the rela-
in Table VII, it appears that the Facebook Login functionality
tively low detection rate for the collection and sharing of
is able to identify the contact information collection practice
contact information (§ V-A) is due to the fact that such
fairly reliably. Obviously, the results are limited to a small
information is often manually entered by the user or obtained
number of 20 app/policy pairs. Further, our system achieves
via other sources, we enhanced our system in this regard.
highrecalloverall.Itperformswellonidentifyingtheabsence
Particularly, we leveraged the Facebook Login library that
of potential inconsistencies. At the same time, similar to
gives apps access to a user’s name and Facebook ID [25].
our results in § V-A, we find a non-negligible number of
ThesystemdetectstheusageoftheFacebookLoginlibraryin
false positives. Particularly, we observe a precision of 0.5
anappbyextractingtheapp’smanifestandresourcefileswith
for collection of device IDs and 0.38 for locations. However,
Apktool[65]andthensearchingforsignaturesrequiredforthe
despite these low precision values, the high recall values
FacebookLogintoworkproperly.Theseincludeanactivityor
suggest that our system is unlikely to miss many potential
intent filter dedicated to the Login interface, a Login button in
inconsistencies. Rather, upon closer manual inspections some
the layout, and the invocation of an initialization, destruction,
of those will prove to be false alarms.
or configuration routine.
Another feature that we added is the download of privacy VII. FUTUREDIRECTIONS
policies linked to from within apps. Our initial policy crawler
The law of notice and choice is intended to enable en-
was limited to downloading policies via an app’s Play store
forcement of data practices in mobile apps and other online
page. As the Cal AG provided guidance to app publishers
services. However, verifying whether an app actually behaves
for linking to the policy from both the Play store and from
according to the law and its privacy policy is decisively hard.
withinanapp[11],ournewapproachisintendedtocoverboth
To alleviate this problem we propose the use of an automated
possibilities.Thesystemfindsthelinksinanappbyextracting
analysis system based on machine learning and static analysis
strings from the APK file using Apktool and then extracting
to identify potential privacy requirement inconsistencies. Our
URLsfromwithinthesestringsthatcontainrelevantkeywords,
system advances app privacy in three main areas: it increases
suchas“privacy.”Ifapolicylinkinsideanappdiffersfromthe
transparencyforotherwiselargelyopaquedatapractices,offers
app’sPlaystorepolicylinkoriftherearemultiplepolicylinks
the scalability necessary for potentially making an impact on
identified within an app, our system downloads and analyzes
theappeco-systemasawhole,andprovidesafirststeptowards
all documents retrieved from these links.
automating mobile app privacy compliance analysis.
C. System Performance Our results suggest the occurrence of potential privacy
requirement inconsistencies on a large scale. However, the
TheCalAGusersreportedthatoursystemhasthepotential possibilities of the techniques introduced here have yet to be
tosignificantlyincreasetheirproductivity.Particularly,asthey fully explored. For example, the privacy policy analysis can
have limited resources it can give them guidance on certain be further developed to capture nuances in policy wording—
areas, e.g., categories of apps, to focus on. They can also put possibly by leveraging the structure of policies (e.g., by
less effort and time into analyzing practices in apps for which identifying definitions of PII and where those are referenced
in a policy). Similarly, the accuracy of the app analysis could
28CompareCal.Bus.&Prof.Code§22577(a)(3),accordingtowhichane-mailaddress
be enhanced by integrating data flow analysis techniques.
by itself qualifies as PII, and §22577(a)(7), which covers information types that only
qualifyasPIIincombinationwithotheridentifiers. However, for performance reasons resources should be used
13sparingly. A practical system for the purpose of large-scale [5] AskSolem&Contributors,“Celery-distributedtaskqueue,”http://docs.celerypr
app analysis necessarily remains relatively lightweight. oject.org/en/latest/,accessed:Dec19,2016.
[6] K.W.Y.Au,Y.F.Zhou,Z.Huang,andD.Lie,“PScout:Analyzingtheandroid
The findings in this study raise the question of extending permissionspecification,”inCCS’12. ACM.
our approach to other areas. We believe, the principles could [7] R.Balebako,A.Marsh,J.Lin,J.Hong,andL.F.Cranor,“Theprivacyandsecurity
be used in analyzing website practices, e.g., by leveraging behaviorsofsmartphoneappdevelopers,”inUSEC’14.
the work of Sen et al. [61]. First and third party cookies [8] R. Bhoraskar, S. Han, J. Jeon, T. Azim, S. Chen, J. Jung, S. Nath, R. Wang,
and D. Wetherall, “Brahmastra: Driving apps to test the security of third-party
and other tracking mechanisms could be observed to evaluate
components,”inUSENIXSecurity’14. USENIXAssoc.
collection and sharing of data. Implementing our approach
[9] T.BookandD.S.Wallach,“Acaseofcollusion:Astudyoftheinterfacebetween
in other mobile platforms, particularly, iOS, is likely more adlibrariesandtheirapps,”inSPSM’13. ACM.
challenging. Notably, the difficulty of decompiling iOS apps [10] California Department of Justice, “Attorney General Kamala D. Harris se-
might necessitate a dynamic app analysis approach [19], [43]. cures global agreement to strengthen privacy protections for users of mo-
bile applications,” http://www.oag.ca.gov/news/press-releases/attorney-general-ka
The web interface of Apple’s App Store also does not seem
mala-d-harris-secures-global-agreement-strengthen-privacy,Feb.2012,accessed:
to provide app pages with a standardized privacy policy link Dec19,2016.
field.However,thesechallengeswouldnotneedtoberesolved [11] ——, “Making your privacy practices public,” https://oag.ca.gov/sites/all/files/
for the integration of a privacy requirement analysis into iOS agweb/pdfs/cybersecurity/making your privacy practices public.pdf, May 2014,
accessed:Dec19,2016.
software development tools.
[12] ——, “Attorney General Kamala D. Harris launches new tool to help
We think that it is necessary to develop the proposed consumers report violations of California Online Privacy Protection Act
(CalOPPA),” https://oag.ca.gov/news/press-releases/attorney-general-kamala-d-ha
privacyrequirementanalysisintandemwithpublicpolicyand rris-launches-new-tool-help-consumers-report,Oct.2016,accessed:Dec19,2016.
law.Regulatorsarecurrentlypushingforappstorestandardiza- [13] P.ChundiandP.M.Subramaniam,“AnApproachtoAnalyzeWebPrivacyPolicy
tion [10] and early enforcement of potentially non-compliant Documents,”inKDDWorkshoponDataMiningforSocialGood,2014.
privacy practices [31]. Approaches like the one proposed here [14] E. Costante, J. den Hartog, and M. Petkovic, “What websites know about you:
Privacypolicyanalysisusinginformationextraction,”inDataPrivacyManagement
canrelieveregulators’workloadsthroughautomationallowing
’13. Springer.
them to focus their limited resources to move from a purely
[15] E.Costante,Y.Sun,M.Petkovic´,andJ.denHartog,“Amachinelearningsolution
reactionary approach towards systematic oversight. As we toassessprivacypolicycompleteness,”inWPES’12. ACM.
also think that many software publishers do not intend non- [16] L.F.Cranor,K.Idouchi,P.G.Leon,M.Sleeper,andB.Ur,“Aretheyactually
compliance with privacy requirements, but rather lose track of anydifferent?comparingthousandsoffinancialinstitutions’privacypractices,”in
WEIS’13.
theirobligationsorareunawareofthem,wealsoseepotential
[17] L. F. Cranor, M. Langheinrich, M. Marchiori, M. Presler-Marshall, and J. M.
for implementing privacy requirement analyses in software
Reagle,“ThePlatformforPrivacyPreferences1.0(P3P1.0)specification,”World
development tools and integrating them into the app vetting WideWebConsortium,RecommendationREC-P3P-20020416,April2002.
process in app stores. [18] S. Demetriou, W. Merrill, W. Yang, A. Zhang, and C. Gunter, “Free for all!
assessing user data exposure to advertising libraries on android,” in NDSS ’16.
ISOC.
ACKNOWLEDGMENT
[19] Z.Deng,B.Saltaformaggio,X.Zhang,andD.Xu,“iRiS:Vettingprivateapiabuse
We would like to thank the anonymous reviewers for their iniOSapplications,”inCCS’15. ACM.
commentsonthedraftofthisstudy.Wearealsogratefulforthe [20] A. Desnos, “Androguard,” http://doc.androguard.re/html/index.html, accessed:
Dec19,2016.
insights provided by our Cal AG collaborators Justin Erlich,
[21] G.Dumpleton,“Modwsgi,”https://modwsgi.readthedocs.io/en/develop/,accessed:
Cassidy Kim, Stacey Schesser, TiTi Nguyen, Joanne McN- Dec19,2016.
abb, Sarah Dalton, Jeremy AronDine, and Sundeep Pattem. [22] W.Enck,P.Gilbert,B.-G.Chun,L.P.Cox,J.Jung,P.McDaniel,andA.N.Sheth,
We further thank our academic collaborators Aswarth Dara, “TaintDroid:Aninformation-flowtrackingsystemforrealtimeprivacymonitoring
onsmartphones,”inOSDI’10. USENIXAssoc.
Peter Story, Mads Schaarup Andersen, Amit Levy, Vaggelis
[23] W. Enck, D. Octeau, P. McDaniel, and S. Chaudhuri, “A study of android
Atlidakis, and Jie SB Li. This study was supported in part
applicationsecurity,”inUSENIXSecurity’11. USENIXAssoc.
by the NSF under grants CNS-1330596, CNS-1330214, and
[24] ESRB,“ESRBratingsguide,”http://www.esrb.org/ratings/ratings guide.aspx,ac-
SBE-1513957, as well as by DARPA and the Air Force cessed:Dec19,2016.
Research Laboratory, under agreement number FA8750-15- [25] Facebook,“Permissionsreference-Facebooklogin,”https://developers.facebook.
2-0277. The US Government is authorized to reproduce and com/docs/facebook-login/permissions,accessed:Dec19,2016.
distributereprintsforGovernmentalpurposesnotwithstanding [26] A. P. Felt, E. Chin, S. Hanna, D. Song, and D. Wagner, “Android permissions
demystified,”inCCS’11. ACM.
any copyright notation thereon. The views and conclusions
[27] FTC,“Privacyonline:Areporttocongress,”https://www.ftc.gov/reports/privacy-
contained herein are those of the authors and should not be
online-report-congress,Jun.1998,accessed:Dec19,2016.
interpreted as necessarily representing the official policies or
[28] ——, “Mobile apps for kids: Current privacy disclosures are disappointing,” h
endorsements,eitherexpressedorimplied,ofDARPA,theAir ttp://www.ftc.gov/os/2012/02/120216mobile apps kids.pdf, Feb. 2012, accessed:
Force Research Laboratory, the NSF, or the US Government. Dec19,2016.
[29] ——,“Mobileappsforkids:Disclosuresstillnotmakingthegrade,”https://ww
w.ftc.gov/reports/mobile-apps-kids-disclosures-still-not-making-grade,Dec.2012,
REFERENCES accessed:Dec19,2016.
[1] V. Afonso, A. Bianchi, Y. Fratantonio, A. Doupe, M. Polino, P. de Geus, [30] ——,“Mobileprivacydisclosures,”www.ftc.gov/os/2013/02/130201mobileprivac
C.Kruegel,andG.Vigna,“Goingnative:Usingalarge-scaleanalysisofandroid yreport.pdf,Feb.2013,accessed:Dec19,2016.
appstocreateapracticalnative-codesandboxingpolicy,”inNDSS’16. ISOC. [31] ——,“FTCwarnschildren’sappmakerBabyBusaboutpotentialCOPPAviola-
[2] Amazon,“AmazonEC2instancetypes,”https://aws.amazon.com/ec2/instance-typ tions,”https://www.ftc.gov/news-events/press-releases/2014/12/ftc-warns-children
es/,accessed:Dec19,2016. s-app-maker-babybus-about-potential-coppa,Dec.2014,accessed:Dec19,2016.
[3] AppBrain, “Android library statistics,” http://www.appbrain.com/stats/libraries/, [32] ——, “What’s the deal? a federal trade commission study on mobile shop-
accessed:Dec19,2016. pingapps,”https://www.ftc.gov/reports/whats-deal-federal-trade-commission-stud
y-mobile-shopping-apps-august-2014,Aug.2014,accessed:Dec19,2016.
[4] S. Arzt, S. Rasthofer, C. Fritz, E. Bodden, A. Bartel, J. Klein, Y. Le Traon,
D. Octeau, and P. McDaniel, “FlowDroid: Precise context, flow, field, object- [33] ——,“Kids’appsdisclosuresrevisited,”https://www.ftc.gov/news-events/blogs/b
sensitiveandlifecycle-awaretaintanalysisforandroidapps,”inPLDI’14. ACM. usiness-blog/2015/09/kids-apps-disclosures-revisited,Sep.2015.
14[34] K.Ghazinour,M.Majedi,andK.Barker,“Amodelforprivacypolicyvisualiza- [65] C.TumblesonandR.Wis´niewski,“Apktool,”https://ibotpeaches.github.io/Apktoo
tion,”inCOMPSAC’09. IEEE. l/,accessed:Dec19,2016.
[35] C.Gibler,J.Crussell,J.Erickson,andH.Chen,“AndroidLeaks: Automatically [66] T.Watanabe,M.Akiyama,T.Sakai,andT.Mori,“Understandingtheinconsisten-
detectingpotentialprivacyleaksinandroidapplicationsonalargescale,”inTRUST ciesbetweentextdescriptionsandtheuseofprivacy-sensitiveresourcesofmobile
’12. Springer. apps,”inSOUPS’15. USENIXAssoc.
[36] M. I. Gordon, D. Kim, J. Perkins, L. Gilham, N. Nguyen, and M. Rinard, [67] S. Wilson, F. Schaub, A. A. Dara, F. Liu, S. Cherivirala, P. G. Leon, M. S.
“Information-flowanalysisofAndroidapplicationsinDroidSafe,”inNDSS’15. Andersen,S.Zimmeck,K.M.Sathyendra,N.C.Russell,T.B.Norton,E.Hovy,
ISOC. J.Reidenberg,andN.Sadeh,“Thecreationandanalysisofawebsiteprivacypolicy
[37] A.Gorla,I.Tavecchia,F.Gross,andA.Zeller,“Checkingappbehavioragainst corpus,”inACL’16. ACL.
appdescriptions,”inICSE’14. ACM. [68] S.Wilson,F.Schaub,R.Ramanath,N.Sadeh,F.Liu,N.A.Smith,andF.Liu,
“Crowdsourcingannotationsforwebsites’privacypolicies:Canitreallywork?”
[38] GPEN,“2014annualreport,”https://www.privacyenforcement.net/node/513,Mar.
inWWW’16.
2015,accessed:Dec19,2016.
[69] L.Yu,X.Luo,X.Liu,andT.Zhang,“Canwetrusttheprivacypoliciesofandroid
[39] M.C.Grace,W.Zhou,X.Jiang,andA.-R.Sadeghi,“Unsafeexposureanalysis
apps?”inDSN’16.
ofmobilein-appadvertisements,”inWISEC’12. ACM.
[70] L.Yu,T.Zhang,X.Luo,andL.Xue,“AutoPPG:Towardsautomaticgeneration
[40] C.Hoke,L.Cranor,P.Leon,andA.Au,“AreTheyWorthReading?AnIn-Depth
ofprivacypolicyforandroidapplications,”inSPSM’15. ACM.
AnalysisofOnlineTrackersPrivacyPolicies,”I/S:AJournalofLawandPolicy
fortheInformationSociety,Apr.2015. [71] M. Zhang, Y. Duan, Q. Feng, and H. Yin, “Towards automatic generation of
security-centricdescriptionsforandroidapps,”inCCS’15. ACM.
[41] J. Huang, X. Zhang, L. Tan, P. Wang, and B. Liang, “AsDroid: Detecting
stealthybehaviorsinandroidapplicationsbyuserinterfaceandprogrambehavior [72] S. Zimmeck and S. M. Bellovin, “Privee: An architecture for automatically
contradiction,”inICSE’14. ACM. analyzingwebprivacypolicies,”inUSENIXSecurity’14. USENIXAssoc.
[42] D.Kong,L.Cen,andH.Jin,“AUTOREB:Automaticallyunderstandingthereview-
to-behaviorfidelityinandroidapplications,”inCCS’15. ACM. APPENDIXA
[43] A. Kurtz, A. Weinlein, C. Settgast, and F. Freiling, “DiOS: Dynamic privacy POLICYANDAPPDATASETS
analysisofiosapplications,”Friedrich-Alexander-Universita¨tErlangen-Nu¨rnberg,
Dept.ofComputerScience,Tech.Rep.CS-2014-03,Jun.2014.
1) Full App Set — Total Apps Collected from the Google
[44] J.Lin,B.Liu,N.Sadeh,andJ.I.Hong,“Modelingusers’mobileappprivacy
Play Store (n=17,991)
preferences:Restoringusabilityinaseaofpermissionsettings,”inSOUPS’14.
USENIXAssoc. 2) Full Policy Set — Total Policies Collected for Apps in
[45] B.Liu,B.Liu,H.Jin,andR.Govindan,“Efficientprivilegede-escalationforad the Full App Set via Google Play Store Links (n=9,295)
librariesinmobileapps,”inMobiSys’15. ACM. 3) Full App/Policy Set — Total App/Policy Pairs from the
[46] F.Liu,R.Ramanath,N.Sadeh,andN.A.Smith,“Asteptowardsusableprivacy Full App and Full Policy Sets adjusted for Links not
policy:Automaticalignmentofprivacystatements,”inCOLING’14.
actually leading to a Policy (n=9,050)
[47] C.D.Manning,P.Raghavan,andH.Schu¨tze,IntroductiontoInformationRetrieval.
CambridgeUniversityPress,2008. 4) Policy Test Set — Random Policies from the OPP-115
[48] A.K.Massey,J.Eisenstein,A.I.Anto´n,andP.P.Swire,“Automatedtextmining Corpus [67] (n=40)
forrequirementsanalysisofpolicydocuments,”inRE’13. 5) App Test Set — Random Apps Associated with the
[49] K. Olmstead and M. Atkinson, “Apps permissions in the Google Play Policies in the Policy Test Set (n=40)
store,” http://www.pewinternet.org/2015/11/10/apps-permissions-in-the-google-pl
6) App/Policy Test Set — App/Policy Pairs from the App
ay-store/,Nov.2015,accessed:Dec19,2016.
and Policy Test Sets (n=40)
[50] Onyxbits,“Raccoon-apkdownloader,”http://www.onyxbits.de/raccoon,accessed:
Dec19,2016. 7) Cal AG App/Policy Set — Random Popular Apps and
[51] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, Associated Policies from the Google Play Store (n=20)
M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D.Cournapeau,M.Brucher,M.Perrot,andE.Duchesnay,“Scikit-learn:Machine
learninginPython,”JournalofMachineLearningResearch,2011. APPENDIXB
[52] Pivotal Software, Inc, “Rabbitmq,” https://www.rabbitmq.com/, accessed: EVALUATINGPOTENTIALINCONSISTENCIES
Dec19,2016.
[53] ponty,“Pyvirtualdisplay,”https://pypi.python.org/pypi/PyVirtualDisplay,accessed: Statistic Policy and App Classification
Dec19,2016.
TP Policy=0 correct and App=1 correct
[54] Progress Software Corporation, “Fiddler,” http://www.telerik.com/fiddler, ac-
FP Policy=0 incorrect and App=1 correct, or
cessed:Dec19,2016.
Policy=0 correct and App=1 incorrect, or
[55] R. Ramanath, F. Liu, N. Sadeh, and N. A. Smith, “Unsupervised alignment of
Policy=0 incorrect and App=1 incorrect
privacypoliciesusinghiddenmarkovmodels,”inACL’14.
FN Policy=1 incorrect and App=0 incorrect, or
[56] J.R.Reidenberg,T.Breaux,L.F.Cranor,B.French,A.Grannis,J.T.Graves,
F.Liu,A.McDonald,T.B.Norton,R.Ramanath,N.C.Russell,N.Sadeh,and Policy=1 incorrect and App=1 correct, or
F.Schaub,“Disagreeableprivacypolicies:Mismatchesbetweenmeaningandusers’ Policy=0 correct and App=0 incorrect
understanding,”BerkeleyTechnologyLawJournal,vol.30,no.1,pp.39–88,2015. TN All remaining combinations
[57] D. Reidsma and J. Carletta, “Reliability measurement without limits,” Comput.
Linguist.,vol.34,no.3,pp.319–326,Sep.2008. TABLE VIII: Evaluating potential inconsistencies. For ex-
[58] A.Ronacher,“Flask,”http://flask.pocoo.org/,accessed:Dec19,2016. ample, a false positive can be the result of a policy being
[59] N. Sadeh, A. Acquisti, T. D. Breaux, L. F. Cranor, A. M. McDonald, J. R. incorrectly classified as not covering a practice (Policy=0)
Reidenberg,N.A.Smith,F.Liu,N.C.Russell,F.Schaub,andS.Wilson,“The
usable privacy policy project,” Carnegie Mellon University, Tech. report CMU- while the practice actually occurs in the corresponding app
ISR-13-119,2013. (App=1), which was correctly identified.
[60] Selenium project, “Selenium,” http://www.seleniumhq.org/, accessed:
Dec19,2016.
[61] S.Sen,S.Guha,A.Datta,S.K.Rajamani,J.Tsai,andJ.M.Wing,“Bootstrapping
privacycomplianceinbigdatasystems,”inSP’14.
[62] R.Slavin,X.Wang,M.Hosseini,W.Hester,R.Krishnan,J.Bhatia,T.Breaux,
andJ.Niu,“Towardaframeworkfordetectingprivacypolicyviolationinandroid
applicationcode,”inICSE’16. ACM.
[63] J. W. Stamey and R. A. Rossi, “Automatically identifying relations in privacy
policies.”inSIGDOC’09. ACM.
[64] The Apache Software Foundation, “Apache,” http://httpd.apache.org/, accessed:
Dec19,2016.
15