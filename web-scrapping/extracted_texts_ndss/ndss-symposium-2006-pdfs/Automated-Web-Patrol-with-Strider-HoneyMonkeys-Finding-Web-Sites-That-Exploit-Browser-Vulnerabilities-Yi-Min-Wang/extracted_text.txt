Automated Web Patrol with Strider HoneyMonkeys:
Finding Web Sites That Exploit Browser Vulnerabilities
Yi-Min Wang, Doug Beck, Xuxian Jiang, Roussi Roussev,
Chad Verbowski, Shuo Chen, and Sam King
Microsoft Research, Redmond
Abstract allow web servers that host compromised URLs to install
malcode on visiting client machines without requiring any
Internet attacks that use malicious web sites to install user interaction beyond visitation. There have been several
malware programs by exploiting browser vulnerabilities manual analyses of these events
are a serious emerging threat. In response, we have [E04,F04,G05,IF05,R05,S05,T05]. Although these analyses
developed an automated web patrol system to automatically provide very useful and detailed information about which
identify and monitor these malicious sites. We describe the vulnerabilities are exploited and which malware programs
design and implementation of the Strider HoneyMonkey are installed, such efforts are not scalable, do not provide a
Exploit Detection System, which consists of a pipeline of comprehensive picture of the problem, and are generally
“monkey programs” running possibly vulnerable browsers ineffective at efficiently finding new malicious sites.
on virtual machines with different patch levels and
To address these issues, we developed a system that
patrolling the Web to seek out and classify web sites that
uses a pipeline of active, client-side, Virtual Machine (VM)-
exploit browser vulnerabilities.
based honeypots [H,HC], called Strider HoneyMonkeys, to
Within the first month of utilizing this system, we perform large-scale, systematic and automated web patrol.
identified 752 unique URLs hosted on 288 web sites that The HoneyMonkey system uses monkey programs1 that run
could successfully exploit unpatched Windows XP within virtual machines with OS’s of various patch levels to
machines. The system automatically constructed topology drive web browsers in an attempt to mimic human web
graphs based on traffic redirection to capture the browsing. Our approach adopts a state-management
relationship between the exploit sites. This allowed us to methodology to cybersecurity: instead of directly detecting
identify several major players who are responsible for a the acts of vulnerability exploits, the system uses the Strider
large number of exploit pages. By monitoring these 752 Tracer [W03] to catch unauthorized file creations and
exploit-URLs on a daily basis, we discovered a malicious configuration changes that are the result of a successful
web site that was performing zero-day exploits of the
exploit.
unpatched javaprxy.dll vulnerability and was operating
behind 25 exploit-URLs. It was confirmed as the first We demonstrate the effectiveness of our method by
“inthe-wild”, zero-day exploit of this vulnerability that was discovering a large community of malicious web sites that
reported to the Microsoft Security Response Center. host exploit pages and by deriving the redirection
Additionally, by scanning the most popular one million relationships among them. We describe a real-world
URLs as classified by a search engine, we found over seven experience with identifying a zero-day exploit2 using this
hundred exploit-URLs, many of which serve popular system. We show the existence of hundreds of malicious
content related to celebrities, song lyrics, wallpapers, video web pages amongst many popular web sites. Finally, we
game cheats, and wrestling. propose a comprehensive anti-exploit process based on this
monitoring system in order to improve Internet safety.
1. Introduction
This paper is organized as follows. Section 2 provides
background information on the problem space by describing
Internet attacks that use a malicious or hacked web site
the techniques used in actual client-side exploits of popular
to exploit unpatched client-side vulnerabilities of visiting
web browsers. Section 3 gives an overview of the Strider
browsers are on the rise. Malcode distributed by this method
HoneyMonkey Exploit Detection System and its
in the past 12 months includes the Download.Ject [D04],
surrounding Anti-Exploit Process. Section 4 evaluates the
Bofra [R04], and Xpire.info [B04] programs. These attacks
effectiveness of HoneyMonkey in both known-vulnerability
1 An automation-enabled program such as the Internet Explorer In this paper, a zero-day exploit
browser allows programmatic access to most of the operations refers to a vulnerability exploit that
that can be invoked by a user. A “monkey program” is a program exists before the patch for the
vulnerability is released. The
that drives the browser in a way that mimics a human user’s
vulnerability can be known or
operation. 2
unknown to the public at that time.and zero-day exploit detection, and presents an analysis of one of the following mechanisms classified into three
the exploit data to help prioritize investigation tasks. Section categories: (1) protocol redirection using HTTP 302
5 discusses the limitations of and possible attacks on the Temporary Redirect; (2) HTML tags including <iframe>,
current HoneyMonkey system and describes several <frame> inside <frameset>, and <META http-
countermeasures including an enhancement based on a equiv=refresh>; (3) script functions including
vulnerability-specific exploit detection mechanism. Section window.location.replace(), window.location.href(),
6 surveys related work and Section 7 concludes the paper. window.open(), window.showModalDialog(), and
<link_ID>.click(), etc. Since redirection is commonly used by
2. Browser-based Vulnerability Exploits
non-malicious sites to enrich content, simply eliminating
redirection from a browser would present significant
Malicious activities performed by actual web sites
complications
exploiting browser vulnerabilities can be divided into four
steps: code obfuscation, URL redirection, vulnerability 2.3. Vulnerability Exploitation
exploitation, and malware installation.
It is not uncommon to see a malicious web page
2.1. Code Obfuscation attempting to exploit multiple browser vulnerabilities in
order to maximize the chance of a successful attack. Figure
To complicate investigation and to escape
1 shows an example HTML fragment that uses various
signaturebased scanning by anti-virus/anti-spyware
primitives to load multiple files from different URLs on the
software, some web sites use a combination of the following
same server to exploit three vulnerabilities fixed in
code obfuscation techniques: (1) dynamic code injection
Microsoft Security Bulletins MS05-002 [M52], MS03-011
using the document.write() function inside a script; (2)
[M311], and MS04-013 [M413]. If any of the exploits
unreadable, long strings with encoded characters such as
<html><head><title></title></head><body> MS05 - 002
<style>
* {CURSOR: url("http://vxxxxxxe.biz/adverts/033/sploit.anr")}
</style>
<APPLET ARCHIVE='count.jar' CODE='BlackBox.class' WIDTH=1 HEIGHT=1>
<PARAM NAME='url' VALUE='http://vxxxxxxe.biz/adverts/033/win32.exe'></APPLET>
<script>
try{ MS03-011
document.write('<object
data=`&#109&#115&#45&#105&#116&#115&#58&#109&#104&#116&#109&#108&#58&#102&#105&#108&#101&#58;//
C:\fo'+'o.mht!'+'http://vxxxx'+'xxe.biz//adv'+'erts//033//targ.ch'+'m::/targ'+'et.htm` type=`text/x-scriptlet`></ob'+'ject>');
}catch(e){}
</script>
MS04-013
</body></html>
Figure 1. Actual sample Web page attempting to exploit multiple vulnerabilities
“%28”, “&#104”, etc. which are then decoded either by the succeeds, a Trojan downloader named win32.exe is
unescape() function inside a script or by the browser; (3) downloaded and executed. Note that although Internet
custom decoding routine included in a script; and (4) sub- Explorer is the common target due to its popularity, other
string replacement using the replace() function. Since code- browsers can also be attacked.
obfuscation is a common technique, this limits the ability of
2.4. Malware Installation
attack-signature-based detectors to detect new attacks that
leverage old exploit code. The purpose of an exploit is almost always to introduce
some piece of arbitrary code on the victim machine, as a
2.2. URL Redirection
way to achieve a larger attack goal. We have observed a
Most malicious web sites automatically redirect plethora of malcode types installed through browser
browser traffic to additional URLs. Specifically, when a exploits, including viruses that infect files, backdoors that
browser visits a primary URL, the response from that URL open entry points for future unauthorized access, bot
instructs the browser to automatically visit one or more programs that allow the attacker to control a whole network
secondary URLs, which may or may not affect the content of compromised systems, Trojan downloaders that connect
that is displayed to the user. Such redirections typically use to the Internet and download other programs, Trojan
droppers that drop files from themselves without accessingthe Internet, and Trojan proxies that redirect network traffic. between 1.7 and 3.2 GHz, a memory size between 512 MB
Some spyware programs and even anti-spyware programs and 2GB, and was responsible for running one
are also installed through exploits. VM configured with 256 MB to 512MB of RAM. Each VM
supported up to 10 simultaneous browser processes in the
3. The HoneyMonkey System scalable mode, with each process visiting a different URL.
Due to the way HoneyMonkeys detect exploits
The HoneyMonkey system attempts to automatically (discussed later), there is a trade-off between the scan rate
detect and analyze a network of web sites that exploit web and the robustness of exploit detection: if the HoneyMonkey
browsers. Figure 2 illustrates the HoneyMonkey Exploit does not wait long enough or if too many simultaneous
Detection System, shown inside the dotted square, and the browser processes cause excessive slowdown, some exploit
surrounding Anti-Exploit Process which includes both pages may not be able to perform a detectable attack (e.g.,
automatic and manual components. beginning a software installation).
3.1. Exploit Detection System Through extensive experiments, we determined that a
wait time of two minutes was a good trade-off. Taking into
The exploit detection system is the heart of the
account the overhead of restarting VMs in a clean state, each
HoneyMonkeys design. This system consists of a 3-stage
machine was able to scan and analyze between 3,000 to
pipeline of virtual machines. Given a large list of input
4,000 URLs per day. We have since improved the scalability
URLs with a potentially low exploit-URL density, each
of the system to a scan rate of 8,000 URLs per day per
HoneyMonkey in Stage 1 starts with a scalable mode by
machine in the scalable mode. (In contrast, the basic mode
visiting N URLs simultaneously inside one unpatched VM.
scans between 500 and 700 URLs per day per machine.) We
When the HoneyMonkey detects an exploit, it switches to
expect that using a more sophisticated VM platform that
the basic, one-URL-per-VM mode to re-test each of the N
enables significantly more VMs per host machine and faster
suspects in order to determine which ones are exploit URLs.
rollback [VMC+05] would significantly increase our
Stage-2 HoneyMonkeys scan Stage 1 detected exploit- scalability.
URLs and perform recursive redirection analysis to identify
3.1.1. Exploit Detection
all web pages involved in exploit activities and to determine
their relationships. Stage-3 HoneyMonkeys continuously Although it is possible to detect browser exploits by
scan Stage-2 detected exploit-URLs using (nearly) fully building signature-based detection code for each known
Figure 2. HoneyMonkey Exploit Detection System and Anti-Exploit Process
patched VMs in order to detect attacks exploiting the latest vulnerability or exploit, this approach is manually intensive.
vulnerabilities. To lower this cost, we take the following blackbox, non-
signature-based approach: we run a monkey program that
We used a network of 20 machines to produce the results
launches a browser instance to visit each input URL and
reported in this paper. Each machine had a CPU speedthen waits for a few minutes to allow downloading of any monkey in the pipeline to further investigate the strength of
code which may have a short time delay. We then detect a the exploit.
group of persistent-state changes to signal exploitation.
3.1.2. Redirection Analysis
Since the monkey is not instructed to click on any dialog
box to permit software installation, any executable files or Many exploit-URLs identified in Stage 1 do not
registry entries created outside the browser sandbox indicate perform the actual exploits but instead act as front-end
an exploit. This approach has the additional important content providers that serve “interesting” content such as
advantage of allowing the detection of known-vulnerability pornography in order to attract browser traffic. This traffic
exploits and zero-day exploits in a uniform way. is then sold and redirected to back-end exploit providers,
Specifically, the same monkey program running on which specialize in exploiting clients and installing
unpatched machines to detect a broad range of browser- malware.
based vulnerability exploits (as shown in Stages 1 and 2)
URLs visited through traffic redirection can be tracked
can run on fully patched machines to detect zero-day
with a Browser Helper Object (BHO) running within each
exploits, as shown in Stage 3.
browser process or by intercepting and analyzing network
At the end of each visit, the HoneyMonkey generates packets. When the HoneyMonkey runs in its “redirection
an XML report containing the following five pieces of analysis” mode, any automatically visited URLs are fed
information: back to the system for further checking. This recursive
scanning allows the construction of topology graphs based
(1) Executable files created or modified outside the
on traffic redirection. In Section 4, we present our analysis
browser sandbox folders: this is the primary mechanism
of topology graphs to demonstrate how they enable the
for exploit detection. It is implemented on top of the Strider
identification of major exploit providers that receive traffic
Tracer [W03], which uses a file-tracing driver to efficiently
from a large number of content providers; they also show
record every single file read/write operation.
how exploit providers organize their web pages in a way that
(2) Processes created: Strider Tracer also tracks all facilitates customized malware installations for each of their
child processes created by the browser process. affiliates. Finally, we are able to positively identify the web
pages that actually perform the exploits by implementing an
(3) Windows registry entries created or modified:
option in our redirection tracker to block all redirection
Strider Tracer additionally includes a driver that efficiently
traffic.
records every single registry [G04] read/write. To highlight
the most critical entries, we use the Strider Gatekeeper and 3.2. Anti-Exploit Process
GhostBuster filters [W04,W05], which target registry
The Anti-Exploit Process involves generating the input
entries most frequently attacked by spyware, Trojans, and
URL lists for HoneyMonkeys to scan, and taking various
rootkits based on an extensive study. This allows
actions based on analyses of the output exploit-URL data.
HoneyMonkey to detect exploits that modify critical
configuration settings (such as the browser home page and
3.2.1. Generating Input URL Lists
the wallpaper) without creating executable files.
We use three sources for generating “interesting” URLs
(4) Vulnerability exploited: to provide additional for analysis. The first category consists of suspicious URLs
information and to address limitations of the black-box
including web sites that are known to host spyware
approach, we have developed and incorporated a
[CWS05] or malware, links appearing in phishing or spam
vulnerability-specific detector, to be discussed in Section 5. emails [S05] or instant messages, web pages serving
This is based on the vulnerability signature of the exploit,
questionable content such as pornography, URL names that
rather than on any particular piece of malcode.
are typos of popular sites [G05], web sites involved in DNS
(5) Redirect-URLs visited: Since malcode is often cache poisoning [HD05,IW05,S04], and similar common
laundered through other sites, this module allows us to track sources of malicious web content.
redirections to determine both the real source of the The second category consists of the most popular web
malcode and those involved in the distribution chain. pages, which, if compromised, can potentially infect a large
To ease cleanup of infected state, we run population. Examples include the top 100,000 web sites
HoneyMonkeys inside a VM. (Our current implementation based on browser traffic ranking [AL] or the top N million
uses Microsoft Virtual PC and Virtual Server.) Upon web sites based on click-through counts as measured by
detecting an exploit, the monkey saves its logs and notifies search engines.
the Monkey Controller on the host machine to destroy the
The third category encompasses URL lists of a more
infected VM and re-spawn a clean HoneyMonkey, which localized scope. For example, an organization may want to
then continues to visit the remaining URL list. The Monkey
regularly verify that its web pages have not been
Controller then passes the detected exploit-URL to the nextcompromised to exploit visitors; a user may want to monitored URLs “upgrade” its own exploit code or when a
investigate whether any recently visited URL was new URL that hosts zero-day exploit code starts receiving
responsible for causing a spyware infection. redirection traffic from any of the monitored URLs. Zero-
day exploit monitoring is perhaps the most valuable
3.2.2. Acting on Output Exploit-URL Data
contribution of the HoneyMonkey because zeroday exploits
Stage 1 Output – Exploit-URLs can be extremely damaging and whether they are actually
being used in the wild is the most critical piece of
The percentage of exploit-URLs in a given list can be
information in the decision process for security guidance,
used to measure the risk of web surfing. For example, by
patch development, and patch release. When a
comparing the percentage numbers from two URL lists
HoneyMonkey detects a zero-day exploit, it reports the URL
corresponding to two different search categories (e.g.,
to the Microsoft Security Response Center, which works
gambling versus shopping), we can assess the relative risk
closely with the enforcement team and the groups owning
of malware infection for people with different browsing
the software with the vulnerability to thoroughly investigate
habits. Also, we have observed that depth-N crawling of
the case and determine the most appropriate course of
exploit pages containing a large number of links, as
action. We will discuss an actual case in Section 4.2.
illustrated at the top of Figure 2, often leads to the
discovery of more exploit pages. Due to the unavoidable delay between patch release and
patch deployment, it is important to know whether the
Stage 2 Output – Traffic-Redirection Topology Graphs
vulnerabilities fixed in the newly released patch are being
The HoneyMonkey system currently serves as a actively exploited in the wild. Such latest-
leadgeneration tool for the Internet safety enforcement patchedvulnerability exploit monitoring can be achieved by
team in the Microsoft legal department. The topology running HoneyMonkeys on nearly fully patched machines,
graphs and subsequent investigations of the malicious which are missing only the latest patch. This provides
behavior of the installed malware programs provide a visibility into the prevalence of such exploits to help provide
prioritized list for potential enforcement actions that guidance on the urgency of patch deployment.
include sending sitetakedown notices, notifying law
enforcement agencies, and filing civil suits against the 4. Experimental Evaluation
individuals responsible for distributing the malware
programs. We have successfully shut down several We present experimental results in three sections:
malicious URLs discovered by the HoneyMonkey. scanning suspicious URLs, zero-day exploit detection, and
scanning popular URLs. We refer to the first and the third
Due to the international nature of the exploit
sets of data as “suspicious-list data” and “popular-list
community, access blocking may be more appropriate and
data”, respectively. All experiments were performed with
effective than legal actions in many cases. Blocking can be
Internet Explorer browser version 6.0.
implemented at different levels: search engines can remove
exploit-URLs from their database; Internet Service We note that the statistics reported in this paper do not
Providers (ISPs) can black-list exploit-URLs to protect allow us to calculate the total number of end-hosts exploited
their entire customer base; corporate proxy servers can by the malicious web sites we have found. Such calculations
prevent employees from accessing any of the exploitURLs; would require knowing precisely the number of machines
and individual users can block their machines from that have visited each exploit page and whether each
communicating with any exploit sites by editing their local machine has patched the specific vulnerabilities targeted by
“hosts” files to map those server hostnames to a local each visited exploit page.
loopback IP address.
4.1. Scanning Suspicious URLs
Exploit-URLs also provide valuable leads to our
4.1.1. Summary Statistics
antispyware product team. Each installed program is
tagged with an “exploit-based installation without user Our first experiment aimed at gathering a list of most
permission” attribute. This clearly distinguishes the likely candidates for exploit-URLs in order to get the
program from other more benign spyware programs that highest hit rate possible. We collected 16,190 potentially
are always installed after a user accepts the licensing malicious URLs from three sources: (1) a web search of
agreement. “known-bad” web sites involved in the installations of
malicious spyware programs [CWS05]; (2) a web search for
Stage 3 Output – Zero-Day Exploit-URLs and Topology Windows “hosts” files [HF] that are used to block
Graphs advertisements and bad sites by controlling the domain
name-to-IP address mapping; (3) depth-2 crawling of some
By constantly monitoring all known exploit-URLs
of the discovered exploit-URLs.
using HoneyMonkeys running on fully patched machines,
we can detect zero-day exploits either when one of theWe used the Stage-1 HoneyMonkeys running on only upgrade when they find the next easy target. (3) Most
unpatched WinXP SP1 and SP2 VMs to scan the 16,190 security-conscious web users diligently apply patches.
URLs and identified 207 as exploit-URLs; this translates Exploit sites with “advanced” capabilities are likely to draw
into a density of 1.28%. This serves as an upper bound on attention from knowledgeable users and become targets for
the infection rate: if a user does not patch his machine at all investigation.
and he exclusively visits risky web sites with questionable
The SP2-FP numbers again demonstrate the
content, his machine will get exploited by approximately
importance of software patching: none of the 752
one out of every 100 URLs he visits. We will discuss the
exploitURLs was able to exploit a fully updated WinXP SP2
exploit-URL density for normal browsing behavior in
machine according to our May/June 2005 data. As we
Section 4.3.
describe in Section 4.2, there was a period of time in early
After recursive redirection analysis by Stage-2 July when this was no longer true. We were able to quickly
HoneyMonkeys, the list expanded from 207 URLs to 752 identify and report the few exploit providers capable of
URLs – a 263% expansion. This reveals that there is a infecting fully patched machines, which led to actions to
sophisticated network of exploit providers hiding behind shut them down.
URL redirection to perform malicious activities.
4.1.2. Topology graphs and node ranking
Figure 3 shows the breakdown of the 752 exploitURLs
Figure 4 shows the topology graph of the 17
among different service-pack (SP1 or SP2) and patch levels,
exploitURLs for SP2-PP. These are among the most
where “UP” stands for “UnPatched”, “PP” stands for
powerful exploit pages in terms of the number of machines
“Partially Patched”, and “FP” stands for “Fully Patched”.
they are capable of infecting and should be considered high
As expected, the SP1-UP number is much higher than the
priorities for investigation. Rectangular nodes represent
SP2-UP number because the former has more known
individual exploit-URLs. Solid arrows between rectangles
vulnerabilities that have existed for a longer time.
represent automatic traffic redirection. Circles represent site
Number of Number of nodes that act as an aggregation point for all exploit pages
Exploit-URLs Exploit Sites hosted on that site, with the site node having a thin edge
connecting each of its child-page rectangles. Nodes that do
Total 752 288 not receive redirected traffic are most likely content
providers. Nodes that receive traffic from multiple exploit
SP1 Unpatched (SP1-UP) 688 268
sites (for example, the large rectangle R at the bottom) are
SP2 Unpatched (SP2-UP) 204 115 most likely exploit providers.
SP2 Partially Patched 17 10
The size of a node is proportional to the number of
(SP2-PP)
cross-site arrows directly connected to it, both incoming and
outgoing. Such numbers provide a good indication of the
SP2 Fully Patched 0 0 relative popularity of exploit-URLs and sites and are
(SP2-FP) referred to as connection counts. It is clear from the picture
that the large rectangle R and its associated circle C have
Figure 3. Exploit statistics for Windows XP as a
the highest connection counts. Therefore, blocking access to
function of patch levels (May/June 2005 data)
this site would be the most effective starting point since it
The SP2-PP numbers are the numbers of exploit pages would disrupt nearly half of this exploit network.
and sites that successfully exploited a WinXP SP2 machine
partially patched up to early 2005. The fact that the numbers
are one order of magnitude lower than their SP2-UP
counterparts demonstrates the importance of patching. An
important observation is that only a small percentage of
exploit sites are updating their exploit capabilities to keep
up with the latest vulnerabilities, even though proof-of-
concept exploit code for most of the vulnerabilities are
publicly posted. We believe this is due to three factors: (1)
Upgrading and testing new exploit code incurs some cost
which needs to be traded off against the increase in the
number of victim machines; (2) Some vulnerabilities are
more difficult to exploit than others; for example, some of
the attacks are nondeterministic or take longer. Most
exploiters tend to stay with existing, reliable exploits, andFigure 4. SP2-PP topology graph (17 URLs, 10 sites)
The topology graph for the 688 SP1-UP exploit-URLs For example, high-ranked exploit sites in Figure 5 should be
is much larger and more complex. It is only useful when heavily monitored because a zero-day exploit page
viewed from a graph manipulation tool and is therefore connected to any of them would likely affect a large number
omitted here. Most of the URLs appear to be pornography of web sites. Legal investigations should focus on top
pages and the aggressive traffic redirection among them exploit providers, rather than content providers that are
leads to the complexity of the bulk of the graph. In the mere traffic redirectors and do not perform exploits
isolated corners, we found a shopping site redirecting traffic themselves.
to five advertising companies that serve exploiting
Site ranking based on number of hosted exploit-URLs
advertisements, a screensaver freeware site, and over 20
exploit search sites. Next, we describe two ranking Figure 6 illustrates the top 129 sites, each hosting more
algorithms that help prioritize the investigations of these than one exploit URL. This ranking helps highlight those
hundreds of URLs and sites. web sites whose internal page hierarchy provides important
insights. First, some web sites host a large number of exploit
Site ranking based on connection counts
pages with a well-organized hierarchical structure. For
Figure 5 illustrates the top 15 exploit sites for SP1-UP example, the #1 site hosts 24 exploit pages that are
according to their connection counts. The bar height organized by what look likes account names for affiliates;
indicates how many other sites a given site has direct traffic- many others organize their exploit pages by affiliate IDs or
redirection relationship with and likely reflects how referring site names; some even organize their pages by the
entrenched a site owner is with the exploit community. The names of the vulnerabilities they exploit and a few of them
bar for each site is composed of three segments of different have the word “exploit” as part of the URL names.
colors: a black segment represents the number of sites that
The second observation is that some sophisticated web
redirect traffic here; a white segment represents the number
sites use transient URLs that contain random strings. This is
of sites to which traffic is redirected; a gray segment
designed to make investigations more difficult. Site ranking
indicates the number of sites that have two-way traffic
based on the number of hosted exploit-URLs helps highlight
redirection relationship with the given site.
such sites so that they are prioritized higher for
For example, site #15 corresponds to a content provider investigation. The zero-day exploits discussed in the next
who is selling traffic to multiple exploit providers and sub-section provide a good example of this.
sharing traffic with a few other content providers. Site #7
corresponds to an exploit provider that is receiving traffic
from multiple web sites. Sites #4, #5, and #9 correspond to
pornography sites that play a complicated role: they redirect
traffic to many exploit providers and receive traffic from
many content providers. Their heavy involvement in exploit
activities and the fact that they are registered to the same
owner suggest that they may be set up primarily for exploit
purposes.
Site ranking, categorization, and grouping play a key
role in the anti-exploit process because it serves as the basis
for deciding the most effective resource allocation for
monitoring, investigation, blocking, and legal actions.4.2. Zero-Day Exploit Detection exploit provider site that was hosting exploitURLs with
names in the following form:
In early July 2005, a Stage-3 HoneyMonkey discovered
our first zero-day exploit. The javaprxy.dll vulnerability was http://[IP address]/[8 chars]/test2/iejp.htm
known at that time without an available patch [J105,J205], where [8 chars] consists of 8 random characters that
and whether it was actually being exploited was a critical appeared to change gradually over time. Takedown notices
piece of information that was previously not known. The were sent after further investigation of the installed malware
HoneyMonkey system detected the first exploit page within programs, and most of the 25 web pages stopped exploiting
2.5 hours of scanning and it was confirmed to be the first in- the javaprxy.dll vulnerability shortly after that. Latest-
the-wild exploitURL of the vulnerability reported to the Patched-Vulnerability Exploit Monitoring
Microsoft Security Response Center. A second exploit-URL
One day after the patch release, HoneyMonkey detected
60
Number of sites from which traffic is received
50
Number of sites with two-way redirection
40 Number of sites to which traffic is redirected
30
20
10
0
Figure 5. Top 15 exploit sites ranked by connection counts, among the 268 SP1-UP exploit sites
from the suspicious list
Site ranking based on the number of hosted exploit URLs
Figure 6. Top 129 SP1-UP exploit sites ranked by the number of exploit-URLs hosted
was detected in the next hour. These two occupy positions another jump in the number of exploit-URLs for the
#132 and #179, respectively, in our list of 752 monitored vulnerability: 53 URLs from 12 sites were upgraded in the
URLs. This information enabled the response center to subsequent six days. Redirection analysis revealed that all
provide customers with a security advisory and a followup of them were redirecting traffic to a previously known
security bulletin [SH, J205]. exploit provider (ranked #1 in Figure 6) who decided to add
a new exploit page for javaprxy.dll to increase its infection
During the subsequent five days, HoneyMonkey
base. A takedown notice was sent after malware
detected that 26 of the 752 exploit-URLs upgraded to the
investigation and all 53 URLs stopped exploiting within a
zero-day exploit. Redirection analysis further revealed that
couple of days.
25 of them were redirecting traffic to a previously unknownImportant Observations question, we gathered the most popular one million URLs
This experience provides concrete evidence that the as measured by the click-through counts from a search
HoneyMonkey system can potentially evolve into a engine and tested them with the HoneyMonkey system. We
fullfledged, systematic and automatic zero-day exploit also compared the results of this popular-list data with the
monitoring system for browser-based attacks. We make the suspicious-list data in Section 4.1. Figure 7 summarizes the
following observations from the initial success: comparison of key data.
(1) Monitoring easy-to-find exploit-URLs is
effective: we predicted that monitoring the 752
exploitURLs would be useful for detecting zero-day
Suspicious List Popular List
exploits because the fact that we could find them quickly
within the first month implies that they are more popular # URLs scanned 16,190 1,000,000
and easier to reach. Although zero-day exploits are
extremely powerful, they need to connect to popular web # Exploit URLs 207 (1.28%) 710 (0.071%)
sites in order to receive traffic to exploit. If they connect to
# Exploit URLs 752 (263%) 1,036
any of the monitored URLs in our list, the HoneyMonkey
can quickly detect the exploits and identify the exploit After Redirection (46%)
providers behind the scene through redirection analysis. (Expansion Factor)
Our zero-day exploit detection experience confirmed the
# Exploit Sites 288 470
effectiveness of this approach.
(2) Monitoring content providers with well-known SP2-to-SP1 Ratio 204/688 = 0.30 131/980 = 0.13
URLs is effective: we predicted that monitoring content
Figure 7. Comparison of the suspicious-list and
providers would be useful for tracking the potentially
popular-list data.
dynamic behavior of exploit providers. Unlike exploit
providers who could easily move from one IP address to 4.3.1. Summary Statistics
another and use random URLs, content providers need to Before redirection analysis
maintain their well-known URLs in order to continue
Of the one million URLs, HoneyMonkey determined
attracting browser traffic. The HoneyMonkey takes
that 710 were exploit pages. This translates into a density of
advantage of this fundamental weakness in the
0.071%, which is between one to two orders of magnitude
browserbased exploit model and utilizes the content
lower than the 1.28% number from the suspicious-list data.
providers as convenient entry points into the exploit
The distribution of exploit-URLs among the ranked list is
network. Again, our zero-day exploit detection experience
fairly uniform, which implies that the next million URLs
confirmed the effectiveness of this approach.
likely exhibit a similar distribution and so there are likely
(3) Monitoring highly ranked and advanced many more exploit URLs to be discovered. Eleven of the
exploitURLs is effective: we predicted that the top exploit 710 exploit pages are very popular: they are among the top
sites we identified are more likely to upgrade their exploits 10,000 of the one million URLs that we scanned. This
because they have a serious investment in this business. demonstrates the need for constant, automatic web patrol of
Also, web sites that appear in the SP2-PP graph are more popular pages in order to protect the Internet from large-
likely to upgrade because they appeared to be more up- scale infections.
todate exploiters. Both predictions have been shown to be
After redirection analysis:
true: the first detected zero-day exploit-URL belongs to the
The Stage-2 HoneyMonkey redirection analysis
#9 site in Figure 5 (which is registered to the same email
expanded the list of 710 exploit-URLs to 1,036 URLs
address that also owns the #4 and #5 sites) and 7 of the top
hosted by 470 sites. This (1,036-710)/710=46% expansion
10 sites in Figure 5 upgraded to the javaprxy.dll exploit;
is much lower than the 263% expansion in the suspiciouslist
nearly half of the SP2-PP exploit-URLs in Figure 4
data, suggesting that the redirection network behind the
upgraded as well.
former is less complex. The SP2-to-SP1 ratio of 0.13 is
4.3. Scanning Popular URLs lower than its counterpart of 0.30 from the suspiciouslist
data (see Figure 7). This suggests that overall the exploit
By specifically searching for potentially malicious web
capabilities in the popular list are not as advanced as those
sites, we were able to obtain a list of URLs that have 1.28%
in the suspicious list, which is consistent with the findings
of the pages performing exploits. A natural question that
from our manual analysis.
most web users will ask is: if I never visit those risky web
sites that serve dangerous or questionable content, do I have Intersecting the 470 exploit sites with the 288 sites from
to worry about vulnerability exploits? To answer this Section 4.1 yields only 17 sites. These numbers suggest that
the degree of overlap between the suspicious list, generallywith more powerful attacks, and the popular list is not the suspicious-list, sites in the popular-list data redirect
alarmingly high at this point. But more and more exploit traffic to at most 4 sites. This suggests that aggressive traffic
sites from the suspicious list may try to “infiltrate” the selling is also a phenomenon unique to the malicious
popular list to increase their infection base. In total, we have pornography community.
collected 1,780 exploit-URLs hosted by 741 sites. Finally, the top four exploit providers in the popularlist
clearly stand out. None of them have any URLs in the
original list of one million URLs, but all of them are behind
a large number of exploit pages which redirect traffic to
them. The #1 site provides exploits to 75 web sites primarily
in the following five categories: (1) celebrities, (2) song
4.3.2. Node ranking
lyrics, (3) wallpapers, (4) video game cheats, and (5)
Site ranking based on connection counts wrestling. The #2 site receives traffic from 72 web sites, the
Figure 8 illustrates the top 15 SP1-UP exploit sites by majority of which are located in one particular country. The
connection counts. There are several interesting differences #3 site is behind 56 related web sites that serve cartoon-
between the two data sets behind the suspicious-list related pornographic content. The #4 site appears to be an
exploiters (Figure 5) and the popular-list exploiters (Figure advertising company serving exploiting links through web
8). First, there is not a single pair of exploit sites in the sites that overlap significantly with those covered by the #1
popular-list data that are doing twoway traffic redirection, site.
which appears to be unique in the malicious pornography
Site ranking based on number of hosted exploit-URLs
community. Second, while it is not uncommon to see web
sites redirecting traffic to more than 10 or even 20 sites in
Figure 8. Top 15 exploit sites ranked by connection counts, among the 426 SP1-UP exploit sites
Site ranking based on number of hosted exploit URLs
Figure 9. Top 122 sites ranked by the number of exploit-URLs, among the 426 SP1-UP exploit sitesFigure 9 illustrates the top 122 sites hosting more than
one exploit URL. Unlike Figure 6, which highlightsmostly exploit provider sites, Figure 9 highlights many and to detect CAPTCHA tests when we see web sites
content provider sites that host a large number of exploit starting to adopt such techniques to evade detection.
pages containing a similar type of content. Again, the top
(3) Detecting the presence of a VM or the
four sites stand out: the #1 site is a content provider of video
HoneyMonkey code: Malicious code could detect a VM by
game cheats information for multiple game consoles. The
executing a series of instructions with high virtualization
#2 site (which also appears as the third entry in Figure 8)
overhead and comparing the elapsed time to some external
hosts a separate URL for each different web site from which
reference [VMC+05]; by detecting the use of reserved x86
it receives traffic. The #3 site is a content provider that has
opcodes normally only used by specific VMs [L05]; by
a separate entry page for each celebrity figure. The #4 site
leveraging information leaked by sensitive, non-privileged
is a content provider of song lyrics with one entry page per
instructions [RP]; and by observing certain file directory
celebrity singer.
contents known to be associated with UML (User-Mode
Linux) [CDF+04] or a specific hardware configuration,
5. Discussions
default MAC address, or I/O backdoor associated with
VMware [HR05].
Now that the effectiveness of the HoneyMonkey system
is widely known [HM], it is expected that exploit sites will Most VM-detection techniques arise due to the fact that
start adopting techniques to evade HoneyMonkey detection. the x86 processors are not fully virtualizable. Fortunately,
We discuss three types of potential evasion techniques and both Intel [VT] and AMD [PVT] have proposed architecture
our countermeasures. Since it has become clear that a extensions that would make x86 processors fully
weakness of the HoneyMonkey is the time window between virtualizable, and thus make detecting a VM more difficult.
a successful exploit that allows foreign code execution and In the meantime, we can adopt antidetection techniques that
the subsequent execution of the HoneyMonkey exploit target known VM-detection methods [CDF+04,VMC+05].
detection code, we have developed and integrated a tool As VMs are increasingly used as general computing
called Vulnerability-Specific Exploit Detector (VSED), platforms, the approach of detecting HoneyMonkeys by
which allows the HoneyMonkey to detect and record the detecting VMs will become less effective.
first sign of an exploit. Such a detector only works for
Alternatively, we developed techniques that allow us to
known vulnerabilities though; detecting zero-day exploits of
also run HoneyMonkey on non-virtual machines so that the
totally unknown vulnerabilities remains a challenge. The
results can be cross-checked to identify sophisticated
VSED tool will be discussed in Section 5.4.
attackers. We implemented support to efficiently checkpoint
5.1. Identifying HoneyMonkey Machines our system (both memory and disk state) when it is in a
known-good state, and roll back to that checkpoint after an
There are three ways for an exploit site to identify
attack has been detected. To checkpoint memory, we utilized
HoneyMonkey machines and skip exploits.
the hibernation functionality already present in Windows to
(1) Targeting HoneyMonkey IP addresses: The efficiently store and restore memory snapshots. To support
easiest way is to black-list the IP addresses of disk checkpoints, we implemented copy-on-write disk
HoneyMonkey machines. We plan to run the HoneyMonkey functionality by modifying the generic Windows disk class
network behind multiple ISPs with dynamically assigned IP driver which is used by most disks today. Our copyon-write
addresses. If an exploit site wants to black-list all IP implementation divides the physical disk into two equally
addresses belonging to these ISPs, it will need to sacrifice a sized partitions. We use the first partition to hold the default
significant percentage of its infection base. One market disk image that we roll back to when restoring a checkpoint,
research study of ISP client membership [ISP] shows that and the second partition as a scratch partition to store all disk
the top 10 US ISPs service over 62% of US Internet users. writes made after taking a checkpoint. We maintain a bitmap
in memory to record which blocks have been written to so
(2) Performing a test to determine if a human is
we know which partition contains the most recent version
present: Currently, HoneyMonkeys do not click on any
of each individual block. As a result, no extra disk reads or
dialog box. A malicious web site could introduce a onetime
writes are needed to provide copy-on-write functionality
dialog box that asks a simple question; after the user clicks
and a rollback can be simply accomplished by zeroing out
the OK button to prove he’s human, the web site drops a
the bitmap. To provide further protection, we can adopt
cookie to suppress the dialog box for future visits. More
resource-hiding techniques to hide the driver from
sophisticated web sites can replace the simple dialog box
sophisticated attackers who are trying to detect the driver to
with a CAPTCHA Turing Test [ABL04] (although this
identify a HoneyMonkey machine.
would raise suspicion because most non-exploiting sites do
not use such tests). We will need to incorporate additional Some exploit sites may be able to obtain the
intelligence into the HoneyMonkeys to handle dialog boxes “signatures” of the HoneyMonkey logging infrastructure
and build a detection mechanism to allow them to disablethe logging or tamper with the log. Since such detection 5.4. Vulnerability-Specific Exploit
code can only be executed after a successful exploit, we can Detector (VSED)
use VSED to detect occurrences of exploits and highlight
To address some of the limitations discussed above and
those that do not have a corresponding filecreation log.
to provide additional information on the exact
Additionally, we are incorporating log signing techniques to
vulnerabilities being exploited, we have developed a
detect missing or modified log entries.
vulnerability-specific detector, called VSED, and integrated
We note that some classes of exploits require writing a it into the HoneyMonkey. The VSED tool implements a
file to disk and then executing that file for running arbitrary source-code level, vulnerability-specific intrusion detection
code. These exploits cannot escape our detection by trying technique that is similar to IntroVirt [JKD+05]. For each
to identify a HoneyMonkey machine because our file-based vulnerability, we manually write “predicates” to test the
detection actually occurs before they can execute code. state of the monitored program to determine when an
attacker is about to trigger a vulnerability. VSED operates
5.2. Exploiting without Triggering
by inserting breakpoints within buggy code to stop
HoneyMonkey Detection
execution before potentially malicious code runs, in order to
Currently, HoneyMonkey cannot detect exploits that do allow secure logging of an exploit alert. For example, VSED
not make any persistent-state changes or make such changes would detect a buffer overflow involving the “strcpy”
only inside browser sandbox. Even with this limitation, the function by setting a breakpoint right before the buggy
HoneyMonkey is able to detect most of today’s Trojans, “strcpy” executes. Once VSED stops the application, the
backdoors, and spyware programs that rely on significant predicate examines the variables passed into “strcpy” to
persistent-state changes to enable automatic restart upon determine if an overflow is going to happen.
reboot. Again, the VSED tool can help address this
To evaluate the effectiveness of VSED for detecting
limitation.
browser-based exploits, we wrote predicates for six recent
HoneyMonkeys only wait for a few minutes for each IE vulnerabilities and tested them against the exploitURLs
URL. So a possible evasion technique is to delay the exploit. from both the suspicious list and the popular list. Although
However, such delays reduce the chance of successful we do not have a comprehensive list of predicates built yet,
infections because real users may close the browser before we can already pinpoint the vulnerabilities exploited by
the exploit happens. We plan to run HoneyMonkeys with hundreds of exploit-URLs. One limitation of VSED is that
random wait times and highlight those exploit pages that it cannot identify zero-day exploits of unknown
exhibit inconsistent behaviors across runs for more in-depth vulnerabilities.
manual analysis.
6. Related Work
5.3. Randomizing the Attacks
There is a rich body of literature on honeypots. Most
Exploit sites may try to inject nondeterministic
honeypots are deployed to mimic vulnerable servers waiting
behavior to complicate the HoneyMonkey detection. They
for attacks from client machines [H,P04,J04,KGO+05]. In
may randomly exploit one in every N browser visits. We
contrast, HoneyMonkeys are deployed to mimic clients
consider this an acceptable trade-off: while this would
drawing attacks from malicious servers.
require multiple scans by the HoneyMonkeys to detect an
exploit, it forces the exploit sites to reduce their infection To our knowledge, there are three other projects related
rates by N times as well. If a major exploit provider is behind to the concept of client-side honeypots: email quarantine,
more than N monitored content providers, the shadow honeypots, and Honeyclient. Sidiroglou et al.
HoneyMonkey can still detect it through redirection [SK05] described an email quarantine system which
tracking in one round of scans. intercepts every incoming message, “opens” all suspicious
attachments inside instrumented virtual machines, uses
Exploit sites may try to randomize URL redirections by
behavior-based anomaly detection to flag potentially
selecting a random subset of machines to forward traffic to
malicious actions, quarantines flagged emails, and only
each time, from a large set of infected machines that are
delivers messages that are deemed safe.
made to host exploit code. Our node ranking algorithm
based on connection counts should discourage this because Anagnostakis et al. [ASA+05] proposed the technique
such sites would end up prioritizing themselves higher for of “shadow honeypots” which are applicable to both servers
investigation. Also, they reveal the identities of infected and clients. The key idea is to combine anomaly detection
machines, whose owners can be notified to clean up the with honeypots by diverting suspicious traffic identified by
machines. anomaly detectors to a shadow version of the target
application that is instrumented to detect potential attacks
and filter out false positives. As a demonstration of client-side protection, the authors deployed their prototype on 7. Summary
Mozilla Firefox browsers.
We have presented the design and implementation of
The two client-side honeypots described above are both
the Strider HoneyMonkey as the first systematic method for
passive in that they are given existing traffic and do not
automated web patrol to hunt for malicious web sites that
actively solicit traffic. In contrast, HoneyMonkeys are
exploit browser vulnerabilities. Our analyses of two sets of
active and are responsible for seeking out malicious web
data showed that the densities of malicious URLs are 1.28%
sites and drawing attack traffic from them. The former has
and 0.071%, respectively. In total, we have identified a large
the advantage of providing effective, focused protection of
community of 741 web sites hosting 1,780 exploit-URLs.
targeted population. The latter has the advantages of staying
We proposed using topology graphs based on redirection
out of the application’s critical path and achieving a broader
traffic to capture the relationship between exploit sites and
coverage, but it does require additional defense against
using site ranking algorithms based on the number of
potential traps/black-holes during the recursive redirection
directly connected sites and the number of hosted exploit-
analysis. The two approaches are complementary and can be
URLs to identify major players. Our success in detecting the
used in conjunction with each other to provide maximum
first-reported, in-the-wild, zero-day exploit-URL of the
protection.
javaprxy.dll vulnerability provided the best demonstration
In parallel with our work, the Honeyclient project [HC] of the effectiveness of our approach by monitoring easy-to-
shares the same goal of trying to identify browserbased find content providers with well-known URLs as well as top
attacks. However, the project has not published any exploit providers with advanced exploit capabilities.
deployment experience or any data on detected Finally, we discussed several techniques that malicious web
exploitURLs. There are also several major differences in sites can adopt to evade HoneyMonkey detection, which
terms of implementation: Honeyclient is not VM-based, motivated us to incorporate an additional vulnerability-
does not use a pipeline of machines with different patch specific exploit detection mechanism to complement the
levels, and does not track URL redirections. HoneyMonkey’s core black-box exploit detection approach.
Existing honeypot techniques can be categorized using
Acknowledgement
two other criteria: (1) physical honeypots [KGO+05] with
dedicated physical machines versus virtual honeypots built
We would like to express our sincere thanks to the
on Virtual Machines [VMW,UML]; (2) lowinteraction
anonymous reviewers and our shepherd Nick Weaver for
honeypots [P04], which only simulate network protocol
their valuable comments.
stacks of different operating systems, versus high-
interaction honeypots [J04], which provide an authentic
References
decoy system environment. HoneyMonkeys belong to the
category of high-interaction, virtual honeypots. [AA] S. Andersen and V. Abella, “Data Execution Prevention.
Changes to Functionality in Microsoft Windows XP Service
In contrast with the black-box, state-change-based
Pack 2, Part 3: Memory Protection Technologies.,”
detection approach used in HoneyMonkey, several papers
http://www.microsoft.com/technet/prodtechnol/winxppro/mai
proposed vulnerability-oriented detection methods, which ntain/ sp2mempr.mspx.
can be further divided into vulnerability-specific and
[ABL04] L. von Ahn, M. Blum, and J. Langford, “Telling Humans
vulnerability-generic methods. The former includes Shield and Computers Apart Automatically,”
[WGS+04], a network-level filter designed to detect worms Communications of the ACM, Feb. 2004.
exploiting known vulnerabilities, and IntroVirt [JKD+05], a
[AL] Alexa, http://www.alexa.com/.
technique for specifying and monitoring vulnerability-
[ASA+05] K. Anagnostakisy, S. Sidiroglouz, P. Akritidis, K.
specific predicates at code level. The latter includes system
Xinidis, E. Markatos, and A. Keromytis. “Detecting Targeted
call-based intrusion detection systems [FHS+96,FKF+03], Attacks Using Shadow Honeypots,” in Proc. USENIX Security
memory layout randomization [ASLR,XKI03], non- Symposium, August 2005.
executable pages [AA] and pointer encryption [CBJ+03]. [ASLR] PaX Address Space Layout Randomization (ASLR).
An advantage of vulnerabilityoriented techniques is the http://pax.grsecurity.net/docs/aslr.txt.
ability to detect an exploit earlier and identify the exact
[B04] Xpire.info, http://www.vitalsecurity.org/xpiresplitinfinity-
vulnerability being exploited. As discussed in Section 5.4, serverhack_malwareinstall-condensed.pdf, Nov. 2004.
we have incorporated IntroVirt-style, vulnerability-specific
[CBJ+03] C. Cowan, S. Beattie, J. Johansen, and P. Wagle.
detection capability into the HoneyMonkey. “PointGuard: Protecting pointers from buffer overflow
vulnerabilities,” in Proc. USENIX Security Symposium, August
2003.
[CDF+04] C. Carella, J. Dike, N. Fox, and M. Ryan, “UML
Extensions for Honeypots in the ISTS Distributed HoneypotProject,” in Proc. IEEE Workshop on Information Assurance, (903235),
2004. http://www.microsoft.com/technet/security/bulletin/ms05037.
[CWS05] “Webroot: CoolWebSearch Top Spyware Threat,” mspx.
http://www.techweb.com/showArticle.jhtml?articleID=16040 [JKD+05] Ashlesha Joshi, Sam King, George Dunlap, Peter Chen,
0314, TechWeb, March 30, 2005. “Detecting Past and Present Intrusions Through Vulnerability-
[D04] Download.Ject, Specific Predicates,” in Proc. SOSP, 2005.
http://www.microsoft.com/security/incident/download_ject.ms [KGO+05] Sven Krasser, Julian Grizzard, Henry Owen, and John
px, June 2004. Levine, “The Use of Honeynets to Increase Computer Network
[E04] Ben Edelman, “Who Profits from Security Holes?”, Nov. Security and User Awareness”, in Journal of Security
2004, http://www.benedelman.org/news/111804-1.html. Education, pp. 23-37, vol. 1, no. 2/3. March 2005.
[F04] “Follow the Money; or, why does my computer keep [L05] Lallous, ” Detect if your program is running inside a
getting infested with spyware?” Virtual Machine,” March 2005,
http://www.livejournal.com/users/tacit/125748.html. http://www.codeproject.com/system/VmDetect.asp.
[FHS+96] S. Forrest, S. Hofmeyr, A. Somayaji, and T. Longsta, [M52] Microsoft Security Bulletin MS05-002, Vulnerability in
“A sense of self for Unix processes,” in Proc. IEEE Symp. on Cursor and Icon Format Handling Could Allow Remote Code
Security and Privacy, May 1996. Execution,
[FKF+03] H. Feng, O. Kolesnikov, P. Fogla, W. Lee, and W. Gong, http://www.microsoft.com/technet/security/Bulletin/MS05002.
“Anomaly detection using call stack information,” in Proc. mspx.
IEEE Symp. on Security and Privacy, May 2003. [M311] Microsoft Security Bulletin MS03-011, Flaw in Microsoft
[G05] “Googkle.com installed malware by exploiting browser VM Could Enable System Compromise,
vulnerabilities,” http://www.f- http://www.microsoft.com/technet/security/Bulletin/MS03011.
secure.com/vdescs/googkle.shtml. mspx.
[G04] Archana Ganapathi, Yi-Min Wang, Ni Lao, and Ji-Rong [M413] Microsoft Security Bulletin MS04-013, Cumulative
Wen, "Why PCs Are Fragile and What We Can Do About It: A Security Update for Outlook Express,
Study of Windows Registry Problems", in Proc. IEEE http://www.microsoft.com/technet/security/Bulletin/MS040
DSN/DCC, June 2004. 13.mspx.
[H] The Honeynet Project, http://www.honeynet.org/. [NK04] Neal Krawetz, Anti-honeypot technology, Security &
Privacy Magazine, IEEE Volume 2, Issue 1, Jan.-Feb. 2004
[HC] Honeyclient Development Project,
Page(s):76–79.
http://www.honeyclient.org/.
[P04] Niels Provos, “A Virtual Honeypot Framework”, in Proc.
[HD05] “Another round of DNS cache poisoning,” Handlers
USENIX Security Symposium, Aug. 2004.
Diary, March 30, 2005, http://isc.sans.org/.
[PVT] AMD Pacifica Virtualization Technology,
[HF] hpHOSTS community managed hosts file, http://www.hosts-
http://enterprise.amd.com/downloadables/Pacifica.ppt.
file.net/downloads.html.
[R05] “Russians use affiliate model to spread spyware,”
[HM] Strider HoneyMonkey Exploit Detection,
http://www.itnews.com.au/newsstory.aspx?CIaNID=18926.
http://research.microsoft.com/HoneyMonkey.
[R04] Team Register, “Bofra exploit hits our ad serving supplier,”
[HR05] T. Holz and F. Raynal, “Detecting Honeypots and other
http://www.theregister.co.uk/2004/11/21/register_adserver_att
suspicious environments,” in Proc. IEEE Workshop on
ack/, November 2004.
Information Assurance and Security, 2005.
[RP] Red Pill, http://invisiblethings.org/papers/redpill.html.
[IF05] “iframeDOLLARS dot biz partnership maliciousness,”
http://isc.sans.org/diary.php?date=2005-05-23. [S04] Symantec Gateway Security Products DNS Cache
Poisoning Vulnerability,
[ISP] ISP Ranking by Subscriber,
http://securityresponse.symantec.com/avcenter/security/Conte
http://www.ispplanet.com/research/rankings/index.html.
nt/2004.06.21.html.
[IW05] “Scammers use Symantec, DNS holes to push adware,”
[S05] “Michael Jackson suicide spam leads to Trojan horse,”
InfoWorld.com, March 7, 2005,
http://www.sophos.com/virusinfo/articles/jackotrojan.html,
http://www.infoworld.com/article/05/03/07/HNsymantecholes
Sophos, June 9, 2005.
andadware_1.html?DESKTOP%20SECURITY.
[SH] “What is Strider HoneyMonkey,”
[J04] Xuxian Jiang, Dongyan Xu, “Collapsar: A VM-Based
http://research.microsoft.com/honeymonkey/article.aspx, Aug.
Architecture for Network Attack Detention Center”, in Proc.
2005.
USENIX Security Symposium, Aug. 2004.
[SK05] Stelios Sidiroglou and Angelos D. Keromytis, “A Network
[J105] Microsoft Security Advisory (903144) - A COM Object
Worm Vaccine Architecture,” in 1st Information Security
(Javaprxy.dll) Could Cause Internet Explorer to Unexpectedly
Practice and Experience Conference (ISPEC), April 2005.
Exit,
http://www.microsoft.com/technet/security/advisory/903144. [T05] Michael Ligh, “Tri-Mode Browser Exploits - MHTML,
mspx. ANI, and ByteVerify,”
http://www.mnin.org/write/2005_trimode.html, April 30,
[J205] Microsoft Security Bulletin MS05-037 - Vulnerability in
2005.
JView Profiler Could Allow Remote Code Execution[UML] Know Your Enemy: Learning with User-Mode Linux. [WGS+04] Helen J. Wang, Chuanxiong Guo, Daniel R. Simon,
Building Virutal Honeynets using UML, and Alf Zugenmaier, “Shield: Vulnerability-Driven Network
http://www.honeynet.org/papers/uml/. Filters for Preventing Known Vulnerability Exploits,” in Proc.
ACM SIGCOMM, August 2004.
[VMC+05] Michael Vrable, Justin Ma, Jay Chen, David Moore,
Erik Vandekieft, Alex Snoeren, Geoff Voelker, and Stefan [XKI03] J. Xu, Z. Kalbarczyk and R. K. Iyer, “Transparent
Savage, “Scalability, Fidelity and Containment in the Potemkin Runtime Randomization for Security,” in Proc. Symp. Reliable
Virtual Honeyfarm,” in Proc. ACM Symposium on Operating and Distributed Systems (SRDS), October 2003.
Systems Principles (SOSP), Oct. 2005.
[XSS] “Code insertion in Blogger comments”, March 28, 2005,
[VMW] Know Your Enemy: Learning with VMware. Building http://www.securityfocus.com/archive/1/394532.
Virutal Honeynets using VMware,
http://www.honeynet.org/papers/vmware/.
[VT] Vanderpool Technology, Technical report, Intel Corporation,
2005.
[W03] Yi-Min Wang, et al., “STRIDER: A Black-box, Statebased
Approach to Change and Configuration Management and
Support”, in Proc. Usenix LISA, Oct. 2003.
[W04] Yi-Min Wang, et al., “Gatekeeper: Monitoring Auto-Start
Extensibility Points (ASEPs) for Spyware Management”, in
Proc. Usenix LISA, 2004
[W05] Yi-Min Wang, Doug Beck, Binh Vo, Roussi Roussev, and
Chad Verbowski, “Detecting Stealth Software with Strider
GhostBuster,” in Proc. DSN, June 2005