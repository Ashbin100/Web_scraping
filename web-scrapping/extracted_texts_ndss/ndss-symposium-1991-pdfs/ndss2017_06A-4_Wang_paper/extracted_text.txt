TenantGuard: Scalable Runtime Verification of
Cloud-Wide VM-Level Network Isolation
Yushun Wang∗, Taous Madi∗, Suryadipta Majumdar∗, Yosr Jarraya†,
Amir Alimohammadifar∗, Makan Pourzandi†, Lingyu Wang∗ and Mourad Debbabi∗
∗CIISE, Concordia University, Canada
Email: {yus wang, t madi, su majum, ami alim, wang, debbabi}@encs.concordia.ca
†Ericsson Security Research, Ericsson Canada
Email: {yosr.jarraya, makan.pourzandi}@ericsson.com
Abstract—Multi-tenancyinthecloudusuallyleadstosecurity compliancewithsecuritystandards(e.g.,ISO27002/27017[6],
concerns over network isolation around each cloud tenant’s [7] and CCM 3.0.1 [8]).
virtual resources. However, verifying network isolation in cloud
Verifying network isolation potentially requires checking
virtual networks poses several uniquechallenges. The sheer size
of virtual networks implies a prohibitive complexity, whereas that VMs are either reachable or isolated from each other
the constant changes in virtual resources demand a short re- exactly as specified in cloud tenants’ security policies. In
sponse time. To make things worse, such networks typically contrast to traditional networks, virtual networks pose unique
allow fine-grained (e.g., VM-level) and distributed (e.g., security challenges to the verification of network isolation.
groups) network access control. Those challenges can either
invalidate existing approaches or cause an unacceptable delay - First, the sheer size of virtual networks inside a cloud
which prevents runtime applications. In this paper, we present implies a prohibitive complexity. For example, a decent-
TenantGuard, a scalable system for verifying cloud-wide, VM- size cloud is said to have around 1,000 tenants and
level network isolation at runtime. We take advantage of the 100,000users,with17percentofusershavingmorethan
hierarchical natureof virtual networks, efficient data structures, 1,000VMs[9],[10].Performingacloud-wideverification
incremental verification, and parallel computation to reduce the
of network isolation at the VM-level for such a cloud
performanceoverheadofsecurityverification.Weimplementour
withpotentiallymillionsofactiveVMpairsusingexisting
approachbasedonOpenStackandevaluateitsperformanceboth
approaches results in a significant delay (e.g., Plotkin et
in-houseandonAmazon EC2,whichconfirmsitsscalabilityand
efficiency (13 seconds for verifying 168 millions of VM pairs). al. [11] take 2 hours to verify 100k VMs). Most existing
WefurtherintegrateTenantGuardwithCongress,anOpenStack techniquesinphysicalnetworksarenotdesignedforsuch
policy service, to verify compliance with respect to isolation a scale, and will naturally suffer from scalability issues
requirementsbasedontenant-specifichigh-levelsecuritypolicies. (a detailed review of related work is given in Section II)
and quantitative comparison with state-of-the-art work is
provided in Section VI.
I. INTRODUCTION
- Second, the self-service nature of a cloud means virtual
The widespread adoption of cloud is still being hindered
resources in a cloud (e.g., VMs and virtual routers or
by security and privacy concerns, especially the lack of trans-
firewalls) can be added, deleted, or migrated at any
parency, accountability, and auditability [1]. Particularly, in
time by cloud tenants themselves. Consequently, tenants
a multi-tenant cloud environment, virtualization allows opti-
may want to verify the network isolation repeatedly or
mal and cost-effective sharing of physical resources, such as
periodically at runtime, instead of performing it only
computing and networking services, among multiple tenants.
once and offline. Moreover, since any verification result
On the other hand, multi-tenancy is also a double-edged
will likely have a much shorter lifespan under such a
sword that often leads cloud tenants to raise questions like:
constantlychangingenvironment,tenantswouldnaturally
“Are my virtual machines (VMs) properlyisolated from other
expect the results to be returned in seconds, instead of
tenants,especiallymycompetitors?”Infact,networkisolation
minutes or hours demanded by existing approaches [11].
is among the foremost security concerns for most cloud
tenants[2],[3],andcloudprovidersoftenhaveanobligationto - Third, a unique feature of virtual networks, quite unlike
provideclearevidencesforsufficientnetworkisolation[4],[5], that in traditional networks, is the fine-grained and dis-
eitheraspartoftheservicelevelagreements,ortodemonstrate tributednatureofnetworkaccesscontrolmechanisms.For
example, instead of only determined by a few physical
routersandfirewalls,thefateofapackettraversingvirtual
Permission to freely reproduce all or part of this paper for noncommercial networkswillalsodependontheforwardingandfiltering
purposes is granted provided that copies bear this notice and the full citation
rules of all the virtual routers, distributed firewalls (e.g.,
on the first page. Reproduction for commercial purposes is strictly prohibited
without the prior written consent of the Internet Society, the first-named author security groups in OpenStack [12]), and network address
(for reproduction of an entire paper only), and the author’s employer if the translation (NAT), which are commonly deployed in a
paper was prepared within the scope of employment. very fine-grained manner, such as on individual VMs.
NDSS ’17, 26 February - 1 March 2017, San Diego, CA, USA
Unfortunately, most existing works fail to reach such a
Copyright 2017 Internet Society, ISBN 1-891562-46-0
granularity since they are mostly designed for (physical)
http://dx.doi.org/10.14722/ndss.2017.23365network-level verification (i.e., between IP prefixes) in- performed every five minutes and is expecting to see the
stead of VM-level verification with distributed firewalls. results within a few seconds, since she knows the result
may only be valid until the next change is made to the
Motivating Example. Figure 1 shows the simplified view of virtual networks (e.g., adding a port by Bob). Finally, to
a multi-tenantcloudenvironment.1Thesolid lineboxesdepict perform the verification, Alice must collect information
the physical machines (N compute nodes and one network from heterogeneous data sources scattered at different
node)insidewhicharetheVMs,distributedfirewalls(security locations (e.g., routing and NAT rules in virtual routers,
groups), and virtual routers or switches. The virtual resources host routes of subnets, and firewall rules implementing
ofdifferenttenants(e.g.,VM_A1ofAlice,andVM_B2ofBob) tenant security groups).
are depicted by different filling patterns.
In this paper, we present TenantGuard, a scalable system
Alice’s resources Bob’s resources External Network for verifying cloud-wide, VM-level network isolation at run-
VM Security group Virtual router time,whileconsideringtheuniquefeaturesofvirtualnetworks,
Network node
Compute node 1 Compute node N such as distributed firewalls. To address the aforementioned
VM_A1 VM_B2
Priv: 10.0.0.12 Priv: 19.0.0.30 Virtual Switch challenges, our main ideas are as follows. First, TenantGuard
Pub: 1.10.0.75 Pub: 1.10.1.12 takes advantage of the hierarchical structure found in most
R_A1
virtual networks (e.g., OpenStack includes several abstraction
R_A2
. . . R_A3 layers organized in a hierarchical manner, including VM
Allow src Allow src R_B1
1.10.1.12 1.10.0.75 ports, subnets, router interfaces, routers, router gateways, and
VLAN 200 VLAN 201
VLAN 100 VLAN 103 VLAN 300 VLAN 103 external networks) to reduce the performance overhead of
Virtual Switch Virtual Switch Virtual Switch verification.Second,TenantGuardadoptsatop-downapproach
byfirstperformingtheverificationatthe(privateandpublic)IP
VNet 101
VNet 110
VNet 200 prefixlevel,andthenpropagatingthepartialverificationresults
VNet 101 VNet 110 down to the VM-level through efficient data structures with
VNet 200
constant search time, such as radix binary tries [15] and X-
fastbinarytries[16].Third,TenantGuardsupportsincremental
Fig.1. AnExampleofaMulti-Tenant Cloud
verification by examining only parts of the virtual networks
- Network isolation may be compromised through ei- affected by a configuration change. Finally, TenantGuard
ther unintentional misconfigurations or malicious attacks leverages existing cloud policy services to check isolation
exploiting implementation flaws. For example, assume results againsttenant-specific high-levelsecurity policies. The
the current security policies of tenants Alice and Bob following summarizes our main contributions:
allow their VMs VM_A1 and VM_B2 to be reach-
- We proposeanefficientcloud-wideVM-levelverification
able from each other, as reflected by the two secu-
approach of network isolation with a practical delay for
rity group rules allow src 1.10.1.12 and allow
runtime applications (13 seconds for verifying 25,246
src 1.10.0.75. Now suppose Alice would like to
VMs and 168 millions of VM pairs, as detailed in
stopaccessestoherVMVM_A1,andthereforeshedeletes
Section VI).
theruleallow src 1.10.1.12andupdatesherhigh
level defined security policy accordingly.However,Alice - We devise a hierarchical model for virtual networks
is not aware of an OpenStack vulnerability OSSA 2015- along with a packet forwarding and filtering function to
021 [13], which causes such a security group change to capture various components of a virtual network (e.g.,
silently fail to be applied to the already running VM security groups, subnets, and virtual routers) and their
VM_A1. At the same time, a malicious user of tenant relationships.
Bob exploits another vulnerability OSSA 2014-008 [14]
by which OpenStack (Neutron) fails to perform proper - We design algorithms that leverage efficient data struc-
authorization checks, allowing the user to create a port tures,incrementalverification,andanopensourceparallel
on Alice’s virtual router R_A3 and subsequently bridges computation platform to reduce the verification delay.
that port to his own router R_B1. Consequently, Alice’s
- We implement and integrate our approach into Open-
VM, VM_A1,willremaintobe accessible byBob,which
Stack [12], a widely deployed open source cloud man-
is a breach of network isolation.
agementsystem.Weevaluatethescalabilityandefficiency
- Todetectpromptlysuchabreachofnetworkisolation,the ofourapproachbyconductingexperimentsbothin-house
challengeAlicefacesisagainthreefold.First,assumethe and on Amazon EC2.
cloud has 25,000 active VMs among which Alice owns
- We further integrate TenantGuard into Congress [17], an
2,000.SinceallthoseVMsmaypotentiallybethesource
OpenStack policycheckingservice, in orderto check the
ofabreach,andeachVMmayhavebothaprivateIPand
complianceofisolationresultsagainsttenants’predefined
adynamicallyallocatedpublicIP,Alicepotentiallyhasto
high-level security policies.
verify the isolation between 25,000×2,000×2 = 100
millions of VM pairs. Second, despite such a high com-
The remainder of this paper is organized as follows.
plexity, Alice wants to schedule the verification to be
Section II reviews the related work. Section III describes the
threat model and virtual network model. Section IV discusses
1Tomakeourdiscussionsmoreconcrete,theexampleswillmostlybebased
our system design and implementation. Section V provides
onOpenStack,andSectionVIIdiscussestheapplicability ofourapproachto
othercloudplatforms. details on TenantGuard’sintegration into OpenStack and Sec-
2tion VI gives experimental results. Section VII discusses the nodes. In order to cover all-pairs, the total number of queries
adaptability and integrity preservation. Section VIII discusses would grow significantly. This hinders the scalability of these
limitations,providesfuturedirections,andconcludesthepaper. approaches to tackle large cloud data centers. Furthermore,
most of these works consider routers/switches as the source
and destination nodes for their verification. NetPlumber and
II. RELATEDWORK
VeriFlow offer similar runtime performance. For a network
TableIsummarizesthecomparisonbetweenexistingworks of 52 nodes, Netplumber [19] checks all-pairs reachability
onnetworkreachabilityverificationandTenantGuard.Thefirst in 60 seconds, whereas a single-machine implementation of
column divides existing works into two categories based on TenantGuard takes only 4.6 seconds to verify a network of
the targetedenvironments,i.e., either cloud-basednetworksor 4,300 nodes (see Figure 10). Libra [23] uses a divide and
non-cloudnetworks.Thesecondandthirdcolumnslistexisting conquertechniquetoverifyforwardingtablesinlargenetworks
worksandindicatetheirverificationmethods,respectively.The forsubnet-levelreachabilityfailures.WhileLibrareliesonthe
next column compares those works to TenantGuard according assumption that rules in switches consist of prefixes aggre-
to various features, e.g., the support of parallel implementa- gating many subnets, we additionally deal with more specific
tion, incremental verification, NAT, and all pairs reachability rules(longerprefixes)byrunningthepreordertraversalonthe
verification (which is the main target of TenantGuard). The radix binary tries.
next two columns respectively compare the scope of those
Works designed for control plane verification in physical
works,i.e.,whethertheworkisdesignedforphysicalorvirtual
networkslikeARC[24],Batfish[25]andERA[26],ifapplied
networks, and whether it addresses control or data plane in
to the cloud, would face the difficulty that (unlike physical
such networks. Note that a L3 network is composed of a
networks) routing rules and ACLs for tenants’ private virtual
controlplaneforbuildingthenetworktypologyandtherouting
networks are not generated by the control plane.
tables based on various routing protocols (e.g., OSPF, BGP),
and a data plane for handling packets according to the built
Network Verification for Cloud Deployments. There are
routing tables. The last two columns show the size of input
several works (e.g., [27], [11], [31], [32], [33], [28]) ver-
andverificationtime,respectively,asreportedinthosepapers.
ifying the virtualized infrastructure in the cloud. Most of
In summary, TenantGuard mainly differs from the state- those solutions focus on verifying configuration correctness
of-the-art works as follows. First, TenantGuard performs ver- of virtualization infrastructures in terms of structural prop-
ification at a different granularity level (i.e., all-pair VM- erties (e.g., Cloud Radar [28]), which is different from the
levelvssingle-pairrouter-level).Second,TenantGuardismore properties targeted by TenantGuard. NoD [27], SecGuru [34]
scalable (e.g., verifying 100k VMs within 17mins). Finally, and their successor (Plotkin et al. [11]) are the closest works
TenantGuardemployscustomalgorithmsinsteadofrelyingon to TenantGuard, as they can check all-pairs reachability in
existing verification tools (e.g., [11], [27], [29], [28]), which physical networks for large cloud data centers. NoD is a
enables TenantGuard to more efficiently deal with complexity logic-based verification engine that has been designed for
factors specific to the cloud network infrastructure such as a checking reachability policies using Datalog definitions and
large numberof VMs, longer routing paths (numberof hops), queries. Plotkin et al. [11] improve the response time of NoD
and increased number of security rules. by exploiting the regularities existing in data centers lessen
the verificationoverheadusing bi-simulationand modallogic.
Non-Cloud Network Verification. In non-cloud networks, The experimental results reported in Section VI show that
several works (e.g., [30], [20], [18], [21], [19], [22], [23]) TenantGuard outperformsthose tools.
propose data plane analysis approaches, while others propose
Congress[35],isanopenprojectforOpenStackplatforms.
control plane analysis (e.g., [25], [24], [26]). Some existing
It enforces policies expressed by tenants and then monitors
works (e.g., [30], [20], [18]) address non-virtualized physical
the state of the cloud to check its compliance. However,
networks. Specifically, Xie et al. [30] propose an automated
reachability requires recursive Datalog queries [35], which
static reachabilityanalysis of physicalIP networksbased on a
are difficult to solve and are not supported by Congress.
graph model. Anteater [20] and Hassel [18] detect violations
Therefore, we integrated TenantGuard into Congress in order
of network invariants such as absent forwarding loops. While
to check network isolation results provided by TenantGuard
thoseworksaresuccessfulforverifyingenterpriseandcampus
against tenants’ security policies defined in Congress(Section
networks,theycannotaddresschallengesoflargescale virtual
V). Additionally, by integrating TenantGuard to Congress,
networks deployed in the cloud with hundreds of thousands
we augmented Congress capabilities to support reachability-
of nodes. For instance, Hassel [18] needs 151 seconds to
related policies as NoD without modifying Datalog-based
compressforwardingtablesbeforespendingan additional560
policy language provided by Congress.
seconds in verifying loop-absence for a topology with 26
nodes.
III. MODELS
Other works (e.g., [21], [19], [22]) propose approaches
In this section, we describe the threat model and propose
for virtualized networks. VeriFlow [21], NetPlumber [19]
a hierarchical model for cloud virtual networks.
(extension of [18]), and AP verifier [22] outperform previ-
ous works by proposing a near real-time verification, where
A. Threat Model
network events are monitored for configuration changes, and
verification is performed only on the impacted part of the Our threat modelis based on two facts. First, our auditing
network.Thoseworksproposequery-basednetworkinvariants solution focuses on verifying the security properties speci-
verification between a specific pair of source and destination fied by cloud tenants, instead of detecting specific attacks
3Physicalvs Controlvs
Features Sizeofinput Verif.
Network Proposals Methods Virtualnet. Dataplane
Time
AllPairs
Paral. Incr. NAT Phys. Virt. Ctr. Data VMs Routers Rules
Reach.
Hassel[18] Customalgorithms • • • - 26 756.5k -
NetPlumber[19] Graph-theoretic • • • • • • - 52 143k 60
Anteater[20] SATsolver • • • • - 178 1,627 -
Veriflow[21] Graph-theoretic • • • - 172 5,000k -
APverifier[22] Customalgorithms • • • • - 58 3,605 -
Non-
Libra[23] Graph-theoretic • • • • • - 11,260 2,650k -
Cloud
ARC[24] Graph-theoretic • • - fewtens - -
Batfish[25] SMTSolver • • • - 21 - 86,400
ERA[26] CustomAlgorithms • • - over1,600 - -
NoD[27] SMTSolver • • • • 100k - 820k 471,600
Cloud Plotkinetal.[11] SMTSolver • • • • 100k - 820k 7,200
CloudRadar[28] Graph-theoretic • 30k - - -
Probstetal.[29] Graph-theoretic • • • 23 - - -
TenantGuard Customalgorithms • • • • • • 100k 1,200 850k 1,055.88
TABLEI. COMPARINGFEATURESANDPERFORMANCEOFDIFFERENTEXISTINGSOLUTIONSWITHTENANTGUARD.THESYMBOL(•)INDICATESTHAT
THEPROPOSALOFFERSTHECORRESPONDINGFEATURE.ALLVERIFICATIONTIMEMEASUREMENTSAREREPORTEDINSECONDS.
or vulnerabilities (which is the responsibility of IDSes or networks(notshowninthefigure)mayexist,whereeach(e.g.,
vulnerabilityscanners).Second,thecorrectnessofourauditing ExtNet_1)canhavearoutablepublicIPaddressblock(e.g.,
results depends on correct input data extracted from logs and 1.10.0.0/22). For inter-tenant traffic, at least one router
databases.Sincean attackmayormaynotviolatethesecurity from each tenant must be involved and the traffic generally
properties specified by the tenant, and logs or databases may traverses external networks. For any communication going
potentially be tamperedwith by attackers, our auditing results throughexternalnetworks,apublicIPaddressis allocatedper
can only signal an attack in some cases. Specifically, the VM(e.g.,VP_A1.Public_IP=1.10.0.75)dependingon
in-scope threats of our solution are attacks that violate the which external network (e.g., ExtNet_1) connects to the
specified security properties and at the same time lead to subnetoftheVM(e.g.,SN_A2).Themappingbetweenprivate
logged events. The out-of-scope threats include attacks that and public IP addresses is maintained through NAT rules at
do not violate the specified security properties, attacks not routers.
captured in the logs or databases, and attacks through which Ext. Net
1.10.0.0/22
the attackers may remove or tamper with their own logged
events. We assume each cloud tenant has defined its own IP: 1.10.0.2 IP: 1.10.0.3
RG_A1 RG_B1
security policies on network isolation in terms of reachability
between VMs. We focus on the virtual network layer (layer RN ouA tiT n gr u rl ue ls e s R_A2 R_A1 R_A3 R_B1
3) in this paper, and our work is complementary to existing
solutionsatotherlayers(e.g.,verificationinphysicalnetworks IF_A21 IF_A22 IF_A11 IF_A12 IF_A31 IF_B11 IF_B12
or isolation w.r.t. to covert channels caused by co-residency; Host routes
SN_A1 SN_A2 SN_A3 SN_B1 SN_B2
more details are given in Section II). Finally, we assume the 10.0.2.0/24 10.0.0.0/24 10.0.1.0/24 19.0.1.0/24 19.0.0.0/24
verificationresults (e.g., which VMs may connectto a tenant)
Security groupsVP_A1 VP_A2 VP_B1 VP_B2
do not disclose sensitive information about other tenants and
Private IP:10.0.0.12 Private IP:10.0.1.22 Private IP:19.0.1.15 Private IP:19.0.0.30
regard potential privacy issues as a future work. Public IP:1.10.0.75 Public IP:1.10.1.9 Public IP:1.10.0.8 Public IP:1.10.1.12
Tenant Alice Tenant Bob
External
B. Virtual Network Model Network Router Subnet Edge
Router Router
Here, we define a hierarchical model to capture various Gateway Interface VM port Forwarding path
componentsofavirtualnetworkandtheirlogicalrelationships.
Fig.2. AnExampleoftheVirtualNetworkModel
The following example provides intuitions on the model we
More generally, Figure 2 may be abstracted as an undi-
propose.
rected graph with typed nodes, as defined in the following.
Example 1: Figure 2 illustrates an instance of our model
Definition 1: A virtual network model is an undirected
that captures the virtual networks of tenants Alice and Bob,
graph G = (V,E), where V is a set of typed nodes
following our example shown in Figure 1. Each tenant can
each of which is associated with a set of attributes s =
create several subnets (e.g., SN_A1 and SN_A2 of Alice). A
{id,tenant id,Public IP,Private IP,type,rules}, where
subnet(e.g.,SN_A2)isgenerallyassociatedwithaCIDR(e.g.,
type∈{vm port,subnet,v router,v router if,v router
10.0.0.0/24) and a set of forwarding rules (host routes)
gw ,ext net}, representing VM port, subnet, router, router
specifyingthedefaultgateway(e.g.,routerinterfaceIF_A11).
interface, router gateway, and external network, respectively.
A newly created VM (e.g., VM_A1, not shown here) will be
E is a set of undirected edges representing the logical
attached to a virtual port (e.g., VP_A1) on a subnet (e.g.,
connectivity among those network components.
SN_A2) and associated with a private IP (e.g., 10.0.0.12).
Ingress and egress security groups are associated with the A virtual network model G can usually be decomposed
virtual ports of VMs and act as virtual firewalls. Routers into a set of maximally connected sub-graphs [36] (denoted
(e.g.,R_A1)interconnectdifferentsubnetstorouteintra-tenant by C = (V ,E ) in later discussions) by removing all
i i i
(e.g., between SN_A2 and SN_A3) and inter-tenant traffic edges between router gateways and external networks. Those
and connect them to external networks (e.g., ExtNet_1) via subgraphsrepresentdifferenttenants’privatevirtualnetworks,
routergateways(e.g.,RG_A1).Severalinterconnectedexternal which are connected to external networks via the removed
4edges. We will leverage this characteristic later in Section IV - A forwarding path for packet p from node u to node
to tackle the complexity issues. v is a sequence of forwarding states (p,(null,u))···
(p′,(v,null)).
C. Forwarding and Filtering Model As a convention, we will use null in forwarding states to
denotea forwardingstate where the symbolicpackethasbeen
In the following,we first model how packets may traverse dropped (e.g., (null,(w,null))), a packet initially placed on
a virtual network, and then formalize the network isolation a node v (e.g., (p,(null,v))), or a packet received by w after
property that we aim to verify. the last hop (e.g., (p,(w,null))).
ForwardingandFiltering.Networkpacketstraversingvirtual Network Isolation. With the virtual network model and for-
networks are typically governed by both filtering (security warding and filtering function just defined, we can formally
group rules) and forwarding (routing) rules, as demonstrated model network isolation and related properties as follows.
in the following example.
Definition 3: Given a virtual network model G=(V,E),
Example 2: Figure 2 shows a dotted line representing the
- for any u,v ∈ V, we say u and v are reachable if there
sequence of edges traversed by a set of packets from VM_A1
exists a packet p ∈ P and a forwarding path for p from
to VM_B2, which represents the forwarding path controlled
u to v. Otherwise, we say u and v are isolated.
by different nodes between both corresponding virtual ports.
Packets sent to the virtual port (e.g., VP_A1) are processed - A forwarding loop exists between u ∈ V and v ∈ V if
by the egress security group rules then either dropped or there exists p ∈ P destined to v and w,w′ ∈ V such
forwarded to the subnet node SN_A2. According to host that (p,(w,w′)) is a reachable forwarding state and that
routes associated with SN_A2 and the destination address fd ((p,(w,w′)))=(p,(w′,w)).
G
(i.e., VM_B2.Public_IP), packets are either dropped or
forwardedto the defaultgateway,whichis the routerinterface - A blackhole exists between u ∈ V and v ∈ V if
IF_A11 of the router R_A1. At the router node, packets’ there exists p ∈ P destined to v and w,w′ ∈ V
headersare matchedwith the routingrules, and are either for-
such that (p,(w,w′)) is a reachable forwardingstate and
warded to RG_A1 and then to the associated externalnetwork fd G((p,(w,w′)))=(null,(w′,null)).
ExtNet_1, or dropped. Packets destined to VM_B2 are then
The properties given in Definition 3 can serve as the
forwarded by ExtNet_1 to the router RG_B1. If matching
building blocks of any network isolation policies specified
forwardingrules for these packets are foundat nodesRG_B1,
by a cloud tenant. The specific forms in which such security
R_B1andSN_B2,thentheedgesbetweenRG_B1andVP_B2
policies are given are not important, as long as such policies
are traversed. At VP_B2, only packets matching the ingress
can unambiguously determine whether two nodes should be
security group rules are forwarded to their destination. Note
reachable or isolated. Therefore, our main goal in verifying
thatatthelevelofR_A1(resp.R_B1),packetsaretransformed
a tenant’s security policies regarding network isolation is to
using NAT rules by replacing the source (resp. destination)
ensureanytwonodesarereachable(resp.isolated)ifandonly
private (resp. public) IP of VM_A1 (resp. VM_B2) with the
ifthisisspecifiedinsuchpolicies.Inaddition,ourverification
corresponding public (resp. private) IP.
algorithmsintroducedinSectionIVcanalsoidentifyforward-
More generally, the following definition models the way ing loops and blackholes as anomalies in virtual networks.
packets traverse virtual networks using a forwarding and
filtering function capturing respectively routing and security IV. TENANTGUARD DESIGN AND IMPLEMENTATION
group rules.
In this section, we first provide an overview of our ap-
Definition 2: Forwarding and Filtering Function.Given proach and then introducethe data structuresand the verifica-
a virtual network model G=(V,E), tion algorthims in details.
- let p ∈ P be a symbolic packet (similarly as in [37]) A. Overview
consisting of a set of header fields (e.g., source and
Due to the sheer size of a cloud, verifyingseparately each
destinationIPs)andtheircorrespondingvaluesin{0,1}L
pair of VMs (query-basedapproach)or directly computingall
such that L is the length of the field’s value, and
possible forwarding paths for all pairs of VMs (henceforth
called thebaselinealgorithm)wouldresultin an unacceptable
- let(p,(u,v))beaforwardingstatewhere(u,v)isthepair
response time, and not scale to large clouds, as will be
of nodes in G representing respectively the previous hop
demonstratedthroughexperimentsinSectionVI.Also,theuse
node(i.e.,thesendernode)andthecurrentnode(i.e.,the
of(possiblyoverlapping)privateIPsanddynamicallyallocated
node v where the packet is located in the current state).
publicIPsinthecloudcanmakethingsevenworse. Toaddress
- The forwarding and filtering function fd returns the those issues, TenantGuard leverages the hierarchical virtual
G
successor forwarding states {(p′ i,(v,w i))} i∈N, such that network model presented in Section III-B by partitioning the
each w ∈V is a receiving node according to the results verification task into a prefix-level verification followed by a
i
of rules matching at node v, and p′ is the symbolic VM-levelverification.Prefix-levelverificationsplitsfurtherthe
i
packet resulting from a set of transformations (e.g., virtual networks into a set of private IP prefixes (i.e., tenants’
NAT) over packet p before being forwarded to w where subnets) and a set of public IP prefixes (i.e., external network
i
{v,w i} ∀i∈N ∈E. IPprefixes),whichresultsinathree-stepapproach,asitwillbe
5Step2.a Ext. Net Step2.b
Host Routes Security 1.10.0.0/22
Policies
Routers Tables
RG_A1 RG_B1
Networking Subnets TenantGuard Com Vp &li Vance Step1
service External R_A1 R_A3 R_B1
networks
Compute
service Ingress/Egress Audit IF_A11 IF_A12 IF_A31 IF_B12
Security Groups Report
SN_A2 SN_A3 SN_B2
Data Collection & Compliance Verification 10.0.0.0/24 10.0.1.0/24 19.0.0.0/24
Preparation & Reporting
Step3
Fig.3. AnOverview ofTenantGuard VP_A1 VP_A2 VP_B2
Yes (Step1) Tenant Alice Tenant Bob
Private IP Prefix-
level verification Fig.5. ExampleApplication ofOurThree-StepVerification Approach
Same
<sn-src, sn-dest> Component?
B. Data Models
(Step2)
No lP eu vb el li c v eI rP i fP icr ae tf ii ox n- Reachable? In order to further improve the scalability and response-
<VM-src, VM-dst>: (Step3) Yes time of our approach, we investigated prefix matching and
not isolated No Isolated ? paV irM s - vL ee riv fe icl aa tl il o- n No packet classification literature. According to our findings, we
found out that both X-fast binary tries [16] and radix binary
Yes
<VM-src, VM-dst>: Subnets: Not tries [15] fit our purpose. Different type of trie structures
isolated reachable have been used in prior works e.g., VeriFlow [21]. Indeed,
Fig.4. AFlowChartIllustrating OurThree-Step Approach X-fast binary tries not only allow efficiently storing of all IP
addresses with their prefix relationships but also provide fast
detailedlater.Furthermore,weuseefficientdatastructuresthat
insertionandsearchingoperations.Furthermore,radixtriesare
allow handlingall-pair verificationat once instead of a query-
efficiently used to store routing and filtering rules as well
based approach.As we will confirm with experimentalresults
as efficiently matching them against packet-headers. In the
in Section VI, those conceptual advances allow to scale to
following, we show how we use them in our approach.
cloud-wide,VM-levelverificationofnetworkisolation.Figure
3providesanoverviewoftheTenantGuardsystem.Inputdata 1)RoutingandSecurity Groups: We employradix tries to
from the cloud infrastructure management system, including store routing and firewall rules and then to perform efficient
routerrules,hostroutes,andsecuritygroups,arecollectedand rulematchingagainstIPprefixes.Weusevariablesforlabeling
processedusingefficientdatastructuresasitwouldbedetailed nodes to store information about the rules and their order.
in Section IV-B. The preservation of collected data integrity
Rules in Router R_A1 NH:null
is discussed in Section III-A. Once the verification results 0
Rule Prefix Next-Hop
are returned, compliance verification compares such results NH:null
with the tenant’s pre-defined security policies. Finally, the r0 10.0.1.0/24 IF_A12 1 Longest
NH:null prefix match
correspondingauditingreportisgeneratedandpresentedtothe r1 1.10.0.0/22 RG_A1 ….
tenant.Figure4providesahigh-levelflowgraphcorresponding 0
r2 1.10.0.0/24 IF_A22 NH: null
to our three-step approach. Each element of the graph will be 0
r3 1.10.1.0/28 IF_A31
detailed in Section IV-C. NH: RG_A1
0 ….
Range 2: r3
To grasp the intuition behind our three-step verification NH:null 1
approach, we present an example. Range 1: r2 Range 3: r1 0 NH: null Preorder
NH: IF_A22 …. traversal
1
Example 3: Figure 5 illustrates the application of our
NH: IF_A31
1.10.0.0 1.10.3.255
three-step verification approach using our running example
shown in Figure 2. In Step 1 (ref. Section IV-C1), prefix- Fig.6. AnIllustrativeRoutingTableinRouterR A1andanExcerptofthe
level isolation verification within the same components/sub- Corresponding RadixTrie
graphusingprivateIPisperformed.Forinstance,theisolation Example 4: Figure 6 illustrates an example of a radix
betweenAlice’ssubnetsSN_A2andSN_A3throughtherouter trie (right-side) for an excerpt of the routing rules of router
R_A1 is verified using their respective private IP prefixes R_A1(upperleft-side)withthedifferentIPranges(lowerleft-
(e.g., 10.0.0.0/24and 10.0.1.0/24).In Step 2 (ref. side) resulting from matching all rules with the IP prefix of
Section IV-C1), prefix-levelisolation verification between dif- ExtNet_1 (i.e., 1.10.0.0/22). Edges of the radix trie are la-
ferentcomponents(e.g.,SN_A2andSN_B2)isperformedvia beledwith binaryvaluesandnodesstoredifferentinformation
each adjacent external network (e.g., ExtNet_1). This step relevanttomatchingthebit-stringsformedbyconcatenatingall
is further decomposedinto Step 2.a for verifying isolation labelsofupstreamedgesstartingfromtherootnode[15].The
between the source subnet (e.g., SN_A2) and the external matchingconsistsintransformingtheIPprefixintoabitstring
network, and Step 2.b for verifying isolation between the (i.e., 0000.0001.0000.1010.00)and using it as a key search to
external network and the destination subnet (e.g., SN_B2). findthecorrespondingnode.Thenode’svariableNHstoresthe
This verification also involves public and private IP NAT. matching rule’s next hop; for firewall rules (not shown in this
Finally, Step 3 (ref. Section IV-C2) performs VM-level example), we use two variables, namely VAL for decision of
security groups verification for any pair of subnets found to the matching rule (i.e., accept/deny), and SN for rules’ order.
be reachable using Step 1 and Step 2. In case of absence of a matching rule, those values are set to
6null.Forinstance,thematchednodeislabeledNH=RG A1, means that the verification is still ongoing for Range
which correspondsto the next hop specified by rule r1 in the 1 and next hop should be evaluated based on the next
routingtable andit representsthe longest-prefixmatchedrule. variable HR.
For routing rules matching with an IP prefix, the common - Variable HR is a sequence of triplets (r id,r if,src)
algorithmusedbyroutersformatchingasinglepacket,namely, thatstoresthehistoryofthevisitednodesfromsourcefor
the longest-prefix match [38], would not be sufficient. There- that IP range, where r id is a router id, r if is a router
fore, we only apply the longest-prefix match algorithm to any interface and src is the original source node. The last
rulethatmatchesthe destinationprefix.Then,we applya pre- result is appended to the beginning of the sequence and
order traversal of the sub-trie starting from the node storing shouldbe usedatthe nextiteration.Formorereadability,
the longest-prefix matching rule. The rationale is that other in Figure 7, we only show the two first items of the
more specific prefixes stored deeper in the radix trie (e.g., triplet from the last outcome (next hop) of routing rules
for a specific address range) will be needed for a consistent matching in R_A1.
matching result, which would result in splitting the matched
1.10.0.0/22
IP prefix into ranges, where each range is governed by the 0 1
appropriate rule, as it will be demonstrated in the following.
Example 5: AsdepictedinFigure6,onceruler1 isfound 0 1
000000000X 111111111X using the longest prefix match algorithm for the IP prefix
1.10.0.0/22, the preorder traversal algorithm is applied on the
sub-triefromthenodematchingwithr1.Thus,rulesr2andr3
are also foundto match 1.10.0.0/22.Considering all matching
rules, the destination prefix IP is split into three ranges,
namely,range1: 1.10.0.0···1.10.0.255,range 2:1.10.1.0···
1.10.1.15, and range 3: 1.10.2.0 ··· 1.10.3.255, respectively
governed by r2, r3 and r1.
Note that for matching rules in security groups, we will
use the first-match algorithm [39].
2)Prefix-to-PrefixVerification Results Processing: The X-
fast binary tries [16] are used (Algorithm 1 in Section IV-C1)
to store and progressively compute verification results, per
hop, in order to assess isolation between two IP prefixes (see
Figure 7). An X-fast trie (denoted by BTries) is a binary
tree, where each node, including the root, is labeled with the
common prefix of the corresponding destination sub-tree. As
in radix tries, the left child specifies a 0 bit at the end of the
prefix,whilethe rightchildspecifiesa bit-value1. Eachnode,
including leaves, is labeled with the bit-string from the root
to that leaf. We use the leaves to store intermediate and final
results as explained in this example. The binary trie’s leaves
are created and modified progressively by the prefix-to-prefix
Algorithm 1.
Example 6: Figure7illustratesanexampleofintermediate
valuesofaBTriesbuiltforsourcesubnetSN_A2anddestina-
tionExtNet_1.Leavesstoretheresultsofmatchingtheradix
trie of Figure 6 with destination IP prefix 1.10.0.0/22, which
is actually the root of the X-fast binary trie. Three variables
are used at the leaf nodes:
- Variable B stores the boundaryof the IP ranges for each
leaf. Its value is either L for the lowestbound,H forthe
highest bound, or LH if a single leaf with a specific IP
address(e.g.,1.10.0.2/32).Theleftmostleaf inFigure7,
B is set to L, which means the currentleaf is the lowest
bound of the IP range Range 1. The next leaf, B is set
to H to delimit the upper bound of Range 1.
- Variable RLB is a two-bit flag that indicates the status
of the verification process, where possible values are 00
for no decision yet, 01 for loop found, 10 for blackhole
found,or 11 for reachabilityverified. In the leftmostleaf
of the binary trie of Figure 7, RLB is set to 00, which
(cid:1) (cid:1)
0 1
0000000000, 0011111110, 0100000000, 0100001110,
L,00, <R_A2, H,00, <R_A2, L,00, <R_A3, H,00, <R_A3,
IF_A22,SN_A2> IF_A22,SN_A2> IF_A31,SN_A2> IF_A31,SN_A2>
(cid:1) (cid:1)
0 1
(cid:1) (cid:1)
0 1
0100001111, 1111111111,
L,11, <R_A1, H,11, <R_A1,
RG_A1,SN_A2> RG_A1,SN_A2>
(cid:1) (cid:1)
0XXXXXXXXX 1XXXXXXXXX
Fig.7. X-fastBinaryTrieforSubnetSN A2andDestination1.10.0.0/22.
Leaves Contain Results from Matching Radix Trie in Figure 6 with the
Destination
C. Verification
In this section, we present our customized algorithms
to perform the three verification steps. The reason that we
opt for customized algorithms, instead of existing large-scale
graph processing systems (e.g., Pregel [40], BGL [41], and
CGMgraph[42])isthatthosearemostlydesignedforgeneral-
purpose graph algorithms like finding shortest-path. None of
them can easily supportnetworkisolationuse cases addressed
in this paper, in particular, path modifications caused by
decision making along the path (e.g. routing, firewalling), or
the path transformational operations (e.g., NAT).
Before starting the actual verification, X-fast binary tries
are created and initialized for each pair of source and des-
tination IP prefixes using the virtual network model G as it
was explained in Section IV-B2. Also, as it was mentioned
earlier, both Step 1 and Step 2 are parts of the prefix-
level verification, where the first step is applied on private
IP addresses while the second takes care of the public IP
addresses. As a result, we will have BTries for pairs of
private IP prefixesof subnetsin the same component(verified
inStep 1)andotherBTriesforpairsIPprefixesofsubnets
and externalnetworks(verifiedin Step 2.a) andvice-versa
(verified in Step 2.b). These two steps will be explained
in Section IV-C1. Afterward, VM-level isolation verification
takesplaceatStep 3,detailsofwhicharein SectionIV-C2.
1)Prefix-Level Verification: The function prefix-to-prefix
(see Algorithm 1) uses the initialized X-fast binary tries btrie
toverifyprefix-levelisolationoneachhopbetweenallpairsof
sourceanddestinationIPprefixes.Foragivenpairofprefixes,
the prefix-to-prefixverifiesroutingruleson a per-hopbasis. In
all hops between a given pair of prefixes, it uses the same
corresponding X-fast binary trie (i.e., having one prefix as
sourcespecifiedinleavesandtheotherasdestinationspecified
in the root of the trie) to update the new results according
to the results of matching the rules within the node’s radix
trie against each IP range. The core of this algorithm is the
matching process (explained in Section IV-B1) and copying
7these results from a temporary trie to the btrie. The latter is Algorithm 1 prefix-to-prefix(btrie)
explained better using the following example. 1: Input/Output:btrie
2: counter=0
Example 7: Figure 8 illustrates the process of copying 3: foreachrange[L,H]inbtrie.leafswithRLB=00do
the leaves from the temporary binary trie, which contain 4: router=get(HR,r id)
the outcome of matching R0 rules with the destination IP
5: dst=getroot(btrie)
6: ifsearchTries(dst,router)=falsethen
prefix, to the the main prefix-to-prefix binary trie within the 7: TempBTrie=Match(RadixTrie(router),dst)
appropriateIPrange(Algorithm1line10).Figure8illustrates 8: else
9: TempBTrie=getBTrie(dst,router)
the modified binary trie after applying a hop per address
10: Copy(btrie,TempBTrie,[L,H])
range verification on the trie of Figure 7 with an excerpt 11: counter=counter+1
of the rules of R_A3 (left-side) and R_A2 (right-side). We 12: ifcounter6=0then
13: prefix-to-prefix(btrie)
compute only the decisions of those routers that are related
to Range 1 andRange 2. Aftermatchingthese tableswith
the destination address, new leaves are created (e.g., Range Algorithm 2 VM-to-VM(VM src,VM dest)
2 is split into Range 21 and Range 22) with new results, 1: Triepub=getBTrie(VMdst.publicIP.CIDR,VMsrc.subnet id)
while for others (i.e., Range 1) only the result is updated in 2: Triepriv=getBTrie(VMdst.privateIP.CIDR,router id)
3: routable=Route-Lookup(Triepub,Triepriv)
the binary trie, as follows:
4: ifroutable=truethen
5: VerifySecGroups(VMsrc,VMdest)
- At R_A2, no routing rule was matched, thus indicating a
black hole (RLB is 10) for range 1.
- At R_A3, matching the destination prefix with the corre- relevant binary tries leaves using the IP addresses of these
sponding rules results in two matching rules (i.e., r31 VMs. This will determine the leaves with boundaries H and
and r32), which partitions Range 2 into two sub- L corresponding to the IP ranges containing VMs’ IPs and
ranges. Range 21 is handled by r31, which leads to verifying the value of the flag RLB. This is explained in the
a loop (RLB = 01) that can be detected by consulting following example.
the variable HR. Packets belonging to Range 22 are
Example 8: ConsiderthecaseofVM_A1andVM_B2from
handled by rule r32 and they can reach the router
ourrunningexampleshowninFigure2.Routelookupforthis
gateway R_A3 (i.e., RLB =11).
pair is achieved by searching for the two X-fast binary tries
Algorithm 1 takes as input the binary trie identifier then denotedbyTriepubandTriepriv, respectively,in Algorithm
updates the trie progressively by creating new leaves and 1. The Triepuband Triepriv triescontain the routingresults
modifying others using per-hop results. At each iteration, it respectively,forthepair(SN_A2,ext_net)and(ext_net,
traverses the leaves of the trie and, for each IP range, it SN_B2).UsingthepublicIPofVM_B2(i.e.,1.10.1.12,which
matchestheradixtriecorrespondingtothenetworkingelement iswithintheprefixofExtNet 1),andtheprivateIPofVM_A1
specified for that range with the destination IP prefix using (i.e., 10.0.0.12, which is within the prefix of subnet SN A2),
algorithms in Section IV-B1. The algorithm terminates if a the corresponding binary trie Triepub is shown in Figure 8.
loop or a blackhole is found, or reachability is verified for all By searchingthe Triepub(see Figure8)usingthe publicIPof
ranges. It uses a temporary trie TempBTrie, which contains VM_B2, one can find that it falls into Range 22. The value
the result of matching the radix trie of the current router with of RLB for this range is 11, which indicates the existence
thedestinationIPprefixlocatedattherootofthebinarytrieas of a route from SN A2 to ExtNet 1. Similarly, TriePriv
discussed in Section IV-B1. This temporary trie is generated can be identified using the public IP of VM_B2, which is
once, but can be re-used, particularly, for the verification attached to router R_B1, and its private IP (i.e., 19.0.0.30).
of other IP prefixes as source (e.g., SN_A2 and SN_A3 in Searching in Triepriv (not shown for the lack of space) for
Figure 3) with the same destination (e.g., ExtNet_1) and theRLB forusingtheprivateIPofVM_B2allowsconcluding
the same router (e.g., R_A1). Function searchTries finds, if ontheexistenceofaroutebetweenVM_A1andVM_B2.More
any, the temporary trie corresponding to the specific router precisely, if RLB in these boundary leaves of both Triepriv
and destinationIPrange.FunctionCopy is used to updatethe and Triepub is equal to 11, we say that a forwarding path
main binary trie btrie with the results stored in the temporary exists between these VMs.
binary trie TempBTrie for each specific range as discussed
At this stage, once a path is found between the subnets of
in Example 7.
thepairofVMs,wethenverifybothsecuritygroupsassociated
2)VM-Level Isolation Verification: Prefix-level results with these VMs. According to the type of communication,
computedin SectionIV-C1 are used to determinesubnetsthat either private or public IP will be used. For each VM within
arenotisolated.Forthosesubnets,weneedtoperformaVM- a source subnet, we use its egress security group radix trie
level isolation verification by checking for each pair of VMs and perform a first-match with the public or private IP of
their corresponding security groups using both private and the destination VM. Then, we use the ingress security group
publicIPaddresses.Algorithm2describestheVM-to-VMpro- rulesofthedestinationVMandperformafirst-matchwiththe
cedureinwhichfunctionRoute-Lookupcheckswhetherthere public or private IP of the VM source. If both results indicate
exists a forwarding path between any two VM ports, whereas matchingruleswiththeacceptdecisions,thenthepairofVMs
the VerifySecGroups function verifies security groups of can be concludedto be reachable using their public or private
these VMs. IP addresses.
The VM-to-VM route lookup is to determine whether 3)ComplexityAnalysis: LetS bethenumberofsubnets,R
these VMs belong to reachable subnets by searching in the be the number of routers between two prefixes (i.e., number
8Rules in Router R_A3 1.10.0.0/22 Rules in Router R_A2
Rule Prefix Next-Hop 0 1 Rule Prefix Next-Hop
r31 1.10.1.0/28 IF_A12 0XXXXXXXXX 1XXXXXXXXX r21 10.0.0.0/24 IF_A21
0 1 0 1
r32 1.10.1.0/30 RG_A3
… … … … … …
0 1
000000000X 111111111X
… …
0 1
0000000000, 0011111110, 0100000000, 0100000011, 0100000100, 0100001111, 0100010000, 1111111111,
L,10, R_A2, H,10, R_A2, L,01, R_A1, L,01, R_A1, L,11, R_A3, H,11, R_A3, L,11,R_A1, H,11, R_A1,
IF_A22 IF_A22 IF_A12 IF_A12 RG_A3 RG_A3 RG_A1 RG_A1
Fig.8. UpdatedBinaryTrieofFigure7BasedonMatchedRules inRouters R A2andR A3
of hops), L be the length of keys (whose maximum value Referring to Figure 4, Step 1 and Step 2 explore disjoint
is 32 for an exact IP address), M be the number of VMs, prefix-level IP address spaces (private addresses spaces vs
and Nex be the number of external networks. Complexities public addresses space). Thus, the two steps do not have
related to the data structure manipulation are known to be any side effects on one another. The results of these two
O(L) for insert operation in X-fast binary tries, O(Log(L)) steps are the pairs of subnets that can reach each other (R)
forsearchoperationsinX-fastbinarytries,andO(L)forradix and those that cannot (U). As we use well-known packet
trie matching per router. header matching algorithms to find reachable paths, the sets
U and R should contain the correctpairs with respect to their
In Step 1 and Step 2, the complexity of prefix-to-
reachability status. The third step relies on the results of Step
prefix reachability verification (Algorithm 1) is O((S2+2×
1 and Step 2 and verifies security groupsfor all pairs of VMs
S×Nex)×R×K×(L+log(L))), where K represents the
belonging only to the set of pairs of reachable subnets in R.
number of operations performed over the data structures for
In this step as well, we rely on state-of-the-art first-match
eachroutingnode.ThiscanbeapproximatedtoO(S2)forlarge
algorithm applied on firewall rules at each VM-side against
data centers where the number of subnets is larger than the
the header of the symbolic packet. Therefore, the correctness
number of external networks (Nex ≪ S) and the number of
of our approach follows from the correctness of those well-
hops is usually limited for delay optimization (R≪S), with
established algorithms in a straightforward manner.
LandK beingconstants.InStep 3,thecomplexityofVM-
level verification (Algorithm 2) is O(2∗(L+Log(L))∗M2) 5)Incremental Verification: The dynamic nature of cloud
and can be approximated to O(M2). leads to frequent changes in the configurations of virtual
networks.Theverificationresultmaybeinvalidatedevenafter
We thus obtain an overall complexity of O(S2 + M2).
a single change, such as the deletion of a security group
However, this only provides a theoretical upper bound, which
rule from a group of VMs, or the addition of a routing
typicallywillnotbereachedinpractice.Ingeneral,depending
rule to a router. However, verifying the cloud-wide network
on the communication patterns in multi-tenant clouds, the
isolation again after each such event is obviously costly and
numberof interconnectedsubnetsis usually smaller than S as
unnecessary.
traffic isolation is the predominant required property in such
environments. For example, it has been reported in [43] that To cope with the effect of each event at run-time, Ten-
inter-tenanttrafficvariesbetween10%and35%only.Thus,if antGuard adopts an incremental event-driven verification ap-
we denote by M′ the numberof VMs belongingto connected proach. This approach first identifies the set of events that
subnets, it is safe to claim the practical complexity for our potentially impact the isolation results. Then, the impact of
solution would be O(S2+ M′2), where M′ ≪M. each such event is identified. Finally, only those parts of the
verification that are affected by the eventwill be re-evaluated.
4)Correctness: According to our model, verifying isola-
Table II lists an excerpt of events that may require updating
tion means checkingwhetherthere exists any layer 3 commu-
nication path between any pair of VMs according to tenants’ the verification results along with their impact. Note that G
policies. Therefore, proving the correctness of our approach
shouldbe updatedfor allthese eventsand the symbol∗ in the
table indicates the network elements impacted by the event.
boils down to proving that our algorithm visits all paths
and returns the desired isolation result for each of them. To illustrate how such events may be handled via incre-
In a typical cloud environment at network virtual layer 3, mentalverification,Algorithm3sketchesthestepsforpartially
private IP addresses are used for communications inside the updatingtheverificationresultsupondeletingasecuritygroup
same network (component), whereas public addresses are rule and upon adding a new routing rule, respectively, as
used forcommunicationsbetweenVMs belongingto different explainedinthefollowing(detailedalgorithmsforotherevents
networks, and with networks outside the cloud. As we are are omitted due to space limitations).
considering both private and public IP addresses (with NAT
mechanism) to investigate the whole symbolic IP packet ad- - Creating a VM: The creation of a new VM (denoted
dress space, our approach explores all IP forwarding paths as VM*) does not affect the verification process unless
by iteratively applying, on each path, relevant forwarding and it gets connected to one or more subnets through virtual
filtering functions (using corresponding matching algorithms) ports, which naturally leads to the update of our graph
of each encountered node in the virtual network connectivity model by creating the corresponding virtual port nodes.
graph for each packet. Furthermore,whentheVMisfirstcreated,itisattachedto
9Event VerificationTasks
CreatingaVM* • InvokeVM-to-VMforVM*onceassourceandonceasdestination
DeletingaVM* • RemovetheresultsrelatedtoVM-to-VMforVM*
• Initializearadixtrieforthehostroutes
• Createnewprefix-to-prefixbinarytrieswhereSN*issourceordestination
CreatingasubnetSN*
• Invokeprefix-to-prefixforsubnetsintheC*relatedtoSN*
• Invokeprefix-to-prefix(Step2.aandStep2.b)forSN*
• Deletetheprefix-to-prefixtrieswhereSN*issourceordestination
DeletingasubnetSN*
• UpdateVM-levelisolationforallVMshavingprivateIPswithintheprefixofSN*eitherassourceordestination
CreatingarouterR* • Initializethecorrespondingradixtrie
• Recalculatealltheprefix-to-prefixtriesforthecomponentC*relatedtoR*
DeletingarouterR*
• PartiallyperformVM-levelisolationforallVMsbelongingtoC*consideredassourceandasdestination
TABLEII. ANEXCERPTOFEVENTSANDTHEIRCORRESPONDINGINCREMENTALVERIFICATIONTASKS.THESYMBOL*INDICATESTHENETWORK
ELEMENTSTHATAREIMPACTEDBYTHEEVENT
thedefaultsecuritygroup.Atthislevel,theresultsofstep updatedforallpairswhereVM*appearsasasourceVM
1 and step 2 of our methodology remain unchanged and (resp. destination VM).
the verification update is confined to step 3 by invoking
- Adding a routing rule: Whenever a new routing rule
the function VM-to-VM.
is added to a router, denoted as R*, belonging to a
- Deleting a VM: AfterdeletingaVM,thegraphmodelis component C*, this would result in updating the cor-
updatedbyremovingtheassociatedvirtualports,thenthe responding radix trie with the decision of the newly
lastresultisupdatedbyremovingallVMpairswherethe inserted rule. Then, for each prefix-to-prefix binary trie
deletedVMappearseitherasasourceorasadestination. built for subnets belonging to the component C*, the
variable HR of the binary trie (holding the history of
- Creating a subnet: When a subnet (denoted as SN*) is visitednodes)isconsulted.IftheIDofR*appearsinthe
newly created, it is specified with a gateway, which is a history of traversed nodes, then the correspondingbinary
routerinterface,andanIPprefix.Ourgraphmodelisup- trie needs to be updated. Then VM-level isolation needs
datedwithanewsubnetnodewithanedgetothegateway to be checked for couples of VMs if the source and/or
interface and the corresponding radix tree is initialized. destination belong to C*. Routing rules and host routes
This event will create new prefix-to-prefix binary tries deletion and addition events are handled similarly.
for which SN* is either a source or a destination. This
wouldresultinre-calculatingstep1forC*,themaximally Algorithm 3 Rules addition/deletion
connected component SN* belongs to, then step 2-a and 1: Onthecreation/deletionofasecuritygroupruler*forasetofVMs*do:
step 2-b for SN*. As long as no VM has been attached 2: updateRadixTrie(r*)
to SN*, the VM-to-VM reachability verification (step 3) 3: foreachVM*inVMs*do
4: ifr*isanegressrulethen
does not require updates. 5: foreachpair(VMsrc,VMdst)where(VMsrc=VM∗do
6: VerifySecGroups(VM∗,VMdest)
- Deleting a subnet: The deletion of a subnet (denoted as 7: ifr*isaningressrulethen
SN*) will lead to deletingthe prefix-to-prefixtries where 8: foreachpair(VMsrc,VMdst)where(VMdst=VM∗do
9: VerifySecGroups(VMsrc,VM∗)
SN* appears either as a source or as a destination. This
10: Onthecreation/deletionofaroutingruler*atrouterR*belongingtoC*do:
will obviously reduce the number of possible forwarding 11: updateRadixTrie(r*)
paths. As such, VM-to-VM reachability also needs to 12: foreachprefix-to-prefixbinarytriebtriebuiltforsubnetsofC∗do
13: ifR∗isinbtrie.leaves.HR then
be updated accordingly for all VMs having their private
14: prefix-to-prefix(btrie)
IPs within the prefix of SN* either as a source or as a 15: foreachpair(VMsrc,VMdst)whereVMsrcinC∗ and/orVMdstinC∗ do
destination. 16: VM-to-VM(VMsrc,VMdst)
- Creatingarouter:Theeventofaddingarouter(denoted Tofacilitatetheverificationupdate,weleveragecachesthat
as R*) would result only in adding a router node in store intermediary and previous prefix-level isolation results,
the graph model and initializing the corresponding radix such as X-fast binary tries. We also utilize the radix tries,
tree. The verification result is affected when the router’s which store the routing rules and the security groups.
interfaces are connected either to the tenant’s network or
As discussed in Section IV-C3, the complexity of Algo-
to the external network, and the routing rules are added.
rithm3(basicallyupdatingtheRadixtree)isconstantbecause
it is linear in the length of a key, which is bounded by 32. In
- Deleting a router: Deleting a router R* requires recal-
culating all the Btries for the component C* the router
contrast,the complexityof a fullverificationis O(M2)where
belongs to. VM-to-VM reachability analysis should also M isthenumberofVMs,canbeaslargeasmillionsforareal
cloud. Therefore, the overhead of our incremental verification
be partially performed in this situation for all VMs
is negligible in comparison to a full verification.
belonging to C* either considered as a source or as a
destination.
V. APPLICATION TOOPENSTACK
- Deletingasecuritygrouprule:Wheneveraningress(or
egress)ruleisdeletedfromasecuritygroup,theactionis We have implemented the proposed system design as a
propagated into all VMs, denoted as VMs*, this security prototypesystembasedonOpenStack[12].Inthissection,we
group is attached to. Consequently, the corresponding briefly discuss implementation details about data collection,
ingress (or egress) radix trie is updated accordingly. Let preprocessing, and parallel verification. In OpenStack, VMs
VM*beamemberofVMs*.Forthedeletionofanegress are managedby the compute service Nova, while networking
(resp. ingress) rule, security groups verification result is serviceNeutronmanagesvirtualnetworkingresourcesinthe
10cloud. Data related to these services is stored in databases dividedrunnableisnotexecutedatthecontroller.Thecompute
containing over one hundred tables. workerclusterconsistsofnodesforperformingtasksassigned
bythe controller.Thenodesdiscovereachotherautomatically
DataCollectionandPreprocessing.TenantGuardallowsboth through the configuration in the same LAN. The result could
on-demandandonregularbasisincrementalauditing.Tobuild be returned directly to the caller, or written into the data
a snapshot of the virtual networking infrastructure for audit- cache cluster in such a way that the data can be in-memory
ing, we collect data from OpenStack databases. Additionally, distributed among the nodes. The latter is especially useful
we leverage the notification service from PyCADF [44] and when the size of data exceeds the capacity of single-machine
Ceilometer [12] services to intercept operational events that memory.
result in a configuration change. Thus, our data collection
module starts by collecting an initial snapshot of the virtual Integration to OpenStack Congress. We further integrate
networking infrastructure. Then, at each detected event, the TenantGuard into OpenStack Congress service [17]. Congress
changedconfigurationis gatheredand the snapshot of the vir- implements policy as a service in OpenStack in order to pro-
tualnetworkinginfrastructureisupdatedtoenableincremental vide governance and compliance for dynamic infrastructures.
verification. Congress can integrate third party verification tools using a
data source driver mechanism [17]. Using Congress policy
Once the data is collected, we perform several prepro-
language that is based on Datalog, we define several tenant
cessing steps, such as building and initializing different data
specific security policies. We then use TenantGuard to detect
structurestobeusedintheverificationstep.Forinstance,from
network isolation breaches between multiple tenants. Tenant-
the list of all subnets, routers, and gateways of all tenants,
Guard’s results are in turn provided as input for Congress
we correlate the information to determine which subnets can
to be asserted by the policy engine. This allows integrating
actually communicate through public IPs. We also determine
compliance status for some policies whose verification is
the lists of subnets involved in the prefix-level verification
not supported by Congress (e.g., reachability verification as
usingpublicIPprefixes,andsubnetspermaximallyconnected
mentionedin Section II). TenantGuardcan successfully verify
subgraphs as explained in Section IV-C1. Additionally, we
VMreachabilityresultsagainstsecuritypoliciesdefinedinside
filter all ‘orphan’ subnets, as they are not connected to any
thesametenantandamongdifferenttenants.TenantGuardcan
other subnets or external networks.
also detect breaches to network isolation. For example, we
In OpenStack, VMs and virtual networking resources are test an attack in which, through unauthorized access to the
respectively managed by Nova and Neutron services. The OpenStackmanagementinterface,theattackerauthorizessome
corresponding configuration data is then stored in Nova and malicious VMs to have access to the virtual networks from
Neutron databases. Therefore, we mainly used SQL queries other tenants. TenantGuard can successfully detect all such
to retrieve data for different tables in those databases. For injected security breaches providing the list of rules in the
instance, VM ports, router interfaces, router gateways and routers that caused the breach.
other virtual ports are collected from table ports in Neutron
database. Therein, we use both device owner and device id VI. EXPERIMENTS
fields to infer the type and affiliation relationship between
This section presents experimentalresults for performance
the virtual ports and their corresponding devices. The packet
evaluation of TenantGuard on a single machine, on Amazon
filteringandforwardingrulesarestoredinneutron.routerrules,
EC2 [45] and using data collected from a real cloud. We also
subnetroutes, and securitygrouprules tables, where rules are
performaquantitativecomparisonwithourbaselinealgorithm
represented by destination-nexthopdata pairs.
and with NoD [27], which is the closest work to ours (as
detailed in Section II, most of the other works are either
Parallelization of Reachability Verification. In addition to
designed for physical networks and not suitable for large
the single machine-based implementation, we have also ex-
scale virtual networks, or they do not support the verification
tendedTenantGuardtoaparallelenvironment.Theparalleliza-
of all-pair reachability at the VM-level as targeted by our
tion is based on building groups of prefixes such that there is
solution).Notethat,inourexperiments,thebaselinealgorithm
no common path in the graph. This allows us to cache the
is notbruteforcebutalreadyanoptimizedalgorithmthatuses
temporary binary tries to store results for routers matching,
efficient data structures, mainly radix tries (although it lacks
which can be reused in other paths. Thus, we divide the list
the other optimization mechanisms of our final solution, e.g.,
of all prefixes into groups of prefixes that would be used as
thethree-stepprefix-to-prefixapproachdetailedinSectionIV).
destinationprefixesandwe createthe samenumberofthreads
asthegroups,whereeachthreadconsidersallpossibleprefixes
A. Experimental Settings
as sources.
Our test cloud is based on OpenStack version Kilo with
The analysis controller is responsible for data collection,
Neutron network driver, implemented by ML2 OpenVSwitch
graph construction, verification tasks scheduling and distri-
and L3 agent plugins, which are popular networking deploy-
bution. The controller obtains the topological view of the
ments [12]. There are one controller node integrated with
compute worker cluster, its computationcapacity and metrics,
networking service, and up to 80 compute nodes. Tenants’
such as the number of cores with CPU loads. Based on such
VMs are initiated fromthe TinyCirrOSimage [12], separated
profiles,thetaskschedulerdynamicallydividestheverification
byVLANinsidethecomputenodes,whileVxLANtunnelsare
computationintoJavarunnables,whichwillbedistributedand
used for the VM communications across the compute nodes.
executed individually across the worker clusters through data
streaming in such a way that all the tasks are performed in We generatetwo series ofdatasets (i.e.,SNET andLNET)
memoryandnodisk IOis involved.Toavoidinterference,the for the evaluation. The SNET datasets represent small to
118
6
4
2
0
100 200 300 400 500
# of VMs/Subnet
)s(
emiT
12
9
6
3
0
0 2,000 4,000 6,000
# of Rules/Router
)s(
emiT
Prefix−to−Prefix Baseline Algo NoD
50
40
30
20
10
0
0 5 10 15 19
# of Hops
)s(
emiT
(a) (b) (c)
Fig.9. Performance ComparisonbyVaryingthe#of(a)VMsperSubnet, (b)Routing Rules,and(c)Hops,whileFixingthe#ofSubnets to62
DataSet VMs Routers Subnets ReachablePaths the time is measured for all-pair. As shown in Figure 9(a),
DS1 4362 300 525 >5.67million
when the numberof VMs per subnetis increased from 100 to
DS2 10168 600 1288 >29.2million
DS3 14414 800 1828 >57.0million 500,theprefix-levelisolationverificationtimeincreasesmuch
DS4 20207 1000 2580 >109million slower than the baseline algorithm (defined in Section IV-A)
DS5 25246 1200 3210 >168million
and NoD. The reason behind these results is, as illustrated in
TABLEIII. LNETDATASETDESCRIPTION complexity analysis in section IV, the prefix-to-prefix algo-
medium virtual networks containing six subnets, while we rithm that reduces the complexity to O(R+N2), in contrast
vary different factors such as the number of VMs per subnet,
toO(R∗N2)inthebaselinealgorithm,whereRisthenumber
the number of rules per router, and the number of hops of hops and N is the number of VMs; when N increases, the
between subnets, to examine corresponding characteristics of
complexityO(R∗N2)increasesmuchfasterthanO(R+N2).
our algorithms, and have been built using OpenStack and On the other hand, as the numberof pairs is one of the major
specifically using Horizon. We then cloned the SNET virtual factors for NoD verification time, we observe increase in the
infrastructure environments to obtain different tenants and verification time while increasing the number of pairs from 1
thus LNET datasets, which represent large networks, where to 5. As shown in Figure 9(b), when the number of routing
each virtual network is organized in a three-tier structure rules per router increases exponentially, the verification time
wherethefirst-tierrouterisconnectedtotheexternalnetwork, for TenantGuard and the baseline algorithm remain relatively
while the others use extra routes to forward packets between stable due to using radix trie and X-fast binary trie, both of
each other, so in essence they are synthetic. Security rules which have constant searching time; However, the baseline
are generated in the same logic behind the deployment of algorithm takes longer time due to the higher numberof pairs
two-tier applications in the cloud. For a given tenant, one to be verified. On the other hand, as NoD is designed for a
group of VMs can only communicate with each other but large number of rules instead of large number of pairs, we
not with outside (or other tenants) networks, while another increase the number of pairs from 1 to 5 while keeping the
group is open to be reached from anywhere. Up to 25,246 number of rules similar to the setting of TenantGuard.
VMs are created in the test cloud, with 1,200 virtual routers,
Additionally, as the number of hops increases the com-
3,210 subnets, and over 43,000 allocated IP addresses. As a
plexity of the verification (it corresponds to the number of
reference, according to a recent report [12], 94% of inter-
virtualrouterson a communicationpath),we varythe number
rogated OpenStack deployments have less than 10,000 IPs.
of hops between VMs. We investigate the average number of
Therefore, we consider the scale of our largest dataset is
hopusuallyencounteredinreallifesystems(e.g.,Internet)and
a representative of large size clouds. The five datasets in
accordingto [47] and [48], the average numberof hopsvaries
LNET are described in Table III. We use open-sourceApache
between12to19;hence,wevarythenumberofhopsbetween
Ignite [46] as the parallel computation platform, which can
2 to19.Figure9(c) showsthattheprefix-to-prefixverification
distributetheworkloadinreal-timeacrosshundredsofservers.
experiencesnegligiblechanges.Incontrast,afourfoldincrease
On the other hand, all datasets both in SNET and LNET for
in the overhead is observed with the baseline algorithm.
NoDaregeneratedsynthaticallyusingtheprovided generator.3
Whereas,theverificationtimeforNoDincreasesexponentially
specially after 14 hops, as their algorithms are not optimized
B. Results
for higher number of hops.
We evaluate the performance of our approaches and the
effect of various factors on the performance. 2)LNET Results Using Amazon EC2: Single-Machine
Mode. The LNET datasets are used to examine the scalability
1)SNET Results: This set of experiments is to test how ofoursystemforlargevirtualnetworks.Hence,factorsexam-
networkstructureandconfigurationinfluencetheperformance ined in the SNET dataset are kept invariant for each subnet,
of our system. All tests using SNET datasets are conducted and the number of tenant’s subnets is varied as shown in
with a Linux PC having 2 Intel i7 2.8GHz CPUs and 2GB TableIII.TherearetwomodesforLNET tests:single-machine
memory. Note that, for SNET datasets the verification time mode and parallel mode. Single-machine tests are conducted
for NoD is measured for 1 to 5 pairs, and for TenantGuard ononeEC2C4.largeinstanceatAWSEC2with2vCPUsand
3.75 GB memory.We measureNoD performanceonly for the
2Notethat forNoD,wevarythenumberofpairs from1to5through the
singlemachinetests, asNoDimplementationdoesnotsupport
Xaxis,andforTenantGuard,weconsiderallpossiblepairsofVMsastheX
axisdepicts thenumberofVMs. parallelization. The data collection and initialization steps are
3Available at:http://web.ist.utl.pt/nuno.lopes/netverif performed on a single machine in both modes.
122
1.5
1
0.5
0
4.3k 10k 14k 20k 25k
# of VMs
)s(
emiT
600
400
200
0
3.3k 4.7k 6.2k 7.8k 9.3k
# of Rules
)s(
emiT
Prefix−to−Prefix Baseline Algo NoD
50
40
30
20
10
2 5 8 11 14 16
# of Worker nodes
(a) Data collection (b) Isolation verification
Fig. 10. Performance Comparison in the Single-Machine Mode with the
LNET Datasets Described in Table III. (a) Showing Data Collection Time,
and(b)ShowingVerification Time
As shown in Figure 10(a), data collection and processing
timevariesbetween1.5to2seconds,includingretrievingdata
from the database, initializing radix tries for routersand secu-
rity groups, etc., which shows that the collection time is not
the prominent part of the execution time. Meanwhile, Figure
10(b) compares the verification time between TenantGuard,
baseline algorithm and NoD. When the number of routing
rules increases along with subnets and VMs, the prefix-to-
prefix algorithm is more efficient than NoD and the baseline
algorithm (e.g., TenantGuard performs 82% faster than NoD
forthelargestdataset).NoD(whilevaryingthenumberofpairs
from20to200throughtheXaxis)andthebaselinealgorithm
show almost similar response time. For 9,300 routing rules
with 25,246VMsin 3,250subnets,ittakes 108secondsusing
the prefix-to-prefix algorithm, 605 seconds for NoD (for 200
pairs)andaround628secondsforthebaselinealgorithm.Note
thatTenantGuardverifiesin totalover168millionsVM pairs.
We report our extended experiment results to further val-
idate the scalability of TenantGuard in Table IV. In this
set of experiment, we increase the number of routing rules
to 850k, and the number of VMs to 100k to compare the
reported results in Plotkin et al. [11]. For the first part, we
compare TenantGuard with NoD for the 850k routing rules
with other parameters as our DS5 datasets, and observe that
TenantGuardcompletesthe all-pair reachabilityverificationin
100.14s, which is significantly faster than NoD (5.5 days). In
the second part, we generate a completely new datasets with
850k routing rules and 100k VMs, and observe that the all-
pair reachability verification takes less than 18 minutes for
TenantGuard,whereas Plotkin et al. [11] needsabout2 hours.
Datasets 850krules 850krulesand100kVMs
NoD[27] 475,200[11] -
Plotkinetal.[11] - 7,200[11]
TenantGuard 100.14 1,055.88
TABLEIV. COMPARINGTHEPERFORMANCE(INSECONDS)BETWEEN
EXISTINGWORKSANDTENANTGUARDTOVERIFYALL-PAIR
REACHABILITY
Parallel Verification Test. Although our approach already
demonstratessignificantperformanceimprovementsoverNoD
and the baseline algorithm, the results are still based on a
single machine. In real clouds, with large deployments (10Ks
of active VMs), there is need for verifying very large virtual
networks. Therefore, we extend our approach to achieve par-
allel verification,where the isolation verification is distributed
among the nodes of worker cluster, except the data collection
and initialization run on a single node. This parallel imple-
mentation provides larger memory capacity to our approach,
and results in much shorter verification times. For the parallel
mode,oneEC2C4.xlargeinstancewith4vCPUsisconfigured
as the controller, and up to 16 instances of the same type
)s(
emiT
8
6
4
2
0
0 20 40 60
Portion of task distributing (%)
)semit(
etaR
pudeepS
4,362 VMs 10,168 VMs 14,414 VMs 20,207 VMs 25,246 VMs
(a) Parallel Mode (b) Speedup Analysis
Fig.11. ThePerformanceImprovementofParallelComputationwithLNET
DataDescribedinTableIII.(a)Verification TimewhileVaryingtheNumber
of Worker Nodes in Amazon EC2 for Different Datasets, and (b) Speedup
Analysis overtheNumberofVMsUsing16WorkerNodes
with a compute worker cluster, while node discovery and
communications are established by their internal IPs.
Figure11(a)showstheperformanceofparallelverification
using 2 to 16 worker nodes. Clearly, for each dataset, by
increasing the number of worker nodes, in contrast to the
result of the single machine mode, the overheads decrease
significantly. For example, in contrast to 108 seconds in the
single machine mode, it only takes approximately 13 seconds
in the parallelmodewith 16 workers, while over160millions
of paths are verified as reachable.
In Figure 11(b), in order to show the scalability of our
approachwhileincreasingthevirtualnetworksize,weexamine
the relationship between the cluster size and speedup gain.
The parallel execution time can be divided into two parts:
task distribution time to send input data from the controller
to different workers, namely T , and execution time on those
d
nodes (we ignore the result generation time due to the small
size of result data). We note that, even if the tasks could be
dividedevenly,whichisunlikelythecaseinpractice,thetasks
could still arrive at worker nodes at different times. As a
result, some of those tasks may start significantly later than
othersdue to networkingdelay,while the overallperformance
isalwaysdecidedbytheslowerrunners.AsT becomeslarger,
d
it becomes more predominant in the overall execution time.
However, due to the lack of knowledge on task execution
sequence in the synchronous mode, we cannot accurately
measure the distribution time. Additionally, there will always
be some tasks which begin later than the other tasks. In order
to minimize this impact and to start tasks at roughlythe same
time, we use an asynchronous task distribution technique. In
Figure 11(b), the x-axis represents the ratio T /T, while T d
is the overall verification time. In addition, the speedup ratio
(R ) is the performance ratio between sequential and parallel
s
programs, represented by y-axis. With the number of worker
nodes increasing, T rises as expected because more data
d
and code need to be transferred among cluster nodes. When
it becomes more dominant, the speedup rate increases more
gradually. For the smallest dataset, R decreases when the
s
number of workers ranges from 8 to 16. The T /T ratio can
d
be used to decide the optimal data size in each node.
Our experiment results show that even a small number
(i.e., 16) of working nodes can handle large-scale verification
(i.e., 168 millions of VM pairs); recalling that real world
clouds have the size of 100,000 users and maximum 1,000
VMsforeachuser.Also, ourspeed-upanalysis(Figure11(b))
illustratesthatafter8nodesthespeedupgoesdown.Therefore,
we restrict the number of working nodes to 16. The result
of incremental verification is not reported, as our discussion
13Routing/Filtering OpenStack[12] AmazonEC2-VPC[49] GoogleGCE[50] MicrosoftAzure[51] VMwarevCD[52] TenantGuardsupport
Intra-tenant Hostroutes,routers Routingtables Routes System and user- Distributed logical Yes/ TenantGuard forwarding and
routing definedroutes routers filteringfunction
Inter-tenant Routers, external Internet gateway/VPC Internetgateway SystemroutetoInter- Edgegateway Yes/ TenantGuard forwarding and
routing gateways peering net filteringfunction
L3filtering Securitygroups Securitygroups Firewallrules Network security Edge firewall ser- Yes/ TenantGuard forwarding and
groups vice filteringfunction
TABLEV. ROUTINGANDFILTERINGINDIFFERENTCLOUDPLATFORMSANDHOWTHEYARESUPPORTEDINTENANTGUARD
in Section IV-C5 shows that the overhead of the incremental supported by TenantGuard via the forwarding and filtering
verification is negligible in comparison to a full verification. function fd .
G
3)Experiment with Real Cloud: We further test Tenant- Preserving integrity of the system. There exist many tech-
Guardusingdatacollectedfromarealcommunitycloudhosted niques on trusted auditing to establish a chain of trust,
at one of the largest telecommunications vendors. The main e.g., [53], [54], [55]. Bellare et al. [53] propose a MAC-
objective is to evaluate the real world applicability of Tenant- based approach. They provide the forward integrity by using
Guard (this dataset is not suitable for performance evaluation a chain of keysanderasing previouskeysso thatany old logs
due to the relatively small scale of the cloud). All tests are cannot be altered. Crosby et al. [54] also present a tree-based
performedinasinglemachineusingthecollecteddatasetwith- historydatastructure,whichpreventslogtamperingwherethe
out any modification. The tested cloud consists of only nine author of the log is untrusted. Apart from tamper prevention,
routers and 10 subnets. Initially, the TenantGuard verification there are some other works to further detect tampering logs.
processfails due to a minor incompatibilityissue between the Chong et al. [57] implementthe Schneier and Kelsey’s secure
OpenStackversionusedinourlab(Kilo)andanearlierversion auditloggingprotocolwith tamperresistanthardware,namely
used inside the real cloud (Juno). From OpenStack Juno to iButton. Furthermore,OpenStack leverages Intel Trusted Exe-
Kilo, two new fields are added to the neutron.networks table, cutionTechnology(TXT)toestablishachainoftrustfromthe
namely, ‘mtu’ int(11) and ‘vlan transparent’ tinyint(1). This embeddedTPMchipsinthehosthardwaretocriticalsoftware
difference between the two versions has prevented Tenant- components using a standalone attestation server [56].
Guard to execute SQL queries against table neutron.networks
due to the missing ‘mtu’ field. After addressing this issue by VIII. CONCLUSION
altering the neutron.networks table, TenantGuard successfully
In this paper, we have proposed a novel and scalable run-
completes the requested verification in several milliseconds.
time approach to the verification of cloud-wide, VM-level
network isolation in large clouds. We presented a new hierar-
VII. DISCUSSION chical model representing virtual networks, and we designed
efficient algorithmsand data structures to supportincremental
In this section, we provide the required effort to adopt and parallel verification. As a proof of concept, we integrated
TenantGuard in other cloud platforms e.g., Amazon, Google, our approach into OpenStack and also extended it to a par-
VMware.Additionally,we discussexistingmethodsto builda allel implementation using Apache Ignite. The experiments
chain of trust to preserve the integrity of the collected data. conducted locally and on Amazon EC2 clearly demonstrated
the efficiency and scalability of our solution. For a large data
AdaptingTenantGuardinothercloudplatforms.wereview
centercomprising25,246VMs,verificationusingourapproach
packet routing and filtering in different cloud platforms and
finished in 13 seconds. The main limitations of this work
show the applicability of TenantGuard. Table V shows how
are as follows. First, since TenantGuard only focuses on the
routing and filtering are implemented in OpenStack, Amazon
virtual network layer, a future direction is to integrate it with
AWS EC2-VPC (Virtual Private Cloud) [49], Google Com-
existing tools working at other layers (e.g., verification tools
pute Engine (GCE) [50], Microsoft Azure [51], and VMware
for physical networks, or co-residency and covert channel
vCloud Director (vCD) [52]. Similar to OpenStack, all other
detection techniques). Second, since TenantGuard relies on
platformsallowtenantstocreateprivatenetworksandtocreate
cloudinfrastructuresforinputdata,howtoensuretheintegrity
routing rules to govern communication between them. Those
of such data (e.g., through trusted computing techniques)
rulesarecapturedbytheforwardingandfilteringfunctionfd
G is another future direction. Third, TenantGuard assumes the
inourmodel.VMsattachedtothoseprivatenetworkscanhave
verification results can be safely disclosed to tenants, which
private IPs and public IPs respectively for intra-tenant and
maynotalwaysbethecase,andaddressingsuchprivacyissues
inter-tenant communication. In the case of inter-tenant com-
comprises an interesting future challenge.
munication,gatewaysareendowedwithNATservicesinorder
to manage mapping between private and public IP addresses.
ACKNOWLEDGMENT
NAT rules are captured in our model by the function fd .
G
InternetgatewaysinEC2-VPC, systemrouteto theInternetin The authorsthankthe anonymousreviewersand our shep-
Azure, and edge gateways in vCD can be represented in our herd, Vyas Sekar, for their valuable comments. We appreciate
model by the component v router gw. Exceptionally, in EC2 YueXin’ssupportintheimplementation.Thisworkispartially
VPC, the VPC peering routing can be employed to enable supported by the Natural Sciences and Engineering Research
private IP connections across tenants’ virtual networks with Council of Canada and Ericsson Canada under CRD Grant
tenants’ agreement. To support this feature, the definition of N01566.
fd will need to be extended. Security groups in OpenStack
G
and EC2, firewall rules in ECG, network security groups in REFERENCES
Azure, and edge firewall services in vCD are set up to filter
[1] Cloud Security Alliance. Security guidance for critical areas offocus
VMs’outbound/inboundpackets.Thosefilteringrulesarealso incloudcomputingv3.0,2011.
14[2] Cloud Security Alliance. Cloud computing top threats in 2016, Feb [30] G. G.Xie, J. Zhan,D. A.Maltz, H. Zhang, A.Greenberg, G.Hjalm-
2016. tysson,andJ.Rexford. Onstatic reachability analysis ofIPnetworks.
[3] V. Del Piccolo, A.Amamou, K.Haddadou, and G.Pujolle. A survey InINFOCOM,2005.
of network isolation solutions for multi-tenant data centers. IEEE [31] TaousMadi,SuryadiptaMajumdar,YushunWang,YosrJarraya,Makan
Communications Surveys Tutorials, PP(99):1–1, 2016. Pourzandi, and Lingyu Wang. Auditing security compliance of the
virtualized infrastructure in the cloud: Application to OpenStack. In
[4] SANSInstitute,InfoSecReadingRoom. Anintroductiontosecuringa
CODASPY,2016.
cloudenvironment, 2012.
[32] SuryadiptaMajumdar,YosrJarraya,TaousMadi,AmirAlimohammad-
[5] AmazonWebServices. Overview ofsecurityprocesses, June2016.
ifar, MakanPourzandi, LingyuWang,andMouradDebbabi. Proactive
[6] ISO Std IEC. ISO 27002:2005. Information Technology-Security verificationofsecuritycomplianceforcloudsthroughpre-computation:
Techniques, 2005. Application toOpenStack. InESORICS,2016.
[7] ISOStdIEC. ISO27017. Informationtechnology- Securitytechniques [33] S. Bleikertz, T. Groß, M. Schunter, and K. Eriksson. Automated
(DRAFT),2012. information flow analysis of virtualized infrastructures. In ESORICS,
[8] Cloud Security Alliance. Cloud control matrix CCM v3.0.1, 2014. 2011.
Available at:https://cloudsecurityalliance.org/research/ccm/. [34] Karthick Jayaraman, Nikolaj Bjørner, Geoff Outhred, and Charlie
[9] OpenStack. OpenStack user survey, 2016. Available at: Kaufman. Automated analysis and debugging of network connectiv-
https://www.openstack.org. ity policies. Technical report, Technical Report MSR-TR-2014-102,
Microsoft Research, 2014.
[10] RightScale. RightScale 2016stateofthecloudreport,2016. Available
at:http://www.rightscale.com. [35] OpenStack. Congress documentation release. Available at:
https://congress.readthedocs.io/en/latest/.
[11] G.D.Plotkin,N.Bjørner,N.P.Lopes,A.Rybalchenko,andG.Vargh-
ese.Scalingnetworkverificationusingsymmetryandsurgery.InPOPL, [36] RobinJ.W.Definitionsandexamples.InIntroductiontoGraphTheory,
2016. SecondEdition, 1979.
[12] OpenStack. OpenStack opensourcecloudcomputing software. Avail- [37] P. Kazemian, G. Varghese, and N. McKeown. Header space analysis:
ableat:http://www.openstack.org. Static checking fornetworks. InNSDI,pages113–126,2012.
[13] OpenStack. Nova network security group changes are [38] Fred Halsall. Computer Networking and the Internet (5th Edition).
not applied to running instances, 2015. Available at: Addison-Wesley LongmanPublishingCo.,Inc.,2005.
https://security.openstack.org/ossa/OSSA-2015-021.html, last visited [39] W.R.Cheswick,S.M.Bellovin,andA.Rubin. FirewallsandInternet
on:May,2016. Security: Repelling theWilyHacker.
[14] OpenStack. Routers can be cross plugged by other tenants, 2014. [40] GrzegorzMalewicz,MatthewHAustern,AartJCBik,JamesCDehnert,
Available at: https://security.openstack.org/ossa/OSSA-2014-008.html, IlanHorn,NatyLeiser,andGrzegorzCzajkowski. Pregel:asystemfor
lastvisitedon:May,2016. large-scale graphprocessing. InSIGMOD,2010.
[15] J. Corbet. Trees I: Radix tree. Available at: [41] DouglasGregorandAndrewLumsdaine. TheparallelBGL:Ageneric
http://lwn.net/Articles/175432/. library fordistributed graphcomputations. POOSC,2,2005.
[16] D. E. Willard. Log-logarithmic worst-case range queries are possible [42] AlbertChanandFrankDehne. CGMgraph/CGMlib:Implementingand
inspaceo(n),1983. Information ProcessingLetters. testing CGM graph algorithms on PC clusters. In European Parallel
Virtual Machine/Message Passing Interface Users’ Group Meeting.
[17] OpenStack. Policy as a Service (Congress). Available at:
Springer, 2003.
http://wiki.openstack.org/wiki/Congress.
[43] H. Ballani, K. Jang, T. Karagiannis, C. Kim, D. Gunawardena, and
[18] P. Kazemian, G. Varghese, and N. McKeown. Header space analysis:
G.O’Shea. Chattytenants andthecloudnetworksharingproblem. In
Static checking fornetworks. InNSDI,2012.
NSDI,2013.
[19] P. Kazemian, M. Chan, H. Zeng, G. Varghese, N. McKeown, and
[44] Cloud auditing data federation (CADF). PyCADF: A Python-based
S. Whyte. Real time network policy checking using header space
CADFlibrary,2015. Available at:https://pypi.python.org/pypi/pycadf.
analysis. InNSDI,2013.
[45] Amazon. Amazon EC2- Virtual Server Hosting. Available at:
[20] H.Mai,AhmedKhurshid,R.Agarwal,M.Caesar,P.Godfrey,andS.T.
https://aws.amazon.com/ec2.
King. Debuggingthedataplane withanteater. InSIGCOMM,2011.
[46] Ignite. Available at:https://ignite.apache.org.
[21] A.Khurshid,X.Zou,W.Zhou,M.Caesar,andP.B.Godfrey.VeriFlow:
verifying network-wide invariants inrealtime. InNSDI,2013. [47] A. Fei, G. Pei, R. Liu, and L. Zhang. Measurements on delay and
hop-count oftheinternet. InGLOBECOM,1998.
[22] H. Yang and S. S. Lam. Real-time verification of network properties
usingatomicpredicates. InICNP,Oct2013. [48] F. Begtasevic and P. V. Mieghem. Measurements of the hopcount in
internet. InPAM,2001.
[23] H.Zeng,S.Zhang,F.Ye, V.Jeyakumar, M.Ju,J.Liu,N.McKeown,
[49] Amazon. Amazon virtual private cloud. Available at:
andA.Vahdat. Libra: Divide andconquer toverify forwarding tables
https://aws.amazon.com/vpc.
inhugenetworks. InNSDI,2014.
[50] Google. Google compute engine subnetworks beta. Available at:
[24] Aaron Gember-Jacobson, Raajay Viswanathan, Aditya Akella, and
https://cloud.google.com.
Ratul Mahajan. Fast control plane analysis using an abstract repre-
sentation. InSIGCOMM,2016. [51] Microsoft. Microsoft Azure virtual network. Available at:
https://azure.microsoft.com.
[25] AriFogel,StanleyFung,LuisPedrosa,MegWalraed-Sullivan, Ramesh
Govindan, Ratul Mahajan, andToddMillstein. Ageneral approach to [52] VMware. VMware vCloud Director. Available at:
networkconfiguration analysis. InNSDI,2015. https://www.vmware.com.
[26] SeyedK.Fayaz,TusharSharma,AriFogel,RatulMahajan,ToddMill- [53] M. Bellare and B. Yee. Forward integrity for secure audit logs.
stein,VyasSekar,andGeorgeVarghese. Efficientnetworkreachability Technical report,Citeseer, 1997.
analysis usingasuccinct control planerepresentation. InOSDI,2016. [54] ScottACrosbyandDanSWallach.Efficientdatastructuresfortamper-
[27] N.P.Lopes,N.Bjørner,P.Godefroid,K.Jayaraman,andG.Varghese. evident logging. InUSENIXSecurity Symposium,2009.
Checking beliefs indynamicnetworks. InNSDI’15,2015. [55] Di Ma and Gene Tsudik. A new approach to secure logging. ACM
[28] So¨ren Bleikertz, Carsten Vogel, and Thomas Groß. Cloud Radar: Transactions onStorage(TOS),5(1):2,2009.
Near real-time detection of security failures in dynamic virtualized [56] OpenStack. Security hardening, 2016. Available at:
infrastructures. InACSAC,2014. http://docs.openstack.org/admin-guide/compute-security.html.
[29] T.Probst,E.Alata, M.Kaaˆniche, and V.Nicomette. Anapproach for [57] Cheun Ngen Chong, Zhonghong Peng, and Pieter H Hartel. Secure
theautomated analysis ofnetworkaccess controls incloudcomputing audit logging withtamper-resistant hardware. InSecurity andPrivacy
infrastructures. InNetwork andSystemSecurity. 2014. intheAgeofUncertainty. Springer, 2003.
15